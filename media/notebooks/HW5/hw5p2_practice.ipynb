{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#print stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'set'>\n"
     ]
    }
   ],
   "source": [
    "stopWords = set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', \n",
    "                              u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', \n",
    "                              u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', \n",
    "                              u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', \n",
    "                              u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', \n",
    "                              u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', \n",
    "                              u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', \n",
    "                              u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', \n",
    "                              u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', \n",
    "                              u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', \n",
    "                              u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', \n",
    "                              u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', \n",
    "                              u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', \n",
    "                              u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', \n",
    "                              u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', \n",
    "                              u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', \n",
    "                              u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n",
    "print type(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDS-W261-HW-05-PHASE1-3031886443.ipynb\r\n",
      "atlas-boon-systems-test.txt\r\n",
      "buildStripes.py\r\n",
      "english\r\n",
      "googlebooks-eng-all-5gram-20090715-0-filtered-first-10-last-1-lines.txt\r\n",
      "googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\r\n",
      "hw5p2_practice.ipynb\r\n",
      "invertedIndex.py\r\n",
      "invertedIndexOnly.py\r\n",
      "mostFrequentWords.py\r\n",
      "pairwise-document-similarity-TEMPLATE.ipynb\r\n",
      "similarity.py\r\n",
      "systems_test.txt\r\n",
      "systems_test_index.txt\r\n",
      "systems_test_index_1\r\n",
      "systems_test_index_2\r\n",
      "systems_test_index_3\r\n",
      "systems_test_similarities_1\r\n",
      "systems_test_similarities_2\r\n",
      "systems_test_similarities_3\r\n",
      "systems_test_stripes_1\r\n",
      "systems_test_stripes_2\r\n",
      "systems_test_stripes_3\r\n",
      "test.txt\r\n"
     ]
    }
   ],
   "source": [
    "#!cp ~/nltk_data/corpora/stopwords/english .\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    # purpose: steps needed to find the top 10 most frequent words\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options':'-k1,1',\n",
    "        }\n",
    "        JOBCONF_STEP2 = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options':'-k1,1nr',\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf = JOBCONF_STEP,\n",
    "                   mapper_init = self.mapper_init,\n",
    "                   mapper = self.mapper,\n",
    "                   mapper_final = self.mapper_final,\n",
    "                   combiner = self.combiner,\n",
    "                   reducer = self.reducer,\n",
    "            ),\n",
    "            MRStep(jobconf = JOBCONF_STEP2,\n",
    "                   mapper = None,\n",
    "                   reducer = self.reducer_sort,  \n",
    "            ),\n",
    "        ]\n",
    "    \n",
    "    # purpose: define the mapper's dict used to track word freq\n",
    "    def mapper_init(self):\n",
    "        self.word_counts = defaultdict(int)\n",
    "        #self.stopWords = set(stopwords.words('english'))\n",
    "        self.stopWords = [sw for sw in open('english').read().strip().split('\\n')]\n",
    "        #print self.stopWords\n",
    "        #self.stopWords = ['a']\n",
    "        #self.stopWords = set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', \n",
    "        #                      u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', \n",
    "        #                      u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', \n",
    "        #                      u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', \n",
    "        #                      u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', \n",
    "        #                      u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', \n",
    "        #                      u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', \n",
    "        #                      u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', \n",
    "        #                      u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', \n",
    "        #                      u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', \n",
    "        #                      u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', \n",
    "        #                      u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', \n",
    "        #                      u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', \n",
    "        #                      u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', \n",
    "        #                      u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', \n",
    "        #                      u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', \n",
    "        #                      u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n",
    "    \n",
    "    # purpose: split the line and capture the count for each ngram word\n",
    "    # input: key(None), value(ngram\\tcount\\tpage\\tbook)\n",
    "    # output: nothing; counts are stored in the mapper's internal dict word_counts\n",
    "    def mapper(self, _, line):\n",
    "        ngram, count, page, book = line.lower().strip().split(\"\\t\")\n",
    "        # drop word if it is an nltk stopword\n",
    "        for word in ngram.split(' '):\n",
    "            if word not in self.stopWords:\n",
    "                self.word_counts[word] += int(count)\n",
    "            \n",
    "    # purpose: emit the word and count pairs to the combiner for summing counts for each word\n",
    "    # input: use the mapper's internal word_counts dict\n",
    "    # output: key(word), value(count for the word emitted from the mapper)\n",
    "    def mapper_final(self):\n",
    "        for word, count in self.word_counts.iteritems():\n",
    "            yield word, count\n",
    "    \n",
    "    # purpose: sum up the counts for the same words processed by a mapper\n",
    "    # input: key(word), value(count for the word emitted from the mapper)\n",
    "    # output: key(word), value(sum of the word's counts)\n",
    "    def combiner(self, word, count):\n",
    "        yield word, sum(count)\n",
    "    \n",
    "    # purpose: sum up all of the word counts across the mappers\n",
    "    # input: key(word), value(sum of the word's counts from the combiners)\n",
    "    # output: key(sum of the word's counts), value(word)\n",
    "    def reducer(self, word, count):\n",
    "        #yield None, (int(sum(count)), word)\n",
    "        yield sum(count), word\n",
    "    \n",
    "    # purpose: get the sorted outputs\n",
    "    def reducer_sort(self, count, word):\n",
    "        for w in word:\n",
    "            yield count, w\n",
    "    \n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mostFrequentWords.root.20171011.123755.580547\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mostFrequentWords.root.20171011.123755.580547/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob1931615472778817968.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1506774973294_0031\n",
      "  Submitted application application_1506774973294_0031\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506774973294_0031/\n",
      "  Running job: job_1506774973294_0031\n",
      "  Job job_1506774973294_0031 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1506774973294_0031 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mostFrequentWords.root.20171011.123755.580547/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=301\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=351\n",
      "\t\tFILE: Number of bytes written=444869\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1005\n",
      "\t\tHDFS: Number of bytes written=301\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12066816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4230144\n",
      "\t\tTotal time spent by all map tasks (ms)=11784\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11784\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4131\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4131\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11784\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4131\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3030\n",
      "\t\tCombine input records=22\n",
      "\t\tCombine output records=22\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=95\n",
      "\t\tInput split bytes=442\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=323\n",
      "\t\tMap output materialized bytes=357\n",
      "\t\tMap output records=22\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=761905152\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=22\n",
      "\t\tReduce output records=22\n",
      "\t\tReduce shuffle bytes=357\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=44\n",
      "\t\tTotal committed heap usage (bytes)=556793856\n",
      "\t\tVirtual memory (bytes) snapshot=4101996544\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob4732870480948297371.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1506774973294_0032\n",
      "  Submitted application application_1506774973294_0032\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506774973294_0032/\n",
      "  Running job: job_1506774973294_0032\n",
      "  Job job_1506774973294_0032 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1506774973294_0032 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mostFrequentWords.root.20171011.123755.580547/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=452\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=301\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=373\n",
      "\t\tFILE: Number of bytes written=443239\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=806\n",
      "\t\tHDFS: Number of bytes written=301\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11164672\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4243456\n",
      "\t\tTotal time spent by all map tasks (ms)=10903\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10903\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4144\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4144\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10903\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4144\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=354\n",
      "\t\tMap input records=22\n",
      "\t\tMap output bytes=323\n",
      "\t\tMap output materialized bytes=379\n",
      "\t\tMap output records=22\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=786403328\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=22\n",
      "\t\tReduce output records=22\n",
      "\t\tReduce shuffle bytes=379\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=44\n",
      "\t\tTotal committed heap usage (bytes)=625475584\n",
      "\t\tVirtual memory (bytes) snapshot=4126224384\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mostFrequentWords.root.20171011.123755.580547/output...\n",
      "1099\t\"wales\"\n",
      "1099\t\"christmas\"\n",
      "1099\t\"child's\"\n",
      "604\t\"case\"\n",
      "604\t\"study\"\n",
      "447\t\"female\"\n",
      "239\t\"collection\"\n",
      "123\t\"fairy\"\n",
      "123\t\"tales\"\n",
      "116\t\"forms\"\n",
      "102\t\"government\"\n",
      "92\t\"george\"\n",
      "92\t\"general\"\n",
      "92\t\"biography\"\n",
      "62\t\"city\"\n",
      "62\t\"circumstantial\"\n",
      "62\t\"sea\"\n",
      "62\t\"narrative\"\n",
      "59\t\"religious\"\n",
      "59\t\"establishing\"\n",
      "59\t\"bill\"\n",
      "55\t\"limited\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mostFrequentWords.root.20171011.123755.580547...\n",
      "Removing temp directory /tmp/mostFrequentWords.root.20171011.123755.580547...\n"
     ]
    }
   ],
   "source": [
    "# test mostFrequentWords.py locally on the google sample\n",
    "!python mostFrequentWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt --file english #| sort -k1,1nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put ./english /user/root/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root supergroup        743 2017-10-11 12:10 /user/root/english\r\n",
      "drwxr-xr-x   - root supergroup          0 2017-10-04 11:53 /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
