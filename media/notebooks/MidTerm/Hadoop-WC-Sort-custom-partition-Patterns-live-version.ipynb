{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code to demostrate the following on Hadoop Stream\n",
    "- Word count\n",
    "- Combiners\n",
    "- Sorts, Secondary sorts, total sort (under construction)\n",
    "- Custom partitions\n",
    "\n",
    "### Written by James G. Shanahan\n",
    "MIDS w261 Machine Learning at Scale\n",
    "September 15, 2014\n",
    "\n",
    "### Data used\n",
    "- Selfcontained datesets generated using code\n",
    "- Gutenberg dataset available http://www.gutenberg.org/cache/epub/48054/pg48054.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gutenberg dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 87483  100 87483    0     0  79720      0  0:00:01  0:00:01 --:--:-- 79674\n"
     ]
    }
   ],
   "source": [
    "#run any unix command\n",
    "!curl http://www.gutenberg.org/cache/epub/48054/pg48054.txt > WordCount/historical_tours.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING www.bbc.net.uk (212.58.246.54): 56 data bytes\n",
      "64 bytes from 212.58.246.54: icmp_seq=0 ttl=52 time=152.135 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=1 ttl=52 time=153.965 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=2 ttl=52 time=170.487 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=3 ttl=52 time=164.168 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=4 ttl=52 time=152.358 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=5 ttl=52 time=153.545 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=6 ttl=52 time=154.728 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=7 ttl=52 time=149.378 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=8 ttl=52 time=156.088 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=9 ttl=52 time=152.145 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=10 ttl=52 time=153.094 ms\n",
      "^C\n",
      "--- www.bbc.net.uk ping statistics ---\n",
      "12 packets transmitted, 11 packets received, 8.3% packet loss\n",
      "round-trip min/avg/max/stddev = 149.378/155.645/170.487/5.896 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ping www.bbc.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount Example in map reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following for a detailed presentation of the word count example\n",
    "-     http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    "- See this notebook for more nuances and more intricate extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        if word == \"debt\":\n",
    "            sys.stderr.write(\"reporter:counter:EDA Counters,Calls,1\\n\")          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load WordCount/reducer.py  #Load code into the current frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a local test of  your mapper.py and reducer on Unix command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "quux\t1\r\n",
      "labs\t1\r\n",
      "foo\t1\r\n",
      "bar\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop Cluster locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export HADOOP_HOME=/usr/local/Cellar/hadoop/2.6.0\r\n",
      "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\r\n",
      "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\r\n",
      "alias hstart=\"/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh\"\r\n",
      "alias hstop=\"/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# MUST start hadoop server\n",
    "!cat ~/.profile\n",
    "#!hstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "16/05/26 08:33:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "16/05/26 08:33:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/26 08:33:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-namenode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-datanode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-secondarynamenode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "16/05/26 08:33:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-resourcemanager-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-nodemanager-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n"
     ]
    }
   ],
   "source": [
    "#stop old cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\n",
    "#hstart Cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh; /usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Start Hadoop UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/mapred-jshanahan-historyserver-JAMES-SHANAHANs-Desktop-Pro-2.local.out\r\n"
     ]
    }
   ],
   "source": [
    "#this may not work for all distributions\n",
    "\n",
    "#STEP 0 Kill ill existing server using kill -9 \"insert your PID here\"\n",
    "# STEP 1:\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/mr-jobhistory-daemon.sh  start historyserver\n",
    "#!/usr/local/Cellar/hadoop/2.6.0/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.6.0/etc/hadoop/ start historyserver\n",
    "\n",
    "#If you are running Hadoop locally, you need to start the JobHistoryServer by running (assuming in hadoop directory)\n",
    "#./sbin/mr-jobhistory-daemon.sh --config ./etc/hadoop/ start historyserver \n",
    "\n",
    "# STEP 2: Then you will be able to access the job information by navigating to:\n",
    "#     \n",
    "# http://192.168.0.10:19888/jobhistory/app\n",
    "# http://127.0.0.1:19888/jobhistory/app\n",
    "\n",
    "# STEP 3: Click your job and then click the Counter tab on the left:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan 30, 2016 9:15:18 AM com.google.inject.servlet.InternalServletModule$BackwardsCompatibleServletContextProvider get\r\n",
      "WARNING: You are attempting to use a deprecated API (specifically, attempting to @Inject ServletContext inside an eagerly created singleton. While we allow this for backwards compatibility, be warned that this MAY have unexpected behavior if you have more than one injector (with ServletModule) running in the same JVM. Please consult the Guice documentation at http://code.google.com/p/google-guice/wiki/Servlets for more information.\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n",
      "INFO: Registering org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices as a root resource class\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n",
      "INFO: Registering org.apache.hadoop.mapreduce.v2.hs.webapp.JAXBContextResolver as a provider class\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n",
      "INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n",
      "INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n",
      "INFO: Binding org.apache.hadoop.mapreduce.v2.hs.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n",
      "INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n",
      "Jan 30, 2016 9:15:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n",
      "INFO: Binding org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices to GuiceManagedComponentProvider with the scope \"PerRequest\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /usr/local/Cellar/hadoop/2.6.0/libexec/logs/mapred-jshanahan-historyserver-JAMES-SHANAHANs-Desktop-Pro-2.local.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: hdfs: not found\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay attention to the order in which you specify the opts for hdfs commands\n",
    "\n",
    "Generic options supported are\n",
    "- -conf <configuration file>     specify an application configuration file\n",
    "- -D <property=value>            use value for given property\n",
    "- -fs <local|namenode:port>      specify a namenode\n",
    "- -jt <local|resourcemanager:port>    specify a ResourceManager\n",
    "- -files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
    "- -libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
    "- -archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
    "\n",
    "The general command line syntax is\n",
    "bin/hadoop command [genericOptions] [commandOptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 08:33:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 16 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 16:57 gutenberg-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:50 gutenberg-output-sorted\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:50 gutenberg-wordCount\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2016-02-02 10:50 historical_tours.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:02 ipAddresses\n",
      "-rw-r--r--   1 jshanahan supergroup        110 2016-02-02 10:23 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:23 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:46 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        400 2016-02-01 12:46 stockprice.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         56 2016-02-02 10:50 testWordCountInput.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:32 wordcount-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:43 wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small test for Word Count (one input file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:32:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:32:12 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted testWordCountInput.txt\n",
      "16/02/02 10:32:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:32:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:32:15 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output\n",
      "16/02/02 10:32:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:32:16 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:32:16 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:32:16 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:32:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 10:32:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 10:32:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local964127427_0001\n",
      "16/02/02 10:32:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:32:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:32:16 INFO mapreduce.Job: Running job: job_local964127427_0001\n",
      "16/02/02 10:32:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:32:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:32:16 INFO mapred.LocalJobRunner: Starting task: attempt_local964127427_0001_m_000000_0\n",
      "16/02/02 10:32:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:32:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/testWordCountInput.txt:0+56\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:32:17 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task:attempt_local964127427_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task 'attempt_local964127427_0001_m_000000_0' done.\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local964127427_0001_m_000000_0\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Starting task: attempt_local964127427_0001_r_000000_0\n",
      "16/02/02 10:32:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:32:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:32:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@67404fc9\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: attempt_local964127427_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:32:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local964127427_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n",
      "16/02/02 10:32:17 INFO reduce.InMemoryMapOutput: Read 40 bytes from map-output for attempt_local964127427_0001_m_000000_0\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 1 files, 44 bytes from disk\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task:attempt_local964127427_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task attempt_local964127427_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:32:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local964127427_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local964127427_0001_r_000000\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task 'attempt_local964127427_0001_r_000000_0' done.\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local964127427_0001_r_000000_0\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Starting task: attempt_local964127427_0001_r_000001_0\n",
      "16/02/02 10:32:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:32:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:32:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2bf04564\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: attempt_local964127427_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:32:17 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local964127427_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/02 10:32:17 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local964127427_0001_m_000000_0\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task:attempt_local964127427_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task attempt_local964127427_0001_r_000001_0 is allowed to commit now\n",
      "16/02/02 10:32:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local964127427_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local964127427_0001_r_000001\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task 'attempt_local964127427_0001_r_000001_0' done.\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local964127427_0001_r_000001_0\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Starting task: attempt_local964127427_0001_r_000002_0\n",
      "16/02/02 10:32:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:32:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:32:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@453344b8\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: attempt_local964127427_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:32:17 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local964127427_0001_m_000000_0 decomp: 27 len: 31 to MEMORY\n",
      "16/02/02 10:32:17 INFO reduce.InMemoryMapOutput: Read 27 bytes from map-output for attempt_local964127427_0001_m_000000_0\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->27\n",
      "16/02/02 10:32:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19 bytes\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 27 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 1 files, 31 bytes from disk\n",
      "16/02/02 10:32:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:32:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19 bytes\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:32:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task:attempt_local964127427_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task attempt_local964127427_0001_r_000002_0 is allowed to commit now\n",
      "16/02/02 10:32:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local964127427_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local964127427_0001_r_000002\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/02 10:32:17 INFO mapred.Task: Task 'attempt_local964127427_0001_r_000002_0' done.\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local964127427_0001_r_000002_0\n",
      "16/02/02 10:32:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:32:17 INFO mapreduce.Job: Job job_local964127427_0001 running in uber mode : false\n",
      "16/02/02 10:32:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:32:17 INFO mapreduce.Job: Job job_local964127427_0001 completed successfully\n",
      "16/02/02 10:32:17 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=421634\n",
      "\t\tFILE: Number of bytes written=1502345\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=224\n",
      "\t\tHDFS: Number of bytes written=123\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=90\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=90\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1451229184\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/02/02 10:32:17 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "#bin/hadoop dfs -put /home/trendwise/Desktop/Learn/RHadoop/wcOutput.txt /RCount/part-00000 \n",
    "#bin/hadoop dfs -get /RCount/part-00000 /home/trendwise/Desktop/Learn/RHadoop/wcOutput.txt\n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -combiner WordCount/reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2  depecated\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------\n",
      "\n",
      "16/02/02 10:32:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello\n",
      "---------------------------\n",
      "\n",
      "16/02/02 10:32:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the words in descreasing order of frequency, but break ties with alphabetical sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort the wordcount out in descreasing order of counts and increasing order of tokens\n",
    "I.e., \"-k2,2nr -k1,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:19:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:19:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output-sorted\n",
      "16/02/02 08:19:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:19:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 08:19:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 08:19:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 08:19:24 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/02 08:19:24 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/02 08:19:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2101934335_0001\n",
      "16/02/02 08:19:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 08:19:24 INFO mapreduce.Job: Running job: job_local2101934335_0001\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Starting task: attempt_local2101934335_0001_m_000000_0\n",
      "16/02/02 08:19:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 08:19:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00000:0+30\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: \n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufend = 62; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task:attempt_local2101934335_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00000:0+30\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task 'attempt_local2101934335_0001_m_000000_0' done.\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Finishing task: attempt_local2101934335_0001_m_000000_0\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Starting task: attempt_local2101934335_0001_m_000001_0\n",
      "16/02/02 08:19:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 08:19:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00002:0+19\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: \n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufend = 43; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214388(104857552); length = 9/6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task:attempt_local2101934335_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00002:0+19\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task 'attempt_local2101934335_0001_m_000001_0' done.\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Finishing task: attempt_local2101934335_0001_m_000001_0\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Starting task: attempt_local2101934335_0001_m_000002_0\n",
      "16/02/02 08:19:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 08:19:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00001:0+7\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: \n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "16/02/02 08:19:24 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task:attempt_local2101934335_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00001:0+7\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task 'attempt_local2101934335_0001_m_000002_0' done.\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Finishing task: attempt_local2101934335_0001_m_000002_0\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Starting task: attempt_local2101934335_0001_r_000000_0\n",
      "16/02/02 08:19:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 08:19:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 08:19:24 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3756e9d2\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 08:19:24 INFO reduce.EventFetcher: attempt_local2101934335_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 08:19:24 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2101934335_0001_m_000002_0 decomp: 19 len: 23 to MEMORY\n",
      "16/02/02 08:19:24 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local2101934335_0001_m_000002_0\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
      "16/02/02 08:19:24 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2101934335_0001_m_000001_0 decomp: 51 len: 55 to MEMORY\n",
      "16/02/02 08:19:24 INFO reduce.InMemoryMapOutput: Read 51 bytes from map-output for attempt_local2101934335_0001_m_000001_0\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 51, inMemoryMapOutputs.size() -> 2, commitMemory -> 19, usedMemory ->70\n",
      "16/02/02 08:19:24 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2101934335_0001_m_000000_0 decomp: 72 len: 76 to MEMORY\n",
      "16/02/02 08:19:24 INFO reduce.InMemoryMapOutput: Read 72 bytes from map-output for attempt_local2101934335_0001_m_000000_0\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 72, inMemoryMapOutputs.size() -> 3, commitMemory -> 70, usedMemory ->142\n",
      "16/02/02 08:19:24 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 08:19:24 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/02 08:19:24 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 112 bytes\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: Merged 3 segments, 142 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: Merging 1 files, 142 bytes from disk\n",
      "16/02/02 08:19:24 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 08:19:24 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 08:19:24 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 128 bytes\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task:attempt_local2101934335_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task attempt_local2101934335_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 08:19:24 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2101934335_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output-sorted/_temporary/0/task_local2101934335_0001_r_000000\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 08:19:24 INFO mapred.Task: Task 'attempt_local2101934335_0001_r_000000_0' done.\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: Finishing task: attempt_local2101934335_0001_r_000000_0\n",
      "16/02/02 08:19:24 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 08:19:25 INFO mapreduce.Job: Job job_local2101934335_0001 running in uber mode : false\n",
      "16/02/02 08:19:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 08:19:25 INFO mapreduce.Job: Job job_local2101934335_0001 completed successfully\n",
      "16/02/02 08:19:25 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423851\n",
      "\t\tFILE: Number of bytes written=1497929\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=191\n",
      "\t\tHDFS: Number of bytes written=75\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=120\n",
      "\t\tMap output materialized bytes=154\n",
      "\t\tInput split bytes=348\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=154\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1831337984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=75\n",
      "16/02/02 08:19:25 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "# This call to Hadoop does NOT work as anticpated\n",
    "# IdentityMapper, IdentityReducer do not trigger the Hadoop framework to sort properly\n",
    "\n",
    "!hdfs dfs -rm -r wordcount-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -input wordcount-output \\\n",
    "   -output wordcount-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "\n",
    "#DOES not work in streaming mode\n",
    "#   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "#  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimi\t3\r\n",
      "Hello\t1\r\n",
      "jimi\t1\r\n",
      "this\t1\r\n",
      "three\t1\r\n",
      "hello\t2\r\n",
      "is\t1\r\n",
      "who\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 30 wordcount-output-sorted/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:19:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-02 08:19 wordcount-output-sorted/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         75 2016-02-02 08:19 wordcount-output-sorted/part-00000\n",
      "ls: `wordcount-output-sorted/part-00000.SORTED.txt': No such file or directory\n",
      "16/02/02 08:19:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0\tHello\t1\n",
      "0\tJimi\t3\n",
      "0\thello\t2\n",
      "13\twho\t1\n",
      "15\tthis\t1\n",
      "22\tthree\t1\n",
      "8\tis\t1\n",
      "8\tjimi\t1\n"
     ]
    }
   ],
   "source": [
    "#Does NOT work \n",
    "!hdfs dfs -ls wordcount-output-sorted/*\n",
    "!rm -r wordcount-output-sorted\n",
    "!hdfs dfs -copyToLocal wordcount-output-sorted \n",
    "\n",
    "!sort -k2,2nr <wordcount-output-sorted/part-00000 >wordcount-output-sorted/part-00000.SORTED.txt\n",
    "!head -n 100 wordcount-output-sorted/part-00000.SORTED.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identityMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting identityMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identityMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#need to specify an identity mapper in order to trigger the sort in Hadoop\n",
    "#\n",
    "#Can also use the identtiy mapper:  -mapper /bin/cat \\  \n",
    "#\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print line.strip()\n",
    "    #print output to stadout/screen \n",
    "    #sys.stderr.write(\"reporter:status:processing line [%s]\" % (line.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:43:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordcount-output-sorted': No such file or directory\n",
      "16/02/02 10:43:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:43:33 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:43:33 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:43:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:43:33 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/02 10:43:33 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/02 10:43:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1433494625_0001\n",
      "16/02/02 10:43:33 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:43:33 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:43:33 INFO mapreduce.Job: Running job: job_local1433494625_0001\n",
      "16/02/02 10:43:33 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Starting task: attempt_local1433494625_0001_m_000000_0\n",
      "16/02/02 10:43:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:43:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00000:0+30\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:43:34 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufend = 34; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task:attempt_local1433494625_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task 'attempt_local1433494625_0001_m_000000_0' done.\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local1433494625_0001_m_000000_0\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Starting task: attempt_local1433494625_0001_m_000001_0\n",
      "16/02/02 10:43:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:43:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00002:0+19\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214388(104857552); length = 9/6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task:attempt_local1433494625_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task 'attempt_local1433494625_0001_m_000001_0' done.\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local1433494625_0001_m_000001_0\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Starting task: attempt_local1433494625_0001_m_000002_0\n",
      "16/02/02 10:43:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:43:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00001:0+7\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:43:34 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: bufstart = 0; bufend = 8; bufvoid = 104857600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "16/02/02 10:43:34 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task:attempt_local1433494625_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task 'attempt_local1433494625_0001_m_000002_0' done.\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local1433494625_0001_m_000002_0\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Starting task: attempt_local1433494625_0001_r_000000_0\n",
      "16/02/02 10:43:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:43:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:43:34 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3156f84a\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:43:34 INFO reduce.EventFetcher: attempt_local1433494625_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:43:34 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1433494625_0001_m_000001_0 decomp: 30 len: 34 to MEMORY\n",
      "16/02/02 10:43:34 INFO reduce.InMemoryMapOutput: Read 30 bytes from map-output for attempt_local1433494625_0001_m_000001_0\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 30, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->30\n",
      "16/02/02 10:43:34 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1433494625_0001_m_000000_0 decomp: 44 len: 48 to MEMORY\n",
      "16/02/02 10:43:34 INFO reduce.InMemoryMapOutput: Read 44 bytes from map-output for attempt_local1433494625_0001_m_000000_0\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 44, inMemoryMapOutputs.size() -> 2, commitMemory -> 30, usedMemory ->74\n",
      "16/02/02 10:43:34 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1433494625_0001_m_000002_0 decomp: 12 len: 16 to MEMORY\n",
      "16/02/02 10:43:34 INFO reduce.InMemoryMapOutput: Read 12 bytes from map-output for attempt_local1433494625_0001_m_000002_0\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 12, inMemoryMapOutputs.size() -> 3, commitMemory -> 74, usedMemory ->86\n",
      "16/02/02 10:43:34 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:43:34 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/02 10:43:34 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 57 bytes\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: Merged 3 segments, 86 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: Merging 1 files, 86 bytes from disk\n",
      "16/02/02 10:43:34 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:43:34 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:43:34 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 73 bytes\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task:attempt_local1433494625_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task attempt_local1433494625_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:43:34 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1433494625_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output-sorted/_temporary/0/task_local1433494625_0001_r_000000\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 10:43:34 INFO mapred.Task: Task 'attempt_local1433494625_0001_r_000000_0' done.\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local1433494625_0001_r_000000_0\n",
      "16/02/02 10:43:34 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:43:34 INFO mapreduce.Job: Job job_local1433494625_0001 running in uber mode : false\n",
      "16/02/02 10:43:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:43:34 INFO mapreduce.Job: Job job_local1433494625_0001 completed successfully\n",
      "16/02/02 10:43:34 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423739\n",
      "\t\tFILE: Number of bytes written=1507492\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=191\n",
      "\t\tHDFS: Number of bytes written=64\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=64\n",
      "\t\tMap output materialized bytes=98\n",
      "\t\tInput split bytes=348\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=98\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1834483712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64\n",
      "16/02/02 10:43:34 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "#Custom partitioner works \n",
    "# partition based on first parts\n",
    "#sort numerically decreasing on the third part\n",
    "!hdfs dfs -rm -r wordcount-output-sorted \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -input wordcount-output \\\n",
    "  -output wordcount-output-sorted \\\n",
    "    -mapper identityMapper.py \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "#   -mapper identityMapper.py \\\n",
    "\n",
    "#Can also use -mapper /bin/cat \\\n",
    "#-mapper /bin/cat \\\n",
    "#-reducer /bin/cat \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:43:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup         64 2016-02-02 10:43 wordcount-output-sorted/part-00000\n",
      "ls: `wordcount-output-sorted/part-00001': No such file or directory\n",
      "ls: `wordcount-output-sorted/part-00002': No such file or directory\n",
      "16/02/02 10:43:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Jimi\t3\t\n",
      "hello\t2\t\n",
      "Hello\t1\t\n",
      "is\t1\t\n",
      "jimi\t1\t\n",
      "this\t1\t\n",
      "three\t1\t\n",
      "who\t1\t\n",
      "cat: `wordcount-output-sorted/part-00001': No such file or directory\n",
      "cat: `wordcount-output-sorted/part-00002': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount-output-sorted/part* \n",
    "!hdfs dfs -cat wordcount-output-sorted/part* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:45:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Jimi\t3\t\n",
      "hello\t2\t\n",
      "Hello\t1\t\n",
      "is\t1\t\n",
      "jimi\t1\t\n",
      "this\t1\t\n",
      "three\t1\t\n",
      "who\t1\t\n"
     ]
    }
   ],
   "source": [
    "!rm -r wordcount-output-sorted\n",
    "!hdfs dfs -copyToLocal wordcount-output-sorted \n",
    "!head -n 30 wordcount-output-sorted/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count for Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 06:47:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:47:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted historical_tours.txt\n",
      "16/05/27 06:47:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:47:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 16 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 16:57 gutenberg-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:50 gutenberg-output-sorted\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:50 gutenberg-wordCount\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2016-05-27 06:47 historical_tours.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:02 ipAddresses\n",
      "-rw-r--r--   1 jshanahan supergroup        110 2016-02-02 10:23 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:23 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:46 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        400 2016-02-01 12:46 stockprice.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         56 2016-02-02 10:50 testWordCountInput.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:32 wordcount-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 10:43 wordcount-output-sorted\n",
      "16/05/27 06:47:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:47:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted testWordCountInput.txt\n",
      "16/05/27 06:47:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:48:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:48:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-wordCount\n",
      "16/05/27 06:48:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 06:48:04 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 06:48:04 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 06:48:04 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 06:48:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 06:48:04 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 06:48:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2110793833_0001\n",
      "16/05/27 06:48:05 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 06:48:05 INFO mapreduce.Job: Running job: job_local2110793833_0001\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Starting task: attempt_local2110793833_0001_m_000000_0\n",
      "16/05/27 06:48:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 06:48:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/historical_tours.txt:0+87483\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 06:48:05 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=1941/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: \n",
      "16/05/27 06:48:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: bufstart = 0; bufend = 108364; bufvoid = 104857600\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26159436(104637744); length = 54961/6553600\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=4182/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=5092/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=4467/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task:attempt_local2110793833_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Records R/W=4467/1\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task 'attempt_local2110793833_0001_m_000000_0' done.\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local2110793833_0001_m_000000_0\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Starting task: attempt_local2110793833_0001_r_000000_0\n",
      "16/05/27 06:48:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 06:48:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 06:48:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2955237a\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: attempt_local2110793833_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 06:48:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2110793833_0001_m_000000_0 decomp: 14910 len: 14914 to MEMORY\n",
      "16/05/27 06:48:05 INFO reduce.InMemoryMapOutput: Read 14910 bytes from map-output for attempt_local2110793833_0001_m_000000_0\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14910, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14910\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14898 bytes\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 14910 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 1 files, 14914 bytes from disk\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14898 bytes\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=1260/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task:attempt_local2110793833_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task attempt_local2110793833_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 06:48:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2110793833_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-wordCount/_temporary/0/task_local2110793833_0001_r_000000\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Records R/W=1260/1 > reduce\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task 'attempt_local2110793833_0001_r_000000_0' done.\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local2110793833_0001_r_000000_0\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Starting task: attempt_local2110793833_0001_r_000001_0\n",
      "16/05/27 06:48:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 06:48:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 06:48:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d73011a\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: attempt_local2110793833_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 06:48:05 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2110793833_0001_m_000000_0 decomp: 14722 len: 14726 to MEMORY\n",
      "16/05/27 06:48:05 INFO reduce.InMemoryMapOutput: Read 14722 bytes from map-output for attempt_local2110793833_0001_m_000000_0\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14722, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14722\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14712 bytes\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 14722 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 1 files, 14726 bytes from disk\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14712 bytes\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=1254/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task:attempt_local2110793833_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task attempt_local2110793833_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 06:48:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2110793833_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-wordCount/_temporary/0/task_local2110793833_0001_r_000001\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Records R/W=1254/1 > reduce\n",
      "16/05/27 06:48:05 INFO mapred.Task: Task 'attempt_local2110793833_0001_r_000001_0' done.\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local2110793833_0001_r_000001_0\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: Starting task: attempt_local2110793833_0001_r_000002_0\n",
      "16/05/27 06:48:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 06:48:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 06:48:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7ee9768f\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: attempt_local2110793833_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 06:48:05 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2110793833_0001_m_000000_0 decomp: 14394 len: 14398 to MEMORY\n",
      "16/05/27 06:48:05 INFO reduce.InMemoryMapOutput: Read 14394 bytes from map-output for attempt_local2110793833_0001_m_000000_0\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14394, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14394\n",
      "16/05/27 06:48:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14381 bytes\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 14394 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 1 files, 14398 bytes from disk\n",
      "16/05/27 06:48:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 06:48:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14381 bytes\n",
      "16/05/27 06:48:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: Records R/W=1222/1\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 06:48:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 06:48:06 INFO mapred.Task: Task:attempt_local2110793833_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 06:48:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 06:48:06 INFO mapred.Task: Task attempt_local2110793833_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 06:48:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2110793833_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-wordCount/_temporary/0/task_local2110793833_0001_r_000002\n",
      "16/05/27 06:48:06 INFO mapred.LocalJobRunner: Records R/W=1222/1 > reduce\n",
      "16/05/27 06:48:06 INFO mapred.Task: Task 'attempt_local2110793833_0001_r_000002_0' done.\n",
      "16/05/27 06:48:06 INFO mapred.LocalJobRunner: Finishing task: attempt_local2110793833_0001_r_000002_0\n",
      "16/05/27 06:48:06 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 06:48:06 INFO mapreduce.Job: Job job_local2110793833_0001 running in uber mode : false\n",
      "16/05/27 06:48:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 06:48:06 INFO mapreduce.Job: Job job_local2110793833_0001 completed successfully\n",
      "16/05/27 06:48:06 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=618696\n",
      "\t\tFILE: Number of bytes written=1772084\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=349932\n",
      "\t\tHDFS: Number of bytes written=73536\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1941\n",
      "\t\tMap output records=13741\n",
      "\t\tMap output bytes=108364\n",
      "\t\tMap output materialized bytes=44038\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=13741\n",
      "\t\tCombine output records=3736\n",
      "\t\tReduce input groups=3736\n",
      "\t\tReduce shuffle bytes=44038\n",
      "\t\tReduce input records=3736\n",
      "\t\tReduce output records=3736\n",
      "\t\tSpilled Records=7472\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=3\n",
      "\t\tTotal committed heap usage (bytes)=1241513984\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87483\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36548\n",
      "16/05/27 06:48:06 INFO streaming.StreamJob: Output directory: gutenberg-wordCount\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm historical_tours.txt \n",
    "!hdfs dfs -copyFromLocal WordCount/historical_tours.txt \n",
    "!hdfs dfs -ls\n",
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r gutenberg-wordCount\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -combiner WordCount/reducer.py \\\n",
    "   -input historical_tours.txt \\\n",
    "   -output gutenberg-wordCount  \\\n",
    "   -numReduceTasks 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:50:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:50:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-output-sorted\n",
      "16/02/02 10:50:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:50:54 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:50:54 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:50:54 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/02 10:50:54 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/02 10:50:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2068509560_0001\n",
      "16/02/02 10:50:54 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:50:54 INFO mapreduce.Job: Running job: job_local2068509560_0001\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00000:0+9528\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: Records R/W=986/1\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufend = 9528; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210456(104841824); length = 3941/6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Records R/W=986/1\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000000_0' done.\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00003:0+9073\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: Records R/W=931/1\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufend = 9073; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210676(104842704); length = 3721/6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Records R/W=931/1\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000001_0' done.\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00002:0+8977\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: Records R/W=921/1\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufend = 8977; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210716(104842864); length = 3681/6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Records R/W=921/1\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000002_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00001:0+8970\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: Records R/W=898/1\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufend = 8970; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210808(104843232); length = 3589/6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Records R/W=898/1\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000003_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_r_000000_0\n",
      "16/02/02 10:50:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5de1209c\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:50:55 INFO reduce.EventFetcher: attempt_local2068509560_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000002_0 decomp: 10821 len: 10825 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10821 bytes from map-output for attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10821, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10821\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000001_0 decomp: 10937 len: 10941 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10937 bytes from map-output for attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10937, inMemoryMapOutputs.size() -> 2, commitMemory -> 10821, usedMemory ->21758\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000000_0 decomp: 11502 len: 11506 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 11502 bytes from map-output for attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11502, inMemoryMapOutputs.size() -> 3, commitMemory -> 21758, usedMemory ->33260\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000003_0 decomp: 10768 len: 10772 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10768 bytes from map-output for attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10768, inMemoryMapOutputs.size() -> 4, commitMemory -> 33260, usedMemory ->44028\n",
      "16/02/02 10:50:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 43985 bytes\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 44028 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merging 1 files, 44026 bytes from disk\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 44012 bytes\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task attempt_local2068509560_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:50:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2068509560_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output-sorted/_temporary/0/task_local2068509560_0001_r_000000\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_r_000000_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_r_000000_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Job job_local2068509560_0001 running in uber mode : false\n",
      "16/02/02 10:50:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Job job_local2068509560_0001 completed successfully\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=620270\n",
      "\t\tFILE: Number of bytes written=2079832\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=128803\n",
      "\t\tHDFS: Number of bytes written=36548\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=3736\n",
      "\t\tMap output bytes=36548\n",
      "\t\tMap output materialized bytes=44044\n",
      "\t\tInput split bytes=464\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3736\n",
      "\t\tReduce shuffle bytes=44044\n",
      "\t\tReduce input records=3736\n",
      "\t\tReduce output records=3736\n",
      "\t\tSpilled Records=7472\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=2378694656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=36548\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36548\n",
      "16/02/02 10:50:55 INFO streaming.StreamJob: Output directory: gutenberg-output-sorted\n"
     ]
    }
   ],
   "source": [
    "#Sort the words in descreasing order of frequency, but break ties with alphabetical sorting\n",
    "!hdfs dfs -rm -r gutenberg-output-sorted\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -mapper identityMapper.py \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "   -input gutenberg-output \\\n",
    "   -output gutenberg-output-sorted  \\\n",
    "   -numReduceTasks 1 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:51:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup      36548 2016-02-02 10:50 gutenberg-output-sorted/part-00000\n",
      "16/02/02 10:51:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "States,\t6\n",
      "Starting\t3\n",
      "Start:\t1\n",
      "Start\t1\n",
      "Students\t1\n",
      "Station,\t2\n",
      "Standish\t2\n",
      "Square;\t1\n",
      "Swing\t2\n",
      "Sunday.\t1\n",
      "Sumner\t2\n",
      "Sullivan\t1\n",
      "Standish's\t1\n",
      "Tavern\"\t1\n",
      "Shows\t1\n",
      "Sheraton\t1\n",
      "Sewall\"\t1\n",
      "Sewall\t1\n",
      "Simmons\t2\n",
      "Sidney\t2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls \"gutenberg-output-sorted/part-*\"\n",
    "!rm -r gutenberg-output-sorted\n",
    "!hdfs dfs -copyToLocal \"gutenberg-output-sorted\" \n",
    "!head -n 20 gutenberg-output-sorted/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States,\t6\r\n",
      "Starting\t3\r\n",
      "Start:\t1\r\n",
      "Start\t1\r\n",
      "Students\t1\r\n",
      "Station,\t2\r\n",
      "Standish\t2\r\n",
      "Square;\t1\r\n",
      "Swing\t2\r\n",
      "Sunday.\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 gutenberg-output-sorted/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:40:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "windows.\t1\n",
      "winter.\t1\n",
      "wish\t2\n",
      "within\t6\n",
      "words\t1\n",
      "work.\t5\n",
      "would\t2\n",
      "wrote\t3\n",
      "your\t52\n",
      "yourself\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail  gutenberg-output/part-00000 |tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Address secondary sort and paritioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddresses.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddresses.txt\n",
    "11.12.1.2\n",
    "11.14.2.3\n",
    "11.11.4.1\n",
    "11.11.3.1\n",
    "11.11.6.1\n",
    "11.11.7.1\n",
    "11.12.1.1\n",
    "11.14.2.2\n",
    "11.12.2.5222\n",
    "11.9999999.2.5222\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:06:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 10:06:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/02 10:06:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:59 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:06:59 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:06:59 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:06:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 10:07:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 10:07:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local378757331_0001\n",
      "16/02/02 10:07:00 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:07:00 INFO mapreduce.Job: Running job: job_local378757331_0001\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Starting task: attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:07:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: bufstart = 0; bufend = 88; bufvoid = 104857600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task:attempt_local378757331_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task 'attempt_local378757331_0001_m_000000_0' done.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Starting task: attempt_local378757331_0001_r_000000_0\n",
      "16/02/02 10:07:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:07:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:07:00 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3eaf4e6f\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:07:00 INFO reduce.EventFetcher: attempt_local378757331_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:07:00 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local378757331_0001_m_000000_0 decomp: 104 len: 108 to MEMORY\n",
      "16/02/02 10:07:00 INFO reduce.InMemoryMapOutput: Read 104 bytes from map-output for attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 104, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->104\n",
      "16/02/02 10:07:00 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merged 1 segments, 104 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merging 1 files, 108 bytes from disk\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task:attempt_local378757331_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task attempt_local378757331_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:07:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_local378757331_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local378757331_0001_r_000000\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task 'attempt_local378757331_0001_r_000000_0' done.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local378757331_0001_r_000000_0\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Job job_local378757331_0001 running in uber mode : false\n",
      "16/02/02 10:07:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Job job_local378757331_0001 completed successfully\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210518\n",
      "\t\tFILE: Number of bytes written=752678\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=160\n",
      "\t\tHDFS: Number of bytes written=88\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=88\n",
      "\t\tMap output materialized bytes=108\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=108\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=727711744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88\n",
      "16/02/02 10:07:01 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom partitioner  and sort works locally and on AWS EM Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:23:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 10:23:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/02 10:23:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:23:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:23:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:23:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 10:23:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 10:23:48 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/02 10:23:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1055179505_0001\n",
      "16/02/02 10:23:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:23:49 INFO mapreduce.Job: Running job: job_local1055179505_0001\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+110\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: bufstart = 0; bufend = 121; bufvoid = 104857600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_m_000000_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000000_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b654ebd\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 44 len: 48 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 44 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 44, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->44\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 44 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 48 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000000\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000000_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000000_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000001_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5fa99f18\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000001_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000001\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000001_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000001_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000002_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1dc18346\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 101 len: 105 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 101 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 101, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->101\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 101 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 105 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000002_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000002\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000002_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000002_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Job job_local1055179505_0001 running in uber mode : false\n",
      "16/02/02 10:23:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Job job_local1055179505_0001 completed successfully\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422085\n",
      "\t\tFILE: Number of bytes written=1515073\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=440\n",
      "\t\tHDFS: Number of bytes written=193\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=121\n",
      "\t\tMap output materialized bytes=159\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=159\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1453326336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=121\n",
      "16/02/02 10:23:50 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "#Custom partitioner works \n",
    "# partition based on first parts\n",
    "#sort numerically decreasing on the third part\n",
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D map.output.key.field.separator=. \\\n",
    "  -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.job.reduces=3 \\\n",
    "  -input ipAddresses.txt \\\n",
    "  -output myOutputDirForIPAddresses \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:23:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.12.2.5222\t\n",
      "11.12.1.1\t\n",
      "11.12.1.2\t\n",
      "==========================\n",
      "16/02/02 10:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "==========================\n",
      "16/02/02 10:23:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.11.7.1\t\n",
      "11.11.6.1\t\n",
      "11.11.4.1\t\n",
      "11.11.3.1\t\n",
      "11.9999999.2.5222\t\n",
      "11.14.2.2\t\n",
      "11.14.2.3\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00000\n",
    "!echo \"==========================\"\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00001\n",
    "!echo \"==========================\"\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a job on AWS by launch an EMR Cluster\n",
    "** custom partition and sort works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run a job on AWS by launch an EMR Cluster\n",
    "#remote login to cluster\n",
    "\n",
    "#on AWS generate sample data file called ipAddresses.txt\n",
    "echo -e \"11.12.1.2\" > ipAddresses.txt\n",
    "echo -e \"11.14.2.3\" >> ipAddresses.txt\n",
    "echo -e \"11.11.4.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.7.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.6.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.5.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.4.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.9.1\" >> ipAddresses.txt\n",
    "echo -e \"11.12.9.1\" >> ipAddresses.txt\n",
    "echo -e \"11.14.2.2\" >> ipAddresses.txt\n",
    "echo -e \"11.12.2.5222\" >> ipAddresses.txt\n",
    "echo -e \"11.9999999.2.5222\" >> ipAddresses.txt\n",
    "\n",
    "\n",
    "#NOTE Hadoop is already running!\n",
    "#\n",
    "hdfs dfs -rm ipAddresses.txt \n",
    "hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "hdfs dfs -ls\n",
    "#no need to start hadoop on an EMR cluster. It is starts at cluster launch time\n",
    "# check by typing hdfs dfs -l\n",
    "hdfs dfs -rm -r myOutputDirForIPAddresses\n",
    "\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-0.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D map.output.key.field.separator=. \\\n",
    "  -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.job.reduces=3 \\\n",
    "  -input ipAddresses.txt \\\n",
    "  -output myOutputDirForIPAddresses \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "\n",
    "hdfs dfs -cat myOutputDirForIPAddresses/*\n",
    "    # This job failed as it did partition the records correctly (i.e., by column 1 and 2)\n",
    "# as required by \n",
    "#         mapred.text.key.partitioner.options=-k1,2 \n",
    "[hadoop@ip-172-31-15-64 ~]$ hdfs dfs -cat myOutputDirForIPAddresses/*\n",
    "11.12.9.1\t\n",
    "11.12.2.5222\t\n",
    "11.12.1.2\t\n",
    "11.11.9.1\t\n",
    "11.11.7.1\t\n",
    "11.11.6.1\t\n",
    "11.11.5.1\t\n",
    "11.11.4.1\t\n",
    "11.11.4.1\t\n",
    "11.14.2.3\t\n",
    "11.14.2.2\t\n",
    "11.9999999.2.5222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-241-02d4bae2e0e3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-241-02d4bae2e0e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scp ipAddresses.txt\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scp ipAddresses.txt\n",
    "!sort -d\".\" -k1,1 -k2,2 <ipAddresses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.12.1.2\r\n",
      "11.14.2.3\r\n",
      "11.11.4.1\r\n",
      "11.12.1.1\r\n",
      "11.14.2.2\r\n",
      "11.12.2.5222\r\n",
      "11.9999999.2.5222"
     ]
    }
   ],
   "source": [
    "!cat ipAddresses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:47:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:47:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 08:47:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:47:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 15 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 16:57 gutenberg-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:45 gutenberg-output-sorted\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:34 gutenberg-wordCount\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2016-02-02 08:34 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         80 2016-02-02 08:47 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:46 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        400 2016-02-01 12:46 stockprice.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         56 2016-02-02 08:34 testWordCountInput.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 18:09 wordcount-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:45 wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('.')\n",
    "    # get date from unix time\n",
    "    print '%s\\t%s\\t%s\\t%s mapper' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    print '%s.%s.%s.%s jimi' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.12.1.2 jimi\r\n",
      "11.14.2.3 jimi\r\n",
      "11.11.4.1 jimi\r\n",
      "11.12.1.1 jimi\r\n",
      "11.14.2.2 jimi\r\n",
      "11.12.2.5222 jimi\r\n",
      "11.9999999.2.5222 jimi\r\n"
     ]
    }
   ],
   "source": [
    "!cat ipAddresses.txt| python ipAddressReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count with a Custom partitioner step \n",
    "**To enable total sorting of the word count frequency when multiple reducers are used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapperWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapperWithPartitionTable.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "group1 = \"abcdefghijklm\"\n",
    "group2 = \"nopqrstuvwxyz\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        firstChar = word[0].lower()\n",
    "        if firstChar in group1:\n",
    "            print \"group1\\t%s\\t%d\" %(word, 1)\n",
    "        elif firstChar in group2:\n",
    "            print \"group2\\t%s\\t%d\" %(word, 1)\n",
    "        else:\n",
    "            print \"group3\\t%s\\t%d\" %(word, 1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "group1\tbar\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tlabs\t1\r\n",
      "group2\tquux\t1\r\n",
      "group2\tquux\t1\r\n",
      "group2\tst#ff\t1\r\n",
      "group3\t#funky\t1\r\n"
     ]
    }
   ],
   "source": [
    "#partition the word count records\n",
    "!echo \"foo foo quux labs foo #funky st#ff bar quux\" | python WordCount/mapperWithPartitionTable.py | sort -k1,1 -k2,2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducerWithPartitionKey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducerWithPartitionKey.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    group, key, value = line.split()   #one minor modification to process the parition key. I.e., drop it\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "#funky\t1\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n",
      "st#ff\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo #funky st#ff bar quux\" | python WordCount/mapperWithPartitionTable.py | \\\n",
    " sort -k1,1 -k2,2 | \\\n",
    " python WordCount/reducerWithPartitionKey.py |\\\n",
    " sort -k2,2nr -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:53:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:53:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:53:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: Running job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_m_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4dcd971d\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 70 len: 74 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 70 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 70, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->70\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 70 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 74 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000000\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d946c52\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000001_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38a63406\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 45 len: 49 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 45 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 45, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 45 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 49 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000002\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000002_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 running in uber mode : false\n",
      "16/02/01 12:53:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 completed successfully\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422129\n",
      "\t\tFILE: Number of bytes written=1498547\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=320\n",
      "\t\tHDFS: Number of bytes written=219\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=169\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=169\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1440743424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=101\n",
      "16/02/01 12:53:50 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "#Partition into 3 reducers (the first 2 fields are used as keys for partition)\n",
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "#In the above example, \"-D stream.map.output.field.separator=.\" specifies \".\" as the field separator \n",
    "#for the map outputs, and the prefix up to the fourth \".\" in a line will be the key and the rest of \n",
    "#the line (excluding the fourth \".\") will be the value. If a line has less than four \".\"s, then the \n",
    "#whole line will be the key and the value will be an empty Text object (like the one created by new Text(\"\")).\n",
    "\n",
    "#Similarly, you can use \"-D stream.reduce.output.field.separator=SEP\" and \"-D stream.num.reduce.output.fields=NUM\"\n",
    "#to specify the nth field separator in a line of the reduce outputs as the separator between the key and the value.\n",
    "\n",
    "#Similarly, you can specify \"stream.map.input.field.separator\" and \"stream.reduce.input.field.separator\" as \n",
    "#the input separator for Map/Reduce inputs. By default the separator is the tab character.\n",
    "\n",
    "#record 11.12.1.2 is treated as a key only with no value\n",
    "\n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:56:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:56:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:57:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:57:00 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:57:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:57:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/02/01 12:57:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: Running job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: numReduceTasks: 5\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressMapper.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_m_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@450849b9\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 22 len: 26 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 22 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 22 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 26 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000000\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c2c6e85\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 65 len: 69 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 65 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 65, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->65\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 65 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 69 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000001_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@449a3b50\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 30 len: 34 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 30 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 30, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->30\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 30 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 34 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000002\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000002_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6665f62e\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000003_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000003\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000003_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3b5a3cf4\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000004_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#5 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000004_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000004_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000004_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000004\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000004_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 running in uber mode : false\n",
      "16/02/01 12:57:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 completed successfully\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=635229\n",
      "\t\tFILE: Number of bytes written=2265774\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=480\n",
      "\t\tHDFS: Number of bytes written=588\n",
      "\t\tHDFS: Number of read operations=75\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=181\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=181\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=2205679616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172\n",
      "16/02/01 12:57:02 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapred.text.key.partitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper ipAddressMapper.py \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 5 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "# desired output \n",
    "11.11.4.1\n",
    "-----------\n",
    "11.12.1.1\n",
    "11.12.1.2\n",
    "-----------\n",
    "11.14.2.2\n",
    "11.14.2.3\n",
    "11.14.2.5222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:57:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 6 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         23 2016-02-01 12:57 myOutputDirForIPAddresses/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         72 2016-02-01 12:57 myOutputDirForIPAddresses/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         31 2016-02-01 12:57 myOutputDirForIPAddresses/part-00002\n",
      "-rw-r--r--   1 jshanahan supergroup         46 2016-02-01 12:57 myOutputDirForIPAddresses/part-00003\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/part-00004\n",
      "16/02/01 12:57:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.11.4.1 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.12.1.1 mapper jimi\t\n",
      "11.12.1.2 mapper jimi\t\n",
      "11.12.2.5222 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.9999999.2.5222 mapper jimi\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls myOutputDirForIPAddresses\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00000\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00001\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:08:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 10:08:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 10:08:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: Running job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufend = 108; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_m_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5748fba6\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000000\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@576ca50f\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000001_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50bda288\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000002\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000002_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 running in uber mode : false\n",
      "16/02/01 10:08:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 completed successfully\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=421940\n",
      "\t\tFILE: Number of bytes written=1498172\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=236\n",
      "\t\tHDFS: Number of bytes written=153\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=108\n",
      "\t\tMap output materialized bytes=138\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=138\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1333788672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=77\n",
      "16/02/01 10:08:45 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D map.output.key.field.separator=. \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:55:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:55:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:55:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: Running job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Starting task: attempt_local72918067_0001_m_000000_0\n",
      "16/02/01 12:55:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:55:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:55:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:55:36 WARN mapred.LocalJobRunner: job_local72918067_0001\n",
      "java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n",
      "Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1069)\n",
      "\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:607)\n",
      "\tat org.apache.hadoop.mapred.lib.IdentityMapper.map(IdentityMapper.java:43)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 running in uber mode : false\n",
      "16/02/01 12:55:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 failed with state FAILED due to: NA\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Counters: 0\n",
      "16/02/01 12:55:36 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "     -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "   -D map.output.key.field.separator=. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:09:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0\t11.12.1.2\n",
      "30\t11.12.1.1\n",
      "10\t11.14.2.3\n",
      "40\t11.14.2.2\n",
      "20\t11.11.4.1\n",
      "50\t11.14.2.5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Sort: Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockprice.txt\n",
    "Google,2015-08-06,647.32\n",
    "Apple,2015-08-01,117.83\n",
    "Facebook,2015-08-05,92.67\n",
    "Facebook,2015-08-01,90.96\n",
    "Oracle,2015-08-04,38.55\n",
    "Apple,2015-08-04,113.77\n",
    "Google,2015-08-05,677.95\n",
    "Facebook,2015-08-08,90.43\n",
    "Oracle,2015-08-03,35.78\n",
    "Apple,2015-08-11,110.09\n",
    "Oracle,2015-08-07,39.67\n",
    "Google,2015-08-09,656.63\n",
    "ABXXXX,2015-08-07,39.67\n",
    "ABXXXX,2015-08-09,656.63\n",
    "Google,2000-08-09,0\n",
    "ABXXXX,2015-08-08,6569999999999.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line\n",
    "    name, date, price= line.split(\",\")\n",
    "    # unix time is for secondary sort\n",
    "    unix_time = datetime.datetime.strptime(date, '%Y-%m-%d').strftime(\"%s\")\n",
    "    # output each record\n",
    "    print '%s\\t%s\\t%s' % (name, unix_time, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    name, unix_time, price = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    date = datetime.datetime.fromtimestamp(int(unix_time)).strftime('%Y-%m-%d')\n",
    "    print '%s\\t%s\\t%s' % (name, date, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2n   #Unix sort\n",
    "#WHY does it NOT work? It takes the whole as a key and sorts alphanumerically\n",
    "\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t965804400\t0\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#this is CORRECT\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#very different to the following line -k1,1 sort \n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice.txt\n",
      "16/02/01 12:46:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm stockprice.txt\n",
    "!hdfs dfs -copyFromLocal stockprice.txt /user/jshanahan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice\n",
      "16/02/01 12:46:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:42 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:46:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: Running job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/stockprice.txt:0+400\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceMapper.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufend = 416; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214336(104857344); length = 61/6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_m_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ebfefd\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 251 len: 255 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 251 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 251, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->251\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 251 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 255 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=9/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000000\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b31deb3\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 201 len: 205 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 201 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->201\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 205 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000001_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 running in uber mode : false\n",
      "16/02/01 12:46:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 completed successfully\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=317416\n",
      "\t\tFILE: Number of bytes written=1132117\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1200\n",
      "\t\tHDFS: Number of bytes written=623\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=16\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=416\n",
      "\t\tMap output materialized bytes=460\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16\n",
      "\t\tReduce shuffle bytes=460\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=16\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=781713408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=400\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=401\n",
      "16/02/01 12:46:44 INFO streaming.StreamJob: Output directory: stockprice\n"
     ]
    }
   ],
   "source": [
    "#NOTE \"-k1,1 -k2,2nr\" -k1,1  is redundanct\n",
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "        -D stream.num.map.output.key.fields=3 \\\n",
    "        -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "        -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "        -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "        -mapper stockPriceMapper.py \\\n",
    "        -reducer stockPriceReducer.py \\\n",
    "        -input stockprice.txt -output stockprice \\\n",
    "        -numReduceTasks 2 \\\n",
    "        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:46 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-02-01 12:46 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        179 2016-02-01 12:46 stockprice/part-00001\n",
      "16/02/01 12:46:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-04\t38.55\n",
      "Oracle\t2015-08-03\t35.78\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-08\t6569999999999.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n",
    "!hdfs dfs -cat stockprice/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:39:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:39:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-01-27 22:39 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         75 2016-01-27 22:39 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-01-27 22:39 stockprice/part-00001\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:40:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-03\t35.78\n",
      "Oracle\t2015-08-04\t38.55\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Sort from Tom White (Chapter 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SecondarySort/otherYears.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile SecondarySort/otherYears.txt\n",
    "0029029070999991903010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991904010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991907010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "002902907099999190610106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991906010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991906010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:23:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "copyFromLocal: `SecondarySort/1901': File exists\n",
      "copyFromLocal: `SecondarySort/1902': File exists\n",
      "copyFromLocal: `SecondarySort/otherYears.txt': File exists\n",
      "16/02/01 09:23:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 9 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-30 09:23 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         59 2016-01-31 20:01 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-31 20:36 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 output-secondarysort-streaming\n",
      "-rw-r--r--   1 jshanahan supergroup        365 2016-02-01 08:55 stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "#copy year 1901 and 1902 to HDFS\n",
    "#val[15:19], int(val[87:92]), val[92:93]\n",
    "# year, temp, q\n",
    "#002902907099999  1901  010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "#0029029070999991901010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "#0029029070999991901010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "\n",
    "\n",
    "!hdfs dfs -copyFromLocal SecondarySort\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_map.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_map.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-7. Map function for secondary sort in Python\n",
    "import re\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp, q) = (val[15:19], int(val[87:92]), val[92:93])\n",
    "    if temp == 9999:\n",
    "        sys.stderr.write(\"reporter:counter:Temperature,Missing,1\\n\")\n",
    "    elif re.match(\"[01459]\", q):\n",
    "        print \"%s\\t%s\" % (year, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_reduce.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-8. Reducer function for secondary sort in Python\n",
    "import sys\n",
    "last_group = None\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp) = val.split(\"\\t\")\n",
    "    group = year\n",
    "    if last_group != group:   #print the first record ONLY for each year; skip all other records for that year\n",
    "        print val\n",
    "        last_group = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\t-100\r\n"
     ]
    }
   ],
   "source": [
    "!cat SecondarySort/1902| python secondary_sort_map.py |sort|python secondary_sort_reduce.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:36:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted output-secondarysort-streaming\n",
      "16/02/01 09:36:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 09:36:11 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local806998882_0001\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_map.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171875/secondary_sort_map.py\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_reduce.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171876/secondary_sort_reduce.py\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: Running job: job_local806998882_0001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1902:0+888978\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63107; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188140(104752560); length = 26257/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1901:0+888190\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63794; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188144(104752576); length = 26253/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/otherYears.txt:0+1078\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 70; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@344c22e1\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 76239 len: 76243 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76239 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76239, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->76243\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76243 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76243 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6565/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000000\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6565/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@625b6997\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 38 len: 42 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 38 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 38, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->40\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 40, usedMemory ->42\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 42 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34986bf\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 76924 len: 76928 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76924 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76924, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->76924\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 50 len: 54 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 50 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 2, commitMemory -> 76924, usedMemory ->76974\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 76974, usedMemory ->76976\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 76951 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76976 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76976 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76960 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6568/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000002\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6568/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 running in uber mode : false\n",
      "16/02/01 09:36:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 completed successfully\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1273416\n",
      "\t\tFILE: Number of bytes written=3433695\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9779130\n",
      "\t\tHDFS: Number of bytes written=94\n",
      "\t\tHDFS: Number of read operations=72\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=18\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=13138\n",
      "\t\tMap output records=13136\n",
      "\t\tMap output bytes=126971\n",
      "\t\tMap output materialized bytes=153297\n",
      "\t\tInput split bytes=331\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11809\n",
      "\t\tReduce shuffle bytes=153297\n",
      "\t\tReduce input records=13136\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=26272\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=2905604096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTemperature\n",
      "\t\tMissing=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1778246\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/02/01 09:36:13 INFO streaming.StreamJob: Output directory: output-secondarysort-streaming\n"
     ]
    }
   ],
   "source": [
    "#To do a secondary sort in Streaming, we can take advantage of a couple of library classes\n",
    "#that Hadoop provides. Here’s the driver that we can use to do a secondary sort:\n",
    "!hdfs dfs -rm -r output-secondarysort-streaming\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,1nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py \\\n",
    "    -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:25:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 09:24 output-secondarysort-streaming/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup          9 2016-02-01 09:24 output-secondarysort-streaming/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         18 2016-02-01 09:24 output-secondarysort-streaming/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         27 2016-02-01 09:24 output-secondarysort-streaming/part-00002\n",
      "16/02/01 09:25:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1902\t244\n",
      "1903\t-78\n",
      "1906\t-72\n",
      "1901\t317\n",
      "1904\t-72\n",
      "1907\t-72\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output-secondarysort-streaming/*\n",
    "!hdfs dfs -cat output-secondarysort-streaming/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/30 09:12:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 09:13:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#stop hadoop/yarn cluster on my local machine (in this case)\n",
    "#!alias hstop=\"/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\"\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chris Caldwell Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chris Caldwell Homework\n",
    "# make a work directory on my local machine\n",
    "!mkdir ChrisC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#getting random ints for indexes\n",
    "indices = random.sample(range(1, 100), 10)\n",
    "\n",
    "#writing out the file\n",
    "intIndFile = open('ChrisC/hw2_1.txt', 'w')\n",
    "#for x in indices:\n",
    "#intIndFile.write(\"{}\\t\".format(x))\n",
    "intIndFile.write(\"\\t\\n\".join(str(x) for x in indices)+'\\t')\n",
    "intIndFile.close()\n",
    "\n",
    "#delete if previous existing file\n",
    "#!hdfs dfs -rm  /user/ubuntu/hw2_1.txt\n",
    "#moving file to hdfs\n",
    "#!hdfs dfs -put hw2_1.txt /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\t\r\n",
      "62\t\r\n",
      "64\t\r\n",
      "59\t\r\n",
      "27\t\r\n",
      "41\t\r\n",
      "18\t\r\n",
      "19\t\r\n",
      "90\t\r\n",
      "94\t"
     ]
    }
   ],
   "source": [
    "!head ChrisC/hw2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ChrisC/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChrisC/mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Input from standard in\n",
    "for ent in sys.stdin:\n",
    "    print(ent.strip())\n",
    "    # split entity into key / value\n",
    "    #key, val = ent.split('\\t')    \n",
    "    #print(\"{}\\t{}\".format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChrisC/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChrisC/reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "#print(\"109\\t\")\n",
    "#Input from standard in\n",
    "for ent in sys.stdin:\n",
    "    print(ent.strip())\n",
    "    # split entity into key / value\n",
    "    #key, val = ent.split('\\t')    \n",
    "    #print(\"{}\\t{}\".format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x ChrisC/mapper.py\n",
    "!chmod a+x ChrisC/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n",
      "3\r\n",
      "5\r\n",
      "8\r\n",
      "9\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"1\\t\\n5\\t\\n3\\t\\n8\\t\\n9\\t\" | python ChrisC/mapper.py |sort -k1,1 | python ChrisC/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:23:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:23:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Make directory on HDFS and copy the data file up there\n",
    "!hdfs dfs -mkdir ChrisC\n",
    "!hdfs dfs -put ChrisC/hw2_1.txt ChrisC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:36:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:36:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw2_1_out\n",
      "16/05/28 19:36:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:36:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/28 19:36:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/28 19:36:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/28 19:36:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 19:36:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/28 19:36:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local621821506_0001\n",
      "16/05/28 19:36:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/28 19:36:11 INFO mapreduce.Job: Running job: job_local621821506_0001\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Starting task: attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/28 19:36:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ChrisC/hw2_1.txt:0+39\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ChrisC/mapper.py]\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: \n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Spilling map output\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: bufstart = 0; bufend = 40; bufvoid = 104857600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task:attempt_local621821506_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task 'attempt_local621821506_0001_m_000000_0' done.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Starting task: attempt_local621821506_0001_r_000000_0\n",
      "16/05/28 19:36:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/28 19:36:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/28 19:36:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@150b04f3\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/28 19:36:11 INFO reduce.EventFetcher: attempt_local621821506_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/28 19:36:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local621821506_0001_m_000000_0 decomp: 62 len: 66 to MEMORY\n",
      "16/05/28 19:36:11 INFO reduce.InMemoryMapOutput: Read 62 bytes from map-output for attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 62, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->62\n",
      "16/05/28 19:36:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 62 bytes to disk to satisfy reduce memory limit\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merging 1 files, 66 bytes from disk\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ChrisC/reducer.py]\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task:attempt_local621821506_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task attempt_local621821506_0001_r_000000_0 is allowed to commit now\n",
      "16/05/28 19:36:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_local621821506_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/hw2_1_out/_temporary/0/task_local621821506_0001_r_000000\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task 'attempt_local621821506_0001_r_000000_0' done.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local621821506_0001_r_000000_0\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Job job_local621821506_0001 running in uber mode : false\n",
      "16/05/28 19:36:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Job job_local621821506_0001 completed successfully\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210436\n",
      "\t\tFILE: Number of bytes written=749278\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=78\n",
      "\t\tHDFS: Number of bytes written=40\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=40\n",
      "\t\tMap output materialized bytes=66\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=66\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=726663168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40\n",
      "16/05/28 19:36:12 INFO streaming.StreamJob: Output directory: hw2_1_out\n"
     ]
    }
   ],
   "source": [
    "#!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "!hdfs dfs -rm -r hw2_1_out\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper ChrisC/mapper.py \\\n",
    "   -reducer ChrisC/reducer.py \\\n",
    "   -input ChrisC/hw2_1.txt -output hw2_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:35:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18\t\n",
      "19\t\n",
      "27\t\n",
      "41\t\n",
      "59\t\n",
      "62\t\n",
      "64\t\n",
      "77\t\n",
      "90\t\n",
      "94\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat hw2_1_out/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
