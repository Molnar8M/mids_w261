{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW3---Supplement\" data-toc-modified-id=\"Assignment---HW3---Supplement-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW3 - Supplement</a></div><div class=\"lev2 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSION\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSION-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSION</a></div><div class=\"lev2 toc-item\"><a href=\"#CONFIGURATION\" data-toc-modified-id=\"CONFIGURATION-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>CONFIGURATION</a></div><div class=\"lev2 toc-item\"><a href=\"#DATASETS\" data-toc-modified-id=\"DATASETS-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>DATASETS</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.4.-Computing-Confidence---Pairs\" data-toc-modified-id=\"HW3.4.-Computing-Confidence---Pairs-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>HW3.4. Computing Confidence - Pairs</a></div><div class=\"lev3 toc-item\"><a href=\"#Count-&amp;-Sort-Pairs\" data-toc-modified-id=\"Count-&amp;-Sort-Pairs-151\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Count &amp; Sort Pairs</a></div><div class=\"lev3 toc-item\"><a href=\"#Sanity-check-for-support.\" data-toc-modified-id=\"Sanity-check-for-support.-152\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Sanity check for support.</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW3 - Supplement\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "\n",
    "## INSTRUCTIONS for SUBMISSION\n",
    "\n",
    "This homework can be completed locally on your computer. __Please submit your notebook to your classroom github repository 24 hours prior to the next live session.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION\n",
    "Before starting your homework, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.13 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n"
     ]
    }
   ],
   "source": [
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an HDFS directory for this assignment\n",
    "!hdfs dfs -mkdir hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Save yourself some typing by defining global variables for paths we'll reuse frequently (like the hadoop-streaming jar file and the HDFS directory for your output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HDFS_DIR = \"/user/root/hw3\"\n",
    "HOME_DIR = os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Fix chrome formatting. _The cell below implements a quick hack based on [this stackoverflow thread](http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line) to fix [this known issue](https://github.com/mathjax/MathJax/issues/1300) with Mathjax formatting in Chrome (a rounding issue adds a border to the right of mathjax markup)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS\n",
    "For this homework we be using a few different datasets. These include:\n",
    "* __The Consumer Complaints Dataset [available here](https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0) -__ _consists of diverse consumer complaints which have been reporte across the United States regarding various types of loans. We'll use this in HW 3.1 to learn about Hadoop Counters and then in HW 3.2 where we count words and compute relative frequencies._\n",
    "\n",
    "* __The Product Purchase Dataset [available here](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0) -__ _consists of data about products sold online and customer browsing behavior. We'll use this data in 3.3 to perform a shopping cart analysis._\n",
    "\n",
    "Follow the directions below to load/create each of these datasets. You may want to familiarize yourself with their contents before proceeding to the homework questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Consumer_Complaints.csv`__  \n",
    "The dataset consists of records of the form:  \n",
    ">Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Download the csv file to your current directory using the dropbox link provided above. You can do this manually or with `curl` or `wget` as we did in HW2. Then use the following cell to put the data into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 48.5M  100 48.5M    0     0  3644k      0  0:00:13  0:00:13 --:--:-- 4355k --:--:--  0:00:01 --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# Download the data and remove the header\n",
    "!curl -L -O  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv\n",
    "!head -n 1 Consumer_Complaints.csv > CC_header.csv\n",
    "!tail -n +2 Consumer_Complaints.csv > CC_records.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\r\n",
      "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first two lines\n",
    "!head -n 2 Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS - adjust path or filename as needed\n",
    "!hdfs dfs -put CC_records.csv {HDFS_DIR}/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`ProductPurchaseData.txt`__  \n",
    "Notes about the data format:\n",
    "> Each line in this dataset represents a browsing session of a customer. On each line, each string of 8 characters represents the id of an item browsed during that session. For example:  \n",
    "`FRO11987 ELE17451 ELE89019 SNA90258 GRO99222`  \n",
    "> would represent a single customer's browsing session in which they viewed 5 products.  \n",
    "\n",
    "Download the csv file to your current directory using the dropbox link provided above. You can do this manually or with `curl` or `wget` as we did in HW2. Take a look at the first few lines of the file, then use the following cell to put the data into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3377k  100 3377k    0     0  1621k      0  0:00:02  0:00:02 --:--:-- 4307k\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "!curl -L -O  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \r\n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \r\n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \r\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the data\n",
    "!head -n 5 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS - adjust path or filename if needed\n",
    "!hdfs dfs -put ProductPurchaseData.txt {HDFS_DIR}/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. Computing Confidence - Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "The relative frequency of a bigram (AKA itemset of size 2) $(w_i, w_j)$ relative to its first word $w_i$ is the number of occurrences of the pair $(w_i, w_j)$ related to the number of occurrences of the word $w_i$. For instance, if the word \"euro\" followed by the word \"crisis\" occurs 10 times and the word \"euro\" occurs 20 times in total, we say that the relative frequency of the pair (\"euro\",\"crisis\") is $\\frac{10}{20}$ = 0.5.\n",
    "\n",
    "$$ f(B|A) = \\frac{count(A,B)}{count(A)} = \\frac{count(A,B)}{\\sum_{B'} count(A,B')}$$\n",
    "\n",
    "__In association rule mining this is refered to as confidence. __   \n",
    "__Ref: https://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf pg.3-4__\n",
    "\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and confidence in decreasing order of confidence for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Feel free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner, and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, confidence\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), and break ties in confidence (between pairs, if any exist) by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pairsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsMapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "############################################################\n",
    "# Using order inversion, we are able to count the total\n",
    "# number of the first word in each pair to send to the reducer \n",
    "# the relative frequencies.\n",
    "############################################################\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    total += 1\n",
    "    for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "        print '%s\\t%s\\t%s' % (subset[0], subset[1], 1)\n",
    "    for word in words:\n",
    "        print '%s\\t%s\\t%s' % (word,\"!\", 1)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsReducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "\n",
    "cur_key = None\n",
    "cur_key_count = 0\n",
    "\n",
    "cur_pair = None\n",
    "cur_pair_count = 0\n",
    "\n",
    "support_count = 100\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    A,B,count = line.split(\"\\t\")\n",
    "    \n",
    "    if A == cur_key:\n",
    "      if B == \"!\":\n",
    "        cur_key_count += int(count)\n",
    "        \n",
    "      elif A+\"-\"+B == cur_pair:\n",
    "        cur_pair_count += int(count)\n",
    "    \n",
    "      else:\n",
    "        if cur_pair:\n",
    "          if cur_pair_count >= support_count:\n",
    "              print '%s\\t%s\\t%s' % (cur_pair,cur_pair_count,str(cur_pair_count/cur_key_count))\n",
    "        \n",
    "        cur_pair = A+\"-\"+B\n",
    "        cur_pair_count = int(count)\n",
    "        \n",
    "    else:\n",
    "      cur_key = A\n",
    "      cur_key_count = int(count)\n",
    "\n",
    "        \n",
    "if cur_pair_count >= support_count:\n",
    "      print '%s\\t%s\\t%s' % (cur_pair,cur_pair_count,str(cur_pair_count/cur_key_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count & Sort Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - koza supergroup          0 2017-09-16 17:07 /user/koza/HW2\r\n",
      "drwxr-xr-x   - koza supergroup          0 2017-07-19 09:42 /user/koza/graph\r\n",
      "drwxr-xr-x   - koza supergroup          0 2017-07-19 09:41 /user/koza/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/koza/hw3\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.4\n",
    "!hdfs dfs -mkdir /user/koza/hw3/3.4/pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put  ProductPurchaseData.txt /user/koza/hw3/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - koza supergroup          0 2017-09-23 16:31 /user/koza/hw3/3.4\r\n",
      "-rw-r--r--   1 koza supergroup    3458517 2017-09-23 16:41 /user/koza/hw3/ProductPurchaseData.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/09/23 17:36:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.4/pairs\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar808882282825266836/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5542986570983811443.jar tmpDir=null\n",
      "17/09/23 17:36:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/23 17:36:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/23 17:36:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/23 17:36:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/23 17:36:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506180544228_0021\n",
      "17/09/23 17:36:20 INFO impl.YarnClientImpl: Submitted application application_1506180544228_0021\n",
      "17/09/23 17:36:20 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1506180544228_0021/\n",
      "17/09/23 17:36:20 INFO mapreduce.Job: Running job: job_1506180544228_0021\n",
      "17/09/23 17:36:26 INFO mapreduce.Job: Job job_1506180544228_0021 running in uber mode : false\n",
      "17/09/23 17:36:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/23 17:36:36 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/09/23 17:36:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/23 17:36:52 INFO mapreduce.Job:  map 100% reduce 62%\n",
      "17/09/23 17:36:55 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "17/09/23 17:36:58 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "17/09/23 17:37:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/23 17:37:00 INFO mapreduce.Job: Job job_1506180544228_0021 completed successfully\n",
      "17/09/23 17:37:00 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=64375512\n",
      "\t\tFILE: Number of bytes written=129116153\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462835\n",
      "\t\tHDFS: Number of bytes written=49974\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27126\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16128\n",
      "\t\tTotal time spent by all map tasks (ms)=27126\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16128\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=27126\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16128\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=33310728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19805184\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2914838\n",
      "\t\tMap output bytes=58545830\n",
      "\t\tMap output materialized bytes=64375518\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=889687\n",
      "\t\tReduce shuffle bytes=64375518\n",
      "\t\tReduce input records=2914838\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=5829676\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=971505664\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49974\n",
      "17/09/23 17:37:00 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.4/pairs\n",
      "17/09/23 17:37:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/hw3/3.4/pairs_sorted\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar1838993432900918183/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5883330935041327255.jar tmpDir=null\n",
      "17/09/23 17:37:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/23 17:37:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/23 17:37:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/23 17:37:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/23 17:37:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506180544228_0022\n",
      "17/09/23 17:37:06 INFO impl.YarnClientImpl: Submitted application application_1506180544228_0022\n",
      "17/09/23 17:37:06 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1506180544228_0022/\n",
      "17/09/23 17:37:06 INFO mapreduce.Job: Running job: job_1506180544228_0022\n",
      "17/09/23 17:37:12 INFO mapreduce.Job: Job job_1506180544228_0022 running in uber mode : false\n",
      "17/09/23 17:37:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/23 17:37:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/23 17:37:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/23 17:37:23 INFO mapreduce.Job: Job job_1506180544228_0022 completed successfully\n",
      "17/09/23 17:37:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=53982\n",
      "\t\tFILE: Number of bytes written=469571\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=54286\n",
      "\t\tHDFS: Number of bytes written=51308\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6656\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2733\n",
      "\t\tTotal time spent by all map tasks (ms)=6656\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2733\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6656\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2733\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8173568\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3356124\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=51308\n",
      "\t\tMap output materialized bytes=53988\n",
      "\t\tInput split bytes=216\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=53988\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=128\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=836763648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=54070\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51308\n",
      "17/09/23 17:37:23 INFO streaming.StreamJob: Output directory: /user/koza/hw3/3.4/pairs_sorted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Get counts:\n",
    "start = time.time()\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/pairs\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -files pairsMapper.py,pairsReducer.py \\\n",
    "    -mapper pairsMapper.py \\\n",
    "    -reducer pairsReducer.py \\\n",
    "    -input /user/koza/hw3/ProductPurchaseData.txt -output /user/koza/hw3/3.4/pairs\n",
    "end = time.time()\n",
    "countTime = end-start\n",
    "\n",
    "\n",
    "\n",
    "# After calculating all the counts, We can do a secondary sort:\n",
    "start = time.time()\n",
    "!hdfs dfs -rm -r /user/koza/hw3/3.4/pairs_sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -D stream.num.map.output.key.field=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input /user/koza/hw3/3.4/pairs/* -output /user/koza/hw3/3.4/pairs_sorted\n",
    "\n",
    "end = time.time()\n",
    "sortTime = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  Time to count and sort pairs  --------\n",
      "\n",
      "Count time:\t 44.506754 sec\n",
      "Sort time:\t 22.396491 sec\n",
      "Total time:\t 66.903245 sec \n"
     ]
    }
   ],
   "source": [
    "print \"-------  Time to count and sort pairs  --------\\n\"\n",
    "print \"Count time:\\t %f sec\\nSort time:\\t %f sec\\nTotal time:\\t %f sec \" % (countTime,sortTime,countTime+sortTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Most frequent pairs --------------\n",
      "\n",
      "\n",
      "Pair                Support count     Confidence \n",
      "\n",
      "DAI62779-ELE17451\t1592\t0.238788060597\t\n",
      "FRO40251-SNA80324\t1412\t0.363823756764\t\n",
      "DAI75645-FRO40251\t1254\t0.458333333333\t\n",
      "FRO40251-GRO85051\t1213\t0.312548312291\t\n",
      "DAI62779-GRO73461\t1139\t0.170841457927\t\n",
      "DAI75645-SNA80324\t1130\t0.413011695906\t\n",
      "DAI62779-FRO40251\t1070\t0.160491975401\t\n",
      "DAI62779-SNA80324\t923\t0.138443077846\t\n",
      "DAI62779-DAI85309\t918\t0.137693115344\t\n",
      "ELE32164-GRO59710\t911\t0.31953700456\t\n",
      "DAI62779-DAI75645\t882\t0.132293385331\t\n",
      "FRO40251-GRO73461\t882\t0.227261015202\t\n",
      "DAI62779-ELE92920\t877\t0.131543422829\t\n",
      "FRO40251-FRO92469\t835\t0.215150734347\t\n",
      "DAI62779-ELE32164\t832\t0.124793760312\t\n",
      "DAI75645-GRO73461\t712\t0.260233918129\t\n",
      "DAI43223-ELE32164\t711\t0.551162790698\t\n",
      "DAI62779-GRO30386\t709\t0.106344682766\t\n",
      "ELE17451-FRO40251\t697\t0.179870967742\t\n",
      "DAI85309-ELE99737\t659\t0.287396423899\t\n",
      "DAI62779-ELE26917\t650\t0.0974951252437\t\n",
      "GRO21487-GRO73461\t631\t0.298345153664\t\n",
      "DAI62779-SNA45677\t604\t0.0905954702265\t\n",
      "ELE17451-SNA80324\t597\t0.154064516129\t\n",
      "DAI62779-GRO71621\t595\t0.0892455377231\t\n",
      "DAI62779-SNA55762\t593\t0.0889455527224\t\n",
      "DAI62779-DAI83733\t586\t0.0878956052197\t\n",
      "ELE17451-GRO73461\t580\t0.149677419355\t\n",
      "GRO73461-SNA80324\t562\t0.156024430872\t\n",
      "DAI62779-GRO59710\t561\t0.0841457927104\t\n",
      "DAI62779-FRO80039\t550\t0.0824958752062\t\n",
      "DAI75645-ELE17451\t547\t0.199926900585\t\n",
      "DAI62779-SNA93860\t537\t0.0805459727014\t\n",
      "DAI55148-DAI62779\t526\t0.539487179487\t\n",
      "DAI43223-GRO59710\t512\t0.396899224806\t\n",
      "ELE17451-ELE32164\t511\t0.131870967742\t\n",
      "DAI62779-SNA18336\t506\t0.0758962051897\t\n",
      "ELE32164-GRO73461\t486\t0.170466502981\t\n",
      "DAI62779-FRO78087\t482\t0.0722963851807\t\n",
      "DAI85309-ELE17451\t482\t0.210204971653\t\n",
      "DAI62779-GRO94758\t479\t0.0718464076796\t\n",
      "DAI62779-GRO21487\t471\t0.0706464676766\t\n",
      "GRO85051-SNA80324\t471\t0.387973640857\t\n",
      "ELE17451-GRO30386\t468\t0.120774193548\t\n",
      "FRO85978-SNA95666\t463\t0.241397288843\t\n",
      "DAI62779-FRO19221\t462\t0.0692965351732\t\n",
      "DAI62779-GRO46854\t461\t0.0691465426729\t\n",
      "DAI43223-DAI62779\t459\t0.355813953488\t\n",
      "ELE92920-SNA18336\t455\t0.380116959064\t\n",
      "DAI88079-FRO40251\t446\t0.986725663717\t\n",
      "\n",
      "----- Least frequent but not less than 100 ------\n",
      "\n",
      "FRO40251-GRO50832\t100\t0.0257665550116\t\n",
      "FRO40251-GRO56989\t100\t0.0257665550116\t\n",
      "FRO78087-GRO30386\t100\t0.0653167864141\t\n",
      "FRO78087-GRO94758\t100\t0.0653167864141\t\n",
      "FRO80039-GRO64900\t100\t0.0447828034035\t\n",
      "GRO38814-SNA93860\t100\t0.0739644970414\t\n",
      "GRO46854-SNA66583\t100\t0.0569476082005\t\n",
      "GRO59710-SNA93860\t100\t0.0499001996008\t\n",
      "GRO64900-SNA45677\t100\t0.0956022944551\t\n",
      "GRO73461-GRO88511\t100\t0.0277623542476\t\n"
     ]
    }
   ],
   "source": [
    "print \"\\n------------- Most frequent pairs --------------\\n\"\n",
    "print \"\\nPair                Support count     Confidence \\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.4/pairs_sorted/part-00000 | head -n 50\n",
    "print \"\\n----- Least frequent but not less than 100 ------\\n\"\n",
    "!hdfs dfs -cat /user/koza/hw3/3.4/pairs_sorted/part-00000 | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check for support.\n",
    "If the support (1592) divided by the count of 'DAI62779' == 0.238788060597, then algorithm is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6667\r\n"
     ]
    }
   ],
   "source": [
    "!grep 'DAI62779' ProductPurchaseData.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23878806059697016"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "1592/6667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Report the compute time for the Pairs job:*     \n",
    "\n",
    "```\n",
    "Count time:\t 44.506754 sec\n",
    "Sort time:\t 22.396491 sec\n",
    "Total time:\t 66.903245 sec  \n",
    "```\n",
    "\n",
    "*Describe the computational setup used:*    \n",
    "   \n",
    "MacBook Pro, 2.5 GHz Intel Core i7  (4 Cores) \n",
    "16 GB 1600 MHz DDR3   \n",
    "\n",
    "Launched map tasks=2   \n",
    "Launched reduce tasks=1   \n",
    "\n",
    "![\"counters 3.4\"](http://candpgeneration.com/images/counters3.4.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "901px",
    "left": "0px",
    "right": "20px",
    "top": "106px",
    "width": "486px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
