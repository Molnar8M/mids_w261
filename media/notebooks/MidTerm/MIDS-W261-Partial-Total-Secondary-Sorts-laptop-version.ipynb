{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDS UC Berkeley, Machine Learning at Scale   \n",
    " \n",
    "__W261__ Summer 2016    \n",
    "__Week 2__: Total Sort, Partial Sort, and Secondary Sort using Hadoop Streaming     \n",
    "\n",
    "__James G. Shanahan__\n",
    "\n",
    "James.Shanahan@gmail.com  \n",
    "MIDS w261 Machine Learning at Scale September 15, 2014\n",
    "Data used\n",
    "\n",
    "Self-contained datesets generated using code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "1. Generate numbers\n",
    "2. Single Reducer Sort\n",
    "3. Partial Sort\n",
    "4. Total Order Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hadoop will always give a total sort on the key (i.e., key part of the key-value pairs produced by the mappers) when using just one reducer.\n",
    "*  When using multiple reducers Hadoop will by default give you a partial sort (i.e., all records within a partition will be sorted by the key (i.e., key part of the key-value pairs produced by the mappers)).\n",
    "*  To achieve a total sort one needs to write a customer mapper to to prepend a partition key to each record,  and then do a secondary sort or the resulting records (This can be done with ONE map-reduce job)\n",
    "\n",
    "Hadoop Streaming provides an extensive command line interface for doing sorts. It builds on the Unix Sort command.\n",
    "\n",
    "This notebook provides examples of the following:\n",
    "\n",
    "* Total Sort using Unix Sort command\n",
    "* Total Sort using Hadoop Streaming with a single reducer mapreduce job\n",
    "* Partial Sort using Hadoop Streaming with a multiple reducer mapreduce job\n",
    "* Secondary Sort\n",
    "* Total Sort using Hadoop Streaming with a multiple reducer mapreduce job (Single job)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data records with column 1 being the primary key of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: SortData: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%mkdir SortData\n",
    "%mkdir SortCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SortCode/generate_numbers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SortCode/generate_numbers.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import random\n",
    "random.seed(9001)\n",
    "N = 13\n",
    "for n in range(N):\n",
    "    print random.randint(0,N),\"\\t\",random.randint(0,N)\n",
    "    \n",
    "print \"2\\tA Group\"\n",
    "print \"2\\tB Group\"\n",
    "print \"2\\tC Group\"\n",
    "print \"2\\tD Group\"\n",
    "print \"6\\tA Group\"\n",
    "print \"6\\tC Group\"\n",
    "print \"6\\tB Group\"\n",
    "print \"6\\tD Group\"\n",
    "print \"3\\tA Group\"\n",
    "print \"3\\tC Group\"\n",
    "print \"3\\tB Group\"\n",
    "print \"3\\tD Group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      25 SortData/thirtyNumbersDataSet.txt\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x SortCode/generate_numbers.py;\n",
    "!./SortCode/generate_numbers.py > SortData/thirtyNumbersDataSet.txt\n",
    "!wc -l SortData/thirtyNumbersDataSet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t2\r\n",
      "6 \t6\r\n",
      "6 \t5\r\n",
      "7 \t9\r\n",
      "6 \t5\r\n",
      "8 \t10\r\n",
      "3 \t6\r\n",
      "0 \t4\r\n",
      "8 \t6\r\n",
      "8 \t4\r\n",
      "2\tA Group\r\n",
      "2\tB Group\r\n",
      "2\tC Group\r\n",
      "2\tD Group\r\n",
      "6\tA Group\r\n",
      "6\tC Group\r\n",
      "6\tB Group\r\n",
      "6\tD Group\r\n",
      "3\tA Group\r\n",
      "3\tC Group\r\n",
      "3\tB Group\r\n",
      "3\tD Group\r\n"
     ]
    }
   ],
   "source": [
    "cat SortData/thirtyNumbersDataSet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "16/05/31 14:57:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "16/05/31 14:57:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/31 14:57:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-namenode-JAMES-SHANAHANs-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-datanode-JAMES-SHANAHANs-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-secondarynamenode-JAMES-SHANAHANs-MacBook-Pro.local.out\n",
      "16/05/31 14:58:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-resourcemanager-JAMES-SHANAHANs-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-nodemanager-JAMES-SHANAHANs-MacBook-Pro.local.out\n"
     ]
    }
   ],
   "source": [
    "#stop old cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\n",
    "#hstart Cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh; /usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 14:58:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2015-02-26 21:03 /user/jshanahan/gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 /user/jshanahan/historical_tours.txt\n",
      "-------\n",
      "16/05/31 14:58:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2015-02-26 21:03 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls  /user/jshanahan\n",
    "!echo \"-------\"\n",
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 14:58:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `Sort': No such file or directory\n",
      "16/05/31 14:58:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/31 14:58:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `Sort': File exists\n",
      "16/05/31 14:58:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# put file in hdfs\n",
    "!hdfs dfs -rm -r Sort\n",
    "!hdfs dfs -mkdir Sort\n",
    "!hdfs dfs -mkdir Sort\n",
    "!hdfs dfs -put SortData/thirtyNumbersDataSet.txt Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Sort using Unix Sort command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============UNSorted============\n",
      "16/05/31 14:59:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 \t3\n",
      "7 \t7\n",
      "8 \t7\n",
      "9 \t12\n",
      "8 \t7\n",
      "10 \t13\n",
      "4 \t8\n",
      "1 \t6\n",
      "10 \t8\n",
      "10 \t5\n",
      "11 \t3\n",
      "12 \t4\n",
      "2 \t8\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "3\tA Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "=================================\n",
      "16/05/31 14:59:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "11 \t3\n",
      "10 \t13\n",
      "10 \t5\n",
      "10 \t8\n",
      "9 \t12\n",
      "8 \t7\n",
      "8 \t7\n",
      "7 \t7\n",
      "6\tA Group\n",
      "6\tB Group\n",
      "6\tC Group\n",
      "6\tD Group\n",
      "4 \t8\n",
      "3\tA Group\n",
      "3\tB Group\n",
      "3\tC Group\n",
      "3\tD Group\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "2 \t8\n",
      "1 \t6\n",
      "0 \t3\n"
     ]
    }
   ],
   "source": [
    "# Total sort of the column 1 (numeric reverse) via command line\n",
    "!echo \"=============UNSorted============\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt \n",
    "!echo \"=================================\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt | sort -k1,1nr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Sort using Single Reducer MapReduce Job\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SortCode/identityMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SortCode/identityFunction.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    key,value = line.split(\"\\t\", 1)\n",
    "    print '%s\\t%s' % (key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/31 14:59:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `Sort/Output': No such file or directory\n",
      "16/05/31 14:59:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/31 14:59:29 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/31 14:59:29 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/31 14:59:29 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/31 14:59:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/31 14:59:30 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/31 14:59:30 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/31 14:59:30 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/31 14:59:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1735338398_0001\n",
      "16/05/31 14:59:31 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: Starting task: attempt_local1735338398_0001_m_000000_0\n",
      "16/05/31 14:59:31 INFO mapreduce.Job: Running job: job_local1735338398_0001\n",
      "16/05/31 14:59:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/31 14:59:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/thirtyNumbersDataSet.txt:0+192\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/31 14:59:31 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/31 14:59:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: \n",
      "16/05/31 14:59:31 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: Spilling map output\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: bufstart = 0; bufend = 192; bufvoid = 104857600\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/31 14:59:31 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/31 14:59:31 INFO mapred.Task: Task:attempt_local1735338398_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/31 14:59:31 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/31 14:59:32 INFO mapred.Task: Task 'attempt_local1735338398_0001_m_000000_0' done.\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local1735338398_0001_m_000000_0\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: Starting task: attempt_local1735338398_0001_r_000000_0\n",
      "16/05/31 14:59:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/31 14:59:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/31 14:59:32 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7789390c\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/31 14:59:32 INFO reduce.EventFetcher: attempt_local1735338398_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/31 14:59:32 INFO mapreduce.Job: Job job_local1735338398_0001 running in uber mode : false\n",
      "16/05/31 14:59:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/31 14:59:32 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1735338398_0001_m_000000_0 decomp: 244 len: 248 to MEMORY\n",
      "16/05/31 14:59:32 INFO reduce.InMemoryMapOutput: Read 244 bytes from map-output for attempt_local1735338398_0001_m_000000_0\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 244, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->244\n",
      "16/05/31 14:59:32 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/31 14:59:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/31 14:59:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 238 bytes\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: Merged 1 segments, 244 bytes to disk to satisfy reduce memory limit\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: Merging 1 files, 248 bytes from disk\n",
      "16/05/31 14:59:32 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/31 14:59:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/31 14:59:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 238 bytes\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/31 14:59:32 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/31 14:59:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/31 14:59:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/31 14:59:32 INFO mapred.Task: Task:attempt_local1735338398_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/31 14:59:32 INFO mapred.Task: Task attempt_local1735338398_0001_r_000000_0 is allowed to commit now\n",
      "16/05/31 14:59:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1735338398_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1735338398_0001_r_000000\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: Records R/W=25/1 > reduce\n",
      "16/05/31 14:59:32 INFO mapred.Task: Task 'attempt_local1735338398_0001_r_000000_0' done.\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local1735338398_0001_r_000000_0\n",
      "16/05/31 14:59:32 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/31 14:59:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/31 14:59:33 INFO mapreduce.Job: Job job_local1735338398_0001 completed successfully\n",
      "16/05/31 14:59:33 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210830\n",
      "\t\tFILE: Number of bytes written=754760\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=384\n",
      "\t\tHDFS: Number of bytes written=192\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=192\n",
      "\t\tMap output materialized bytes=248\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=248\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=529530880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192\n",
      "16/05/31 14:59:33 INFO streaming.StreamJob: Output directory: Sort/Output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D mapred.text.key.comparator.options=-nr \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer /bin/cat \\\n",
    "   -input Sort/thirtyNumbersDataSet.txt  -output Sort/Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============  UNSorted  ============\n",
      "16/05/31 14:59:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 \t3\n",
      "7 \t7\n",
      "8 \t7\n",
      "9 \t12\n",
      "8 \t7\n",
      "10 \t13\n",
      "4 \t8\n",
      "1 \t6\n",
      "10 \t8\n",
      "10 \t5\n",
      "11 \t3\n",
      "12 \t4\n",
      "2 \t8\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "3\tA Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "\n",
      "TOTAL Sort primary key numeric decresing\n",
      "\n",
      "16/05/31 14:59:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "11 \t3\n",
      "10 \t13\n",
      "10 \t8\n",
      "10 \t5\n",
      "9 \t12\n",
      "8 \t7\n",
      "8 \t7\n",
      "7 \t7\n",
      "6\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "4 \t8\n",
      "3\tA Group\n",
      "3\tD Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2 \t8\n",
      "1 \t6\n",
      "0 \t3\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input file\n",
    "!echo \"=============  UNSorted  ============\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt\n",
    "!echo  \"\\nTOTAL Sort primary key numeric decresing\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat Sort/Output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default MapReduce Job with Multiple Reducer leads to a Partial Sort\n",
    "\n",
    "\n",
    "Set number of reducer tasks to 3\n",
    "<li>CASE 1. use default key-value behavior but treat the key as numeric and reverse it \n",
    "<p><i>-D mapred.text.key.comparator.options=-nr </i>\n",
    "<p></li>\n",
    "<li>CASE 2. use default key-value behavior but treat the key as numeric and but do NOT reverse it </li>\n",
    "<p><i> -D mapred.text.key.comparator.options=-nr \\ #ignored </i>\n",
    " <p><i>  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\ </i>\n",
    " <p><i>   -D mapreduce.partition.keycomparator.options=\"-k1,1n\" \\ </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:45:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:45:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "16/05/27 18:45:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:45:26 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 18:45:26 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 18:45:26 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 18:45:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 18:45:27 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 18:45:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1750108363_0001\n",
      "16/05/27 18:45:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 18:45:27 INFO mapreduce.Job: Running job: job_local1750108363_0001\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1750108363_0001_m_000000_0\n",
      "16/05/27 18:45:27 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:45:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/thirtyNumbersDataSet.txt:0+192\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: \n",
      "16/05/27 18:45:27 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: bufstart = 0; bufend = 192; bufvoid = 104857600\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/27 18:45:27 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task:attempt_local1750108363_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task 'attempt_local1750108363_0001_m_000000_0' done.\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local1750108363_0001_m_000000_0\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1750108363_0001_r_000000_0\n",
      "16/05/27 18:45:27 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:45:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:45:27 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c5bfe8b\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: attempt_local1750108363_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:45:27 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1750108363_0001_m_000000_0 decomp: 73 len: 77 to MEMORY\n",
      "16/05/27 18:45:27 INFO reduce.InMemoryMapOutput: Read 73 bytes from map-output for attempt_local1750108363_0001_m_000000_0\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 73, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->73\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 73 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 1 files, 77 bytes from disk\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task:attempt_local1750108363_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task attempt_local1750108363_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 18:45:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1750108363_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1750108363_0001_r_000000\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task 'attempt_local1750108363_0001_r_000000_0' done.\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local1750108363_0001_r_000000_0\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1750108363_0001_r_000001_0\n",
      "16/05/27 18:45:27 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:45:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:45:27 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3816799e\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: attempt_local1750108363_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:45:27 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1750108363_0001_m_000000_0 decomp: 144 len: 148 to MEMORY\n",
      "16/05/27 18:45:27 INFO reduce.InMemoryMapOutput: Read 144 bytes from map-output for attempt_local1750108363_0001_m_000000_0\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 144, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->144\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 138 bytes\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 144 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 1 files, 148 bytes from disk\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 138 bytes\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:45:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: Records R/W=14/1\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task:attempt_local1750108363_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task attempt_local1750108363_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 18:45:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1750108363_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1750108363_0001_r_000001\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Records R/W=14/1 > reduce\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task 'attempt_local1750108363_0001_r_000001_0' done.\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local1750108363_0001_r_000001_0\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1750108363_0001_r_000002_0\n",
      "16/05/27 18:45:27 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:45:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:45:27 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c98f859\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: attempt_local1750108363_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:45:27 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1750108363_0001_m_000000_0 decomp: 31 len: 35 to MEMORY\n",
      "16/05/27 18:45:27 INFO reduce.InMemoryMapOutput: Read 31 bytes from map-output for attempt_local1750108363_0001_m_000000_0\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 31, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31\n",
      "16/05/27 18:45:27 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 25 bytes\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 31 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 1 files, 35 bytes from disk\n",
      "16/05/27 18:45:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:45:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 25 bytes\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:45:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task:attempt_local1750108363_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task attempt_local1750108363_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 18:45:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1750108363_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1750108363_0001_r_000002\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "16/05/27 18:45:27 INFO mapred.Task: Task 'attempt_local1750108363_0001_r_000002_0' done.\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local1750108363_0001_r_000002_0\n",
      "16/05/27 18:45:27 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 18:45:28 INFO mapreduce.Job: Job job_local1750108363_0001 running in uber mode : false\n",
      "16/05/27 18:45:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 18:45:28 INFO mapreduce.Job: Job job_local1750108363_0001 completed successfully\n",
      "16/05/27 18:45:28 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422823\n",
      "\t\tFILE: Number of bytes written=1509806\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=768\n",
      "\t\tHDFS: Number of bytes written=420\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=192\n",
      "\t\tMap output materialized bytes=260\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=260\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1035993088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192\n",
      "16/05/27 18:45:28 INFO streaming.StreamJob: Output directory: Sort/Output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D mapred.text.key.comparator.options=-nr \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer /bin/cat \\\n",
    "   -numReduceTasks 3 \\\n",
    "   -input Sort/thirtyNumbersDataSet.txt  -output Sort/Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- UNSORTED INPUT-------------------\n",
      "\n",
      "16/05/27 18:45:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 \t3\n",
      "7 \t7\n",
      "8 \t7\n",
      "9 \t12\n",
      "8 \t7\n",
      "10 \t13\n",
      "4 \t8\n",
      "1 \t6\n",
      "10 \t8\n",
      "10 \t5\n",
      "11 \t3\n",
      "12 \t4\n",
      "2 \t8\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "3\tA Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "\\Partial Sort:  primary key numeric decresing\n",
      "\n",
      "\n",
      " --- Partial Sort:  part-00000 ----- \n",
      "\n",
      "16/05/27 18:45:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "9 \t12\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "0 \t3\n",
      "\n",
      " --- Partial Sort:  part-00001 ----- \n",
      "\n",
      "16/05/27 18:45:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "10 \t8\n",
      "10 \t13\n",
      "10 \t5\n",
      "7 \t7\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "6\tA Group\n",
      "4 \t8\n",
      "3\tD Group\n",
      "3\tB Group\n",
      "3\tC Group\n",
      "3\tA Group\n",
      "1 \t6\n",
      "\n",
      " --- Partial Sort:  part-00002 ----- \n",
      "\n",
      "16/05/27 18:45:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11 \t3\n",
      "8 \t7\n",
      "8 \t7\n",
      "2 \t8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#have a look at the input file\n",
    "!echo  \"\\n-------- UNSORTED INPUT-------------------\\n\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt\n",
    "!echo  \"\\Partial Sort:  primary key numeric decresing\\n\"\n",
    "# \n",
    "!echo  \"\\n --- Partial Sort:  part-00000 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00000\n",
    "!echo  \"\\n --- Partial Sort:  part-00001 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00001\n",
    "!echo  \"\\n --- Partial Sort:  part-00002 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:46:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:46:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "16/05/27 18:46:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:46:09 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 18:46:09 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 18:46:09 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 18:46:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 18:46:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 18:46:09 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/27 18:46:09 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 18:46:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local233110586_0001\n",
      "16/05/27 18:46:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 18:46:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 18:46:09 INFO mapreduce.Job: Running job: job_local233110586_0001\n",
      "16/05/27 18:46:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 18:46:09 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 18:46:09 INFO mapred.LocalJobRunner: Starting task: attempt_local233110586_0001_m_000000_0\n",
      "16/05/27 18:46:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:46:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:46:09 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/thirtyNumbersDataSet.txt:0+192\n",
      "16/05/27 18:46:09 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: \n",
      "16/05/27 18:46:10 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: bufstart = 0; bufend = 192; bufvoid = 104857600\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/27 18:46:10 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task:attempt_local233110586_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task 'attempt_local233110586_0001_m_000000_0' done.\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local233110586_0001_m_000000_0\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Starting task: attempt_local233110586_0001_r_000000_0\n",
      "16/05/27 18:46:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:46:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:46:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5bab2487\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: attempt_local233110586_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:46:10 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local233110586_0001_m_000000_0 decomp: 73 len: 77 to MEMORY\n",
      "16/05/27 18:46:10 INFO reduce.InMemoryMapOutput: Read 73 bytes from map-output for attempt_local233110586_0001_m_000000_0\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 73, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->73\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 73 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 1 files, 77 bytes from disk\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task:attempt_local233110586_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task attempt_local233110586_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 18:46:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local233110586_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local233110586_0001_r_000000\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task 'attempt_local233110586_0001_r_000000_0' done.\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local233110586_0001_r_000000_0\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Starting task: attempt_local233110586_0001_r_000001_0\n",
      "16/05/27 18:46:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:46:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:46:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7cbfdf3f\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: attempt_local233110586_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:46:10 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local233110586_0001_m_000000_0 decomp: 144 len: 148 to MEMORY\n",
      "16/05/27 18:46:10 INFO reduce.InMemoryMapOutput: Read 144 bytes from map-output for attempt_local233110586_0001_m_000000_0\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 144, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->144\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 138 bytes\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 144 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 1 files, 148 bytes from disk\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 138 bytes\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:46:10 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: Records R/W=14/1\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task:attempt_local233110586_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task attempt_local233110586_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 18:46:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local233110586_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local233110586_0001_r_000001\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Records R/W=14/1 > reduce\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task 'attempt_local233110586_0001_r_000001_0' done.\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local233110586_0001_r_000001_0\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Starting task: attempt_local233110586_0001_r_000002_0\n",
      "16/05/27 18:46:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:46:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:46:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49013234\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: attempt_local233110586_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 18:46:10 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local233110586_0001_m_000000_0 decomp: 31 len: 35 to MEMORY\n",
      "16/05/27 18:46:10 INFO reduce.InMemoryMapOutput: Read 31 bytes from map-output for attempt_local233110586_0001_m_000000_0\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 31, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31\n",
      "16/05/27 18:46:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 25 bytes\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 31 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 1 files, 35 bytes from disk\n",
      "16/05/27 18:46:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 18:46:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 25 bytes\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:46:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task:attempt_local233110586_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task attempt_local233110586_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 18:46:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local233110586_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local233110586_0001_r_000002\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "16/05/27 18:46:10 INFO mapred.Task: Task 'attempt_local233110586_0001_r_000002_0' done.\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local233110586_0001_r_000002_0\n",
      "16/05/27 18:46:10 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 18:46:10 INFO mapreduce.Job: Job job_local233110586_0001 running in uber mode : false\n",
      "16/05/27 18:46:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 18:46:10 INFO mapreduce.Job: Job job_local233110586_0001 completed successfully\n",
      "16/05/27 18:46:10 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422823\n",
      "\t\tFILE: Number of bytes written=1506174\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=768\n",
      "\t\tHDFS: Number of bytes written=420\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=192\n",
      "\t\tMap output materialized bytes=260\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=260\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=1095237632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192\n",
      "16/05/27 18:46:10 INFO streaming.StreamJob: Output directory: Sort/Output\n"
     ]
    }
   ],
   "source": [
    "#specify the key and \n",
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D mapred.text.key.comparator.options=-nr \\\n",
    "   -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer /bin/cat \\\n",
    "   -numReduceTasks 3 \\\n",
    "   -input Sort/thirtyNumbersDataSet.txt  -output Sort/Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- UNSORTED INPUT-------------------\n",
      "\n",
      "16/05/27 18:48:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 \t3\n",
      "7 \t7\n",
      "8 \t7\n",
      "9 \t12\n",
      "8 \t7\n",
      "10 \t13\n",
      "4 \t8\n",
      "1 \t6\n",
      "10 \t8\n",
      "10 \t5\n",
      "11 \t3\n",
      "12 \t4\n",
      "2 \t8\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "3\tA Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "\\Partial Sort:  primary key numeric decresing\n",
      "\n",
      "\n",
      " --- Partial Sort:  part-00000 ----- \n",
      "\n",
      "16/05/27 18:48:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "9 \t12\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "0 \t3\n",
      "\n",
      " --- Partial Sort:  part-00001 ----- \n",
      "\n",
      "16/05/27 18:48:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "10 \t8\n",
      "10 \t13\n",
      "10 \t5\n",
      "7 \t7\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "6\tA Group\n",
      "4 \t8\n",
      "3\tD Group\n",
      "3\tB Group\n",
      "3\tC Group\n",
      "3\tA Group\n",
      "1 \t6\n",
      "\n",
      " --- Partial Sort:  part-00002 ----- \n",
      "\n",
      "16/05/27 18:48:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11 \t3\n",
      "8 \t7\n",
      "8 \t7\n",
      "2 \t8\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input file\n",
    "!echo  \"\\n-------- UNSORTED INPUT-------------------\\n\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt\n",
    "!echo  \"\\Partial Sort:  primary key numeric decresing\\n\"\n",
    "# \n",
    "!echo  \"\\n --- Partial Sort:  part-00000 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00000\n",
    "!echo  \"\\n --- Partial Sort:  part-00001 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00001\n",
    "!echo  \"\\n --- Partial Sort:  part-00002 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE REDUCERS TOTAL SORT \n",
    "### __ MAPPER prepend a partition key to each row such that each record is routed to the appropriate partition (creates order in the partition files)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SortCode/prependPartitionKeyMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SortCode/prependPartitionKeyMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(filename='prependPartitionKeyMapper.log',level=logging.DEBUG)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\", 1)\n",
    "    \n",
    "    partitionKey=\"c\"     \n",
    "    if int(key) < 5:\n",
    "        partitionKey = \"a\"        \n",
    "    elif int(key) < 10:\n",
    "        partitionKey = \"b\"        \n",
    "        \n",
    "    logging.debug(\"%s\\t%s\\t%s\" % (partitionKey, key,value))\n",
    "    print \"%s\\t%s\\t%s\" % (partitionKey, key,value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __UNIT TEST - run the hadoop command for a mapper only job by specifying 0 reducers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:58:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:58:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output-Intermediate\n",
      "16/05/27 18:58:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 18:58:32 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 18:58:32 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 18:58:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 18:58:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 18:58:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local992016862_0001\n",
      "16/05/27 18:58:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 18:58:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 18:58:32 INFO mapreduce.Job: Running job: job_local992016862_0001\n",
      "16/05/27 18:58:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 18:58:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 18:58:32 INFO mapred.LocalJobRunner: Starting task: attempt_local992016862_0001_m_000000_0\n",
      "16/05/27 18:58:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 18:58:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 18:58:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/thirtyNumbersDataSet.txt:0+192\n",
      "16/05/27 18:58:32 INFO mapred.MapTask: numReduceTasks: 0\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./prependPartitionKeyMapper.py]\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 18:58:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 18:58:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 18:58:32 INFO mapred.LocalJobRunner: \n",
      "16/05/27 18:58:33 INFO mapred.Task: Task:attempt_local992016862_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 18:58:33 INFO mapred.LocalJobRunner: \n",
      "16/05/27 18:58:33 INFO mapred.Task: Task attempt_local992016862_0001_m_000000_0 is allowed to commit now\n",
      "16/05/27 18:58:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local992016862_0001_m_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output-Intermediate/_temporary/0/task_local992016862_0001_m_000000\n",
      "16/05/27 18:58:33 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 18:58:33 INFO mapred.Task: Task 'attempt_local992016862_0001_m_000000_0' done.\n",
      "16/05/27 18:58:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local992016862_0001_m_000000_0\n",
      "16/05/27 18:58:33 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 18:58:33 INFO mapreduce.Job: Job job_local992016862_0001 running in uber mode : false\n",
      "16/05/27 18:58:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 18:58:33 INFO mapreduce.Job: Job job_local992016862_0001 completed successfully\n",
      "16/05/27 18:58:33 INFO mapreduce.Job: Counters: 20\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=105150\n",
      "\t\tFILE: Number of bytes written=374523\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=192\n",
      "\t\tHDFS: Number of bytes written=242\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tInput split bytes=118\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=257949696\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=242\n",
      "16/05/27 18:58:33 INFO streaming.StreamJob: Output directory: Sort/Output-Intermediate\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r Sort/Output-Intermediate\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D mapred.reduce.tasks=0 \\\n",
    "   -mapper SortCode/prependPartitionKeyMapper.py \\\n",
    "   -input Sort/thirtyNumbersDataSet.txt  -output Sort/Output-Intermediate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the prependPartitionKeyMapper mapper job\n",
      "16/05/27 18:58:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t0 \t3\n",
      "b\t7 \t7\n",
      "b\t8 \t7\n",
      "b\t9 \t12\n",
      "b\t8 \t7\n",
      "c\t10 \t13\n",
      "a\t4 \t8\n",
      "a\t1 \t6\n",
      "c\t10 \t8\n",
      "c\t10 \t5\n",
      "c\t11 \t3\n",
      "c\t12 \t4\n",
      "a\t2 \t8\n",
      "a\t2\tA Group\n",
      "a\t2\tB Group\n",
      "a\t2\tC Group\n",
      "a\t2\tD Group\n",
      "b\t6\tA Group\n",
      "b\t6\tC Group\n",
      "b\t6\tB Group\n",
      "b\t6\tD Group\n",
      "a\t3\tA Group\n",
      "a\t3\tC Group\n",
      "a\t3\tB Group\n",
      "a\t3\tD Group\n"
     ]
    }
   ],
   "source": [
    "print \"Output from the prependPartitionKeyMapper mapper job\"\n",
    "!hdfs dfs -cat Sort/Output-Intermediate/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __UNIT TEST- an example of  a secondary  sort map/reduce job \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 19:46:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:46:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "16/05/27 19:46:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:46:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 19:46:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 19:46:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 19:46:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 19:46:34 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 19:46:34 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 19:46:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1477657752_0001\n",
      "16/05/27 19:46:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 19:46:35 INFO mapreduce.Job: Running job: job_local1477657752_0001\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1477657752_0001_m_000000_0\n",
      "16/05/27 19:46:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:46:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/Output-Intermediate/part-00000:0+242\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: \n",
      "16/05/27 19:46:35 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: bufstart = 0; bufend = 267; bufvoid = 104857600\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/27 19:46:35 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task:attempt_local1477657752_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task 'attempt_local1477657752_0001_m_000000_0' done.\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1477657752_0001_m_000000_0\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1477657752_0001_r_000000_0\n",
      "16/05/27 19:46:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:46:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:46:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@f777322\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: attempt_local1477657752_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:46:35 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1477657752_0001_m_000000_0 decomp: 58 len: 62 to MEMORY\n",
      "16/05/27 19:46:35 INFO reduce.InMemoryMapOutput: Read 58 bytes from map-output for attempt_local1477657752_0001_m_000000_0\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 58, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->58\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 58 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 1 files, 62 bytes from disk\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: Records R/W=5/1\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task:attempt_local1477657752_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task attempt_local1477657752_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 19:46:35 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1477657752_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1477657752_0001_r_000000\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Records R/W=5/1 > reduce\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task 'attempt_local1477657752_0001_r_000000_0' done.\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1477657752_0001_r_000000_0\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1477657752_0001_r_000001_0\n",
      "16/05/27 19:46:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:46:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:46:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42f1c0b1\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: attempt_local1477657752_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:46:35 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1477657752_0001_m_000000_0 decomp: 162 len: 166 to MEMORY\n",
      "16/05/27 19:46:35 INFO reduce.InMemoryMapOutput: Read 162 bytes from map-output for attempt_local1477657752_0001_m_000000_0\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 1 files, 166 bytes from disk\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 19:46:35 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: Records R/W=12/1\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task:attempt_local1477657752_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task attempt_local1477657752_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 19:46:35 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1477657752_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1477657752_0001_r_000001\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Records R/W=12/1 > reduce\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task 'attempt_local1477657752_0001_r_000001_0' done.\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1477657752_0001_r_000001_0\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1477657752_0001_r_000002_0\n",
      "16/05/27 19:46:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:46:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:46:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@585e84df\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: attempt_local1477657752_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:46:35 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1477657752_0001_m_000000_0 decomp: 103 len: 107 to MEMORY\n",
      "16/05/27 19:46:35 INFO reduce.InMemoryMapOutput: Read 103 bytes from map-output for attempt_local1477657752_0001_m_000000_0\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 103, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->103\n",
      "16/05/27 19:46:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 103 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 1 files, 107 bytes from disk\n",
      "16/05/27 19:46:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:46:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:46:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task:attempt_local1477657752_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task attempt_local1477657752_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 19:46:35 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1477657752_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local1477657752_0001_r_000002\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Records R/W=8/1 > reduce\n",
      "16/05/27 19:46:35 INFO mapred.Task: Task 'attempt_local1477657752_0001_r_000002_0' done.\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1477657752_0001_r_000002_0\n",
      "16/05/27 19:46:35 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 19:46:36 INFO mapreduce.Job: Job job_local1477657752_0001 running in uber mode : false\n",
      "16/05/27 19:46:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 19:46:36 INFO mapreduce.Job: Job job_local1477657752_0001 completed successfully\n",
      "16/05/27 19:46:36 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423387\n",
      "\t\tFILE: Number of bytes written=1514909\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=968\n",
      "\t\tHDFS: Number of bytes written=495\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=267\n",
      "\t\tMap output materialized bytes=335\n",
      "\t\tInput split bytes=124\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=24\n",
      "\t\tReduce shuffle bytes=335\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1455423488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=242\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=267\n",
      "16/05/27 19:46:36 INFO streaming.StreamJob: Output directory: Sort/Output\n",
      "\n",
      "-------- UNSORTED INPUT-------------------\n",
      "\n",
      "16/05/27 19:46:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0 \t3\n",
      "7 \t7\n",
      "8 \t7\n",
      "9 \t12\n",
      "8 \t7\n",
      "10 \t13\n",
      "4 \t8\n",
      "1 \t6\n",
      "10 \t8\n",
      "10 \t5\n",
      "11 \t3\n",
      "12 \t4\n",
      "2 \t8\n",
      "2\tA Group\n",
      "2\tB Group\n",
      "2\tC Group\n",
      "2\tD Group\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "3\tA Group\n",
      "3\tC Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "\\Partial Sort:  primary key numeric decresing\n",
      "\n",
      "\n",
      " --- Total Sort:  part-00000 ----- \n",
      "\n",
      "16/05/27 19:46:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "c\t12 \t4\t\n",
      "c\t11 \t3\t\n",
      "c\t10 \t13\t\n",
      "c\t10 \t8\t\n",
      "c\t10 \t5\t\n",
      "\n",
      " --- Total Sort:  part-00001 ----- \n",
      "\n",
      "16/05/27 19:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t4 \t8\t\n",
      "a\t3\tC Group\t\n",
      "a\t3\tA Group\t\n",
      "a\t3\tB Group\t\n",
      "a\t3\tD Group\t\n",
      "a\t2\tD Group\t\n",
      "a\t2\tC Group\t\n",
      "a\t2\tB Group\t\n",
      "a\t2\tA Group\t\n",
      "a\t2 \t8\t\n",
      "a\t1 \t6\t\n",
      "a\t0 \t3\t\n",
      "\n",
      " --- Total Sort:  part-00002 ----- \n",
      "\n",
      "16/05/27 19:46:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "b\t9 \t12\t\n",
      "b\t8 \t7\t\n",
      "b\t8 \t7\t\n",
      "b\t7 \t7\t\n",
      "b\t6\tA Group\t\n",
      "b\t6\tC Group\t\n",
      "b\t6\tB Group\t\n",
      "b\t6\tD Group\t\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "#     Basic Secondary SORT (works!)\n",
    "#     # but it still has the partition key in each record\n",
    "#-----------------------------------------------------------\n",
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D stream.map.output.field.separator=\"\\t\" \\\n",
    "   -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer /bin/cat \\\n",
    "   -numReduceTasks 3 \\\n",
    "   -input Sort/Output-Intermediate  -output Sort/Output \\\n",
    "   -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "\n",
    "#have a look at the input file\n",
    "!echo  \"\\n-------- UNSORTED INPUT-------------------\\n\"\n",
    "!hdfs dfs -cat Sort/thirtyNumbersDataSet.txt\n",
    "!echo  \"\\Partial Sort:  primary key numeric decresing\\n\"\n",
    "# \n",
    "!echo  \"\\n --- Total Sort:  part-00000 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00000\n",
    "!echo  \"\\n --- Total Sort:  part-00001 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00001\n",
    "!echo  \"\\n --- Total Sort:  part-00002 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 19:50:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:50:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "16/05/27 19:50:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 19:50:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 19:50:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 19:50:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 19:50:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: reduce.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.reduce.output.key.value.fields.spec\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 19:50:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2061432059_0001\n",
      "16/05/27 19:50:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 19:50:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 19:50:11 INFO mapreduce.Job: Running job: job_local2061432059_0001\n",
      "16/05/27 19:50:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 19:50:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 19:50:11 INFO mapred.LocalJobRunner: Starting task: attempt_local2061432059_0001_m_000000_0\n",
      "16/05/27 19:50:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:50:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/Output-Intermediate/part-00000:0+242\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 19:50:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 19:50:11 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 19:50:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 19:50:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:50:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:50:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:50:12 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 19:50:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: \n",
      "16/05/27 19:50:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 19:50:12 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 19:50:12 INFO mapred.MapTask: bufstart = 0; bufend = 267; bufvoid = 104857600\n",
      "16/05/27 19:50:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/27 19:50:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task:attempt_local2061432059_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task 'attempt_local2061432059_0001_m_000000_0' done.\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local2061432059_0001_m_000000_0\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Starting task: attempt_local2061432059_0001_r_000000_0\n",
      "16/05/27 19:50:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:50:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:50:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@756f302c\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: attempt_local2061432059_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:50:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2061432059_0001_m_000000_0 decomp: 58 len: 62 to MEMORY\n",
      "16/05/27 19:50:12 INFO reduce.InMemoryMapOutput: Read 58 bytes from map-output for attempt_local2061432059_0001_m_000000_0\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 58, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->58\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 58 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 1 files, 62 bytes from disk\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task:attempt_local2061432059_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task attempt_local2061432059_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 19:50:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2061432059_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local2061432059_0001_r_000000\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task 'attempt_local2061432059_0001_r_000000_0' done.\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local2061432059_0001_r_000000_0\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Starting task: attempt_local2061432059_0001_r_000001_0\n",
      "16/05/27 19:50:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:50:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:50:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7d057295\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: attempt_local2061432059_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:50:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2061432059_0001_m_000000_0 decomp: 162 len: 166 to MEMORY\n",
      "16/05/27 19:50:12 INFO reduce.InMemoryMapOutput: Read 162 bytes from map-output for attempt_local2061432059_0001_m_000000_0\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 1 files, 166 bytes from disk\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task:attempt_local2061432059_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task attempt_local2061432059_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 19:50:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2061432059_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local2061432059_0001_r_000001\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task 'attempt_local2061432059_0001_r_000001_0' done.\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local2061432059_0001_r_000001_0\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Starting task: attempt_local2061432059_0001_r_000002_0\n",
      "16/05/27 19:50:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:50:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:50:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5b11ef4\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: attempt_local2061432059_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:50:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2061432059_0001_m_000000_0 decomp: 103 len: 107 to MEMORY\n",
      "16/05/27 19:50:12 INFO reduce.InMemoryMapOutput: Read 103 bytes from map-output for attempt_local2061432059_0001_m_000000_0\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 103, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->103\n",
      "16/05/27 19:50:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 103 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 1 files, 107 bytes from disk\n",
      "16/05/27 19:50:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:50:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task:attempt_local2061432059_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task attempt_local2061432059_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 19:50:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2061432059_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local2061432059_0001_r_000002\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:50:12 INFO mapred.Task: Task 'attempt_local2061432059_0001_r_000002_0' done.\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local2061432059_0001_r_000002_0\n",
      "16/05/27 19:50:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 19:50:12 INFO mapreduce.Job: Job job_local2061432059_0001 running in uber mode : false\n",
      "16/05/27 19:50:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 19:50:12 INFO mapreduce.Job: Job job_local2061432059_0001 completed successfully\n",
      "16/05/27 19:50:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423387\n",
      "\t\tFILE: Number of bytes written=1512105\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=968\n",
      "\t\tHDFS: Number of bytes written=354\n",
      "\t\tHDFS: Number of read operations=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=267\n",
      "\t\tMap output materialized bytes=335\n",
      "\t\tInput split bytes=124\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=24\n",
      "\t\tReduce shuffle bytes=335\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=38\n",
      "\t\tTotal committed heap usage (bytes)=1140326400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=242\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192\n",
      "16/05/27 19:50:12 INFO streaming.StreamJob: Output directory: Sort/Output\n",
      "\n",
      "-------- UNSORTED INPUT-------------------\n",
      "\n",
      "16/05/27 19:50:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t0 \t3\n",
      "b\t7 \t7\n",
      "b\t8 \t7\n",
      "b\t9 \t12\n",
      "b\t8 \t7\n",
      "c\t10 \t13\n",
      "a\t4 \t8\n",
      "a\t1 \t6\n",
      "c\t10 \t8\n",
      "c\t10 \t5\n",
      "c\t11 \t3\n",
      "c\t12 \t4\n",
      "a\t2 \t8\n",
      "a\t2\tA Group\n",
      "a\t2\tB Group\n",
      "a\t2\tC Group\n",
      "a\t2\tD Group\n",
      "b\t6\tA Group\n",
      "b\t6\tC Group\n",
      "b\t6\tB Group\n",
      "b\t6\tD Group\n",
      "a\t3\tA Group\n",
      "a\t3\tC Group\n",
      "a\t3\tB Group\n",
      "a\t3\tD Group\n",
      "\\Partial Sort:  primary key numeric decresing\n",
      "\n",
      "\n",
      " --- Total Sort:  part-00000 ----- \n",
      "\n",
      "16/05/27 19:50:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "11 \t3\n",
      "10 \t13\n",
      "10 \t8\n",
      "10 \t5\n",
      "\n",
      " --- Total Sort:  part-00001 ----- \n",
      "\n",
      "16/05/27 19:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "4 \t8\n",
      "3\tC Group\n",
      "3\tA Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "2\tD Group\n",
      "2\tC Group\n",
      "2\tB Group\n",
      "2\tA Group\n",
      "2 \t8\n",
      "1 \t6\n",
      "0 \t3\n",
      "\n",
      " --- Total Sort:  part-00002 ----- \n",
      "\n",
      "16/05/27 19:50:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9 \t12\n",
      "8 \t7\n",
      "8 \t7\n",
      "7 \t7\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#     Basic Secondary SORT (Works)\n",
    "#     Select a subset of the fields\n",
    "#     i.e., drop the partition key\n",
    "#-----------------------------------\n",
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D stream.map.output.field.separator=\"\\t\" \\\n",
    "   -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "   -D reduce.output.key.value.fields.spec=1:2- \\\n",
    "   -mapper /bin/cat \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \\\n",
    "   -numReduceTasks 3 \\\n",
    "   -input Sort/Output-Intermediate  -output Sort/Output \\\n",
    "   -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "\n",
    "#have a look at the input file\n",
    "!echo  \"\\n-------- UNSORTED INPUT-------------------\\n\"\n",
    "!hdfs dfs -cat Sort/Output-Intermediate/*\n",
    "!echo  \"\\Partial Sort:  primary key numeric decresing\\n\"\n",
    "# \n",
    "!echo  \"\\n --- Total Sort:  part-00000 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00000\n",
    "!echo  \"\\n --- Total Sort:  part-00001 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00001\n",
    "!echo  \"\\n --- Total Sort:  part-00002 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 19:56:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:56:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted Sort/Output\n",
      "16/05/27 19:56:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 19:56:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/27 19:56:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/27 19:56:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/27 19:56:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 19:56:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: reduce.output.key.value.fields.spec is deprecated. Instead, use mapreduce.fieldsel.reduce.output.key.value.fields.spec\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/27 19:56:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local87522653_0001\n",
      "16/05/27 19:56:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/27 19:56:09 INFO mapreduce.Job: Running job: job_local87522653_0001\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Starting task: attempt_local87522653_0001_m_000000_0\n",
      "16/05/27 19:56:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:56:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/Sort/thirtyNumbersDataSet.txt:0+192\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./SortCode/prependPartitionKeyMapper.py]\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/27 19:56:09 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: Records R/W=25/1\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/27 19:56:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: \n",
      "16/05/27 19:56:09 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: Spilling map output\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: bufstart = 0; bufend = 267; bufvoid = 104857600\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214300(104857200); length = 97/6553600\n",
      "16/05/27 19:56:09 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task:attempt_local87522653_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Records R/W=25/1\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task 'attempt_local87522653_0001_m_000000_0' done.\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local87522653_0001_m_000000_0\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Starting task: attempt_local87522653_0001_r_000000_0\n",
      "16/05/27 19:56:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:56:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:56:09 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@69ad4fd4\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: attempt_local87522653_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:56:09 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local87522653_0001_m_000000_0 decomp: 58 len: 62 to MEMORY\n",
      "16/05/27 19:56:09 INFO reduce.InMemoryMapOutput: Read 58 bytes from map-output for attempt_local87522653_0001_m_000000_0\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 58, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->58\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merged 1 segments, 58 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 1 files, 62 bytes from disk\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 48 bytes\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task:attempt_local87522653_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task attempt_local87522653_0001_r_000000_0 is allowed to commit now\n",
      "16/05/27 19:56:09 INFO output.FileOutputCommitter: Saved output of task 'attempt_local87522653_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local87522653_0001_r_000000\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task 'attempt_local87522653_0001_r_000000_0' done.\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local87522653_0001_r_000000_0\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Starting task: attempt_local87522653_0001_r_000001_0\n",
      "16/05/27 19:56:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:56:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:56:09 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@389d382b\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: attempt_local87522653_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:56:09 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local87522653_0001_m_000000_0 decomp: 162 len: 166 to MEMORY\n",
      "16/05/27 19:56:09 INFO reduce.InMemoryMapOutput: Read 162 bytes from map-output for attempt_local87522653_0001_m_000000_0\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 162, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->162\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merged 1 segments, 162 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 1 files, 166 bytes from disk\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 153 bytes\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task:attempt_local87522653_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task attempt_local87522653_0001_r_000001_0 is allowed to commit now\n",
      "16/05/27 19:56:09 INFO output.FileOutputCommitter: Saved output of task 'attempt_local87522653_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local87522653_0001_r_000001\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task 'attempt_local87522653_0001_r_000001_0' done.\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local87522653_0001_r_000001_0\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Starting task: attempt_local87522653_0001_r_000002_0\n",
      "16/05/27 19:56:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/27 19:56:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/27 19:56:09 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@f4dd3e3\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: attempt_local87522653_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/27 19:56:09 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local87522653_0001_m_000000_0 decomp: 103 len: 107 to MEMORY\n",
      "16/05/27 19:56:09 INFO reduce.InMemoryMapOutput: Read 103 bytes from map-output for attempt_local87522653_0001_m_000000_0\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 103, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->103\n",
      "16/05/27 19:56:09 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merged 1 segments, 103 bytes to disk to satisfy reduce memory limit\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 1 files, 107 bytes from disk\n",
      "16/05/27 19:56:09 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/27 19:56:09 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93 bytes\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO FieldSelectionMapReduce: fieldSeparator: \t\n",
      "mapOutputKeyValueSpec: 0-:\n",
      "reduceOutputKeyValueSpec: 1:2-\n",
      "allMapValueFieldsFrom: -1\n",
      "allReduceValueFieldsFrom: 2\n",
      "mapOutputKeyFieldList.length: 0\n",
      "mapOutputValueFieldList.length: 0\n",
      "reduceOutputKeyFieldList.length: 1\n",
      "\t1\n",
      "reduceOutputValueFieldList.length: 0\n",
      "\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task:attempt_local87522653_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task attempt_local87522653_0001_r_000002_0 is allowed to commit now\n",
      "16/05/27 19:56:09 INFO output.FileOutputCommitter: Saved output of task 'attempt_local87522653_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/Sort/Output/_temporary/0/task_local87522653_0001_r_000002\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/05/27 19:56:09 INFO mapred.Task: Task 'attempt_local87522653_0001_r_000002_0' done.\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local87522653_0001_r_000002_0\n",
      "16/05/27 19:56:09 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/27 19:56:10 INFO mapreduce.Job: Job job_local87522653_0001 running in uber mode : false\n",
      "16/05/27 19:56:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 19:56:10 INFO mapreduce.Job: Job job_local87522653_0001 completed successfully\n",
      "16/05/27 19:56:10 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423363\n",
      "\t\tFILE: Number of bytes written=1501265\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=768\n",
      "\t\tHDFS: Number of bytes written=354\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=25\n",
      "\t\tMap output records=25\n",
      "\t\tMap output bytes=267\n",
      "\t\tMap output materialized bytes=335\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=24\n",
      "\t\tReduce shuffle bytes=335\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=50\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=3\n",
      "\t\tTotal committed heap usage (bytes)=1246232576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=192\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=192\n",
      "16/05/27 19:56:10 INFO streaming.StreamJob: Output directory: Sort/Output\n",
      "\n",
      "-------- UNSORTED INPUT-------------------\n",
      "\n",
      "16/05/27 19:56:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t0 \t3\n",
      "b\t7 \t7\n",
      "b\t8 \t7\n",
      "b\t9 \t12\n",
      "b\t8 \t7\n",
      "c\t10 \t13\n",
      "a\t4 \t8\n",
      "a\t1 \t6\n",
      "c\t10 \t8\n",
      "c\t10 \t5\n",
      "c\t11 \t3\n",
      "c\t12 \t4\n",
      "a\t2 \t8\n",
      "a\t2\tA Group\n",
      "a\t2\tB Group\n",
      "a\t2\tC Group\n",
      "a\t2\tD Group\n",
      "b\t6\tA Group\n",
      "b\t6\tC Group\n",
      "b\t6\tB Group\n",
      "b\t6\tD Group\n",
      "a\t3\tA Group\n",
      "a\t3\tC Group\n",
      "a\t3\tB Group\n",
      "a\t3\tD Group\n",
      "\\Partial Sort:  primary key numeric decresing\n",
      "\n",
      "\n",
      " --- Total Sort:  part-00000 ----- \n",
      "\n",
      "16/05/27 19:56:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "11 \t3\n",
      "10 \t13\n",
      "10 \t8\n",
      "10 \t5\n",
      "\n",
      " --- Total Sort:  part-00001 ----- \n",
      "\n",
      "16/05/27 19:56:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "4 \t8\n",
      "3\tC Group\n",
      "3\tA Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "2\tD Group\n",
      "2\tC Group\n",
      "2\tB Group\n",
      "2\tA Group\n",
      "2 \t8\n",
      "1 \t6\n",
      "0 \t3\n",
      "\n",
      " --- Total Sort:  part-00002 ----- \n",
      "\n",
      "16/05/27 19:56:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9 \t12\n",
      "8 \t7\n",
      "8 \t7\n",
      "7 \t7\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#     TOTAL SORT (Works)\n",
    "#     Mapper: Prepend partition key  USING SortCode/prependPartitionKeyMapper.py \\\n",
    "#     Shuffle: partition and do secondary sort\n",
    "#     Reduce: drop the partition key USING -D reduce.output.key.value.fields.spec=1:2- \n",
    "#-----------------------------------\n",
    "!hdfs dfs -rm -r Sort/Output\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "   -D stream.map.output.field.separator=\"\\t\" \\\n",
    "   -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "   -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "   -D reduce.output.key.value.fields.spec=1:2- \\\n",
    "   -mapper SortCode/prependPartitionKeyMapper.py \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \\\n",
    "   -numReduceTasks 3 \\\n",
    "   -input Sort/thirtyNumbersDataSet.txt  -output Sort/Output \\\n",
    "   -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "\n",
    "#have a look at the input file\n",
    "!echo  \"\\n-------- UNSORTED INPUT-------------------\\n\"\n",
    "!hdfs dfs -cat Sort/Output-Intermediate/*\n",
    "!echo  \"\\Partial Sort:  primary key numeric decresing\\n\"\n",
    "# \n",
    "!echo  \"\\n --- Total Sort:  part-00000 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00000\n",
    "!echo  \"\\n --- Total Sort:  part-00001 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00001\n",
    "!echo  \"\\n --- Total Sort:  part-00002 ----- \\n\"\n",
    "!hdfs dfs -cat Sort/Output/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Step - Combine the secondary sort output files in the appropriate order__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 0, 12, Sort/Output/part-00000\n",
      "i is 1, 4, Sort/Output/part-00001\n",
      "i is 2, 9, Sort/Output/part-00002\n",
      "12:Sort/Output/part-00000\n",
      "16/05/27 22:38:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "12 \t4\n",
      "11 \t3\n",
      "10 \t13\n",
      "10 \t8\n",
      "10 \t5\n",
      "9:Sort/Output/part-00002\n",
      "16/05/27 22:38:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9 \t12\n",
      "8 \t7\n",
      "8 \t7\n",
      "7 \t7\n",
      "6\tA Group\n",
      "6\tC Group\n",
      "6\tB Group\n",
      "6\tD Group\n",
      "4:Sort/Output/part-00001\n",
      "16/05/27 22:38:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "4 \t8\n",
      "3\tC Group\n",
      "3\tA Group\n",
      "3\tB Group\n",
      "3\tD Group\n",
      "2\tD Group\n",
      "2\tC Group\n",
      "2\tB Group\n",
      "2\tA Group\n",
      "2 \t8\n",
      "1 \t6\n",
      "0 \t3\n"
     ]
    }
   ],
   "source": [
    "import subprocess \n",
    "import re\n",
    "\n",
    "lines=\"\"\n",
    "p = subprocess.Popen([\"hdfs\", \"dfs\", \"-ls\", \"Sort/Output/part-*\" ],  stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "for line in p.stdout.readlines():\n",
    "    lines = lines + line\n",
    "it = re.finditer(regex, line)\n",
    "regex = re.compile('(Sort\\/Output\\/part-\\d*)')\n",
    "it = re.finditer(regex, lines)\n",
    "\n",
    "outputPARTFiles=[]\n",
    "for match in it:\n",
    "  outputPARTFiles.append(match.group(0))\n",
    "\n",
    "partKeys=[]\n",
    "for f in outputPARTFiles:\n",
    "    partKeys.append(int(subprocess.Popen([\"hdfs\", \"dfs\", \"-tail\", f], stdout=subprocess.PIPE).stdout.read().splitlines()[0].split('\\t')[0].strip()))\n",
    "\n",
    "d={}\n",
    "for i in range(len(outputPARTFiles)):\n",
    "    print \"i is %d, %d, %s\" %(i, partKeys[i], outputPARTFiles[i])\n",
    "    d[partKeys[i]] = outputPARTFiles[i]\n",
    "\n",
    "\n",
    "#TOTAL Sort in decreasing order\n",
    "for k in sorted(d.items(), key=lambda x: x[0], reverse=True):\n",
    "    print \"%d:%s\"%(k[0], k[1])\n",
    "    p = subprocess.Popen([\"hdfs\", \"dfs\", \"-tail\", k[1]],  stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    for line in p.stdout.readlines():\n",
    "        print line,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 22:11:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-05-27 19:56 Sort/Output/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         31 2016-05-27 19:56 Sort/Output/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        100 2016-05-27 19:56 Sort/Output/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         61 2016-05-27 19:56 Sort/Output/part-00002\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls Sort/Output/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
