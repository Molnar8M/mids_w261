{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#MIDS-w261-Machine-Learning-at-Scale\" data-toc-modified-id=\"MIDS-w261-Machine-Learning-at-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS w261 Machine Learning at Scale</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#MidTerm-Exam\" data-toc-modified-id=\"MidTerm-Exam-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>MidTerm Exam</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Please-insert-your-contact-information-here\" data-toc-modified-id=\"Please-insert-your-contact-information-here-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Please insert your contact information here</a></span></li></ul></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Exam-Instructions\" data-toc-modified-id=\"Exam-Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exam Instructions</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Exam-Question-Notes:\" data-toc-modified-id=\"Exam-Question-Notes:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exam Question Notes:</a></span></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Exam-questions-begins-here\" data-toc-modified-id=\"Exam-questions-begins-here-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exam questions begins here</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Data-and--Starter-code-for-questions-6-and-7\" data-toc-modified-id=\"Data-and--Starter-code-for-questions-6-and-7-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data and  Starter code for questions 6 and 7</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Data-for-question-11\" data-toc-modified-id=\"Data-for-question-11-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Data for question 11</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Starter-Code-for-question-17\" data-toc-modified-id=\"Starter-Code-for-question-17-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Starter Code for question 17</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Starter-Code-for-question-18\" data-toc-modified-id=\"Starter-Code-for-question-18-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Starter Code for question 18</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#Starter-Code-for-question-19\" data-toc-modified-id=\"Starter-Code-for-question-19-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Starter Code for question 19</a></span></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/media/notebooks/MidTerm/stuff/MIDS-W261-MidTerm-3031886443-Fall2017.ipynb#END-of-Exam\" data-toc-modified-id=\"END-of-Exam-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>END of Exam</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS w261 Machine Learning at Scale\n",
    "## MidTerm Exam  \n",
    "\n",
    "\n",
    "MIDS Machine Learning at Scale\n",
    "\n",
    "\n",
    "\n",
    "### Please insert your contact information here\n",
    "__Insert you name here__           : Jennifer Casper  \n",
    "__Insert you email here__          : jenncasper@berkeley.edu   \n",
    "__Insert your  UC Berkeley ID here__: 3031886443\n",
    "\n",
    "# Exam Instructions\n",
    "\n",
    "1. : Please insert Name and Email address in the first cell of this notebook\n",
    "2. : Please keep all your work and responses in ONE (1) notebook only \n",
    "3. : For the midterm you will need access to MrJob and Jupyter on your local machines (should be more than sufficient) or on Altiscale/AWS to complete some of the questions (like fill in the code to do X).\n",
    "4. : As for question types:\n",
    "    + Knowledge test Programmatic/doodle (take photos; embed the photos in your notebook, along with the photos directory in a zip file) \n",
    "    + All programmatic questions can be run locally on your laptop (using MrJob only) or on the cluster\n",
    "\n",
    "5. : This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not discuss with each other or any other person/group. If any collusion, then this will result in a zero grade and will be grounds for dismissal from the entire program. Please complete this exam by yourself within the time limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Question Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and  Starter code for questions 6 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\r\n",
      "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
     ]
    }
   ],
   "source": [
    "!cat kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "#coding: utf-8\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class kldivergence(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, 0.2]\n",
    "    # emit \"b\"    [1, 0.4] etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "\n",
    "    # on a component i calculate (e.g., \"b\")\n",
    "    # Kullback–Leibler divergence of Q from P is defined\n",
    "    #  (P(i) log (P(i) / Q(i))\n",
    "    #\n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:  #String 1\n",
    "                p = v[1]\n",
    "            else:          # String 2\n",
    "                q = v[1]\n",
    "                \n",
    "        ###### SOLUTION #############        \n",
    "        #yield # your code here\n",
    "        val = p * math.log(float(p)/q)\n",
    "        yield (None, val)\n",
    "        #############################\n",
    "\n",
    "    #Aggegate components            \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                \n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/kldivergence.root.20171019.002145.413039\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/kldivergence.root.20171019.002145.413039/output...\n",
      "\"KLDivergence\"\t0.0808827844\n",
      "Removing temp directory /tmp/kldivergence.root.20171019.002145.413039...\n"
     ]
    }
   ],
   "source": [
    "!python kldivergence.py -r local kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'KLDivergence', 0.0808827844)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob.job import MRJob\n",
    "from kldivergence import kldivergence\n",
    "\n",
    "#dont forget to save kltext.txt (see earlier cell)\n",
    "mr_job = kldivergence(args=['kltext.txt'])\n",
    "#print mr_job\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence_smooth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence_smooth.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class kldivergence_smooth(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, (1+1)/(5+24)]\n",
    "    # emit \"b\"    [1, (2+1)/(5+24) etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        \n",
    "        # (ni+1)/(n+24)\n",
    "        \n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            ###### SOLUTION ############# \n",
    "            #yield #your code here\n",
    "            yield key, [index, (count[key]+1)*(1.0/(len(letter_list)*24))]\n",
    "\n",
    "    \n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:\n",
    "                p = v[1]\n",
    "            else:\n",
    "                q = v[1]\n",
    "        ###### SOLUTION #############         \n",
    "        #yield # your code here\n",
    "        val = p * math.log(float(p)/q)\n",
    "        yield (None, val)\n",
    "\n",
    "    # Aggregate components             \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence_smooth.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/kldivergence_smooth.root.20171019.003144.911731\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/kldivergence_smooth.root.20171019.003144.911731/output...\n",
      "\"KLDivergence\"\t0.0033441926\n",
      "Removing temp directory /tmp/kldivergence_smooth.root.20171019.003144.911731...\n"
     ]
    }
   ],
   "source": [
    "!python kldivergence_smooth.py -r local kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'KLDivergence', 0.0033441926)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from kldivergence_smooth import kldivergence_smooth\n",
    "mr_job = kldivergence_smooth(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for question 11"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "# Record format\n",
    "#   Class docID:\"doc contents string\"\n",
    "ham d1: “good.”\n",
    "ham d2: “very good.”\n",
    "spam d3: “bad.”\n",
    "spam d4: “very bad.”\n",
    "spam d5: “very bad, very BAD.”"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test Data\n",
    "? d6: “good? bad! very Bad!” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        \n",
    "        print \"Centroids: \", self.centroid_points\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/Kmeans.root.20171019.005217.316184\n",
      "Running step 1 of 1...\n",
      "Traceback (most recent call last):\n",
      "  File \"Kmeans.py\", line 73, in <module>\n",
      "    MRKmeans.run()\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/job.py\", line 439, in run\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/job.py\", line 451, in execute\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/job.py\", line 612, in run_combiner\n",
      "  File \"Kmeans.py\", line 52, in combiner\n",
      "    for x,y,n in inputdata:\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/job.py\", line 611, in <genexpr>\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/job.py\", line 708, in read_lines\n",
      "  File \"/tmp/Kmeans.root.20171019.005217.316184/job_local_dir/0/mapper/0/mrjob.zip/mrjob/protocol.py\", line 90, in read\n",
      "ValueError: need more than 1 value to unpack\n",
      "Step 1 of 1 failed: Command '['sh', '-ex', 'setup-wrapper.sh', '/opt/anaconda/bin/python', 'Kmeans.py', '--step-num=0', '--combiner']' returned non-zero exit status 1\n"
     ]
    }
   ],
   "source": [
    "!python Kmeans.py -r local Kmeandata.csv --file=Centroids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-1.20784625035, -1.29634172982], [-1.03672454029, -0.239157997242], [0.266210156746, -1.69559478144]]\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-1.20784625035, -1.29634172982], [-1.03672454029, -0.239157997242], [0.266210156746, -1.69559478144]]\n",
      "2 [5.057319669, -0.0453899984]\n",
      "0 [-5.2416540286, -1.0465461339]\n",
      "1 [-1.6195809805999999, 3.5384873393]\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-5.2416540286, -1.0465461339], [-1.6195809806, 3.5384873393], [5.057319669, -0.0453899984]]\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-5.2416540286, -1.0465461339], [-1.6195809806, 3.5384873393], [5.057319669, -0.0453899984]]\n",
      "2 [5.023800693, 0.0016164835000000001]\n",
      "0 [-5.1054967261, -0.12404451030000001]\n",
      "1 [-0.1820097262, 4.7940655784]\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-5.1054967261, -0.1240445103], [-0.1820097262, 4.7940655784], [5.023800693, 0.0016164835]]\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-5.1054967261, -0.1240445103], [-0.1820097262, 4.7940655784], [5.023800693, 0.0016164835]]\n",
      "2 [5.0402327161, -0.026294230000000002]\n",
      "0 [-4.9960056034, -0.0080036745]\n",
      "1 [0.0431289365, 4.9768153511]\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-4.9960056034, -0.0080036745], [0.0431289365, 4.9768153511], [5.0402327161, -0.02629423]]\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-4.9960056034, -0.0080036745], [0.0431289365, 4.9768153511], [5.0402327161, -0.02629423]]\n",
      "2 [5.0402327161, -0.026294230000000002]\n",
      "0 [-4.9858056889, 0.0009376094000000001]\n",
      "1 [0.053065423800000004, 4.9877934239]\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-4.9858056889, 0.0009376094], [0.0530654238, 4.9877934239], [5.0402327161, -0.02629423]]\n",
      "Current path: /media/notebooks/media/notebooks/MidTerm/stuff\n",
      "Centroids:  [[-4.9858056889, 0.0009376094], [0.0530654238, 4.9877934239], [5.0402327161, -0.02629423]]\n",
      "2 [5.0402327161, -0.026294230000000002]\n",
      "0 [-4.9858056889, 0.0009376094000000001]\n",
      "1 [0.053065423800000004, 4.9877934239]\n",
      "\n",
      "\n",
      "Centroids\n",
      "\n",
      "[[-4.9858056889, 0.0009376094000000001], [0.053065423800000004, 4.9877934239], [5.0402327161, -0.026294230000000002]]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy import random\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv', '--file=Centroids.txt'])\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = []\n",
    "k = 3\n",
    "for i in range(k):\n",
    "    centroid_points.append([random.uniform(-3,3),random.uniform(-3,3)])\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "            \n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "        \n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "        break\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Starter Code for question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = np.array([6, -3, -2, 1])\n",
    "x = np.array([[1], \n",
    "              [2], \n",
    "              [3], \n",
    "              [4]])\n",
    "y = 2\n",
    "lam = 0.1\n",
    "total_loss = 22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Starter Code for question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate data artificially\n",
    "X = np.arange(1, 20)\n",
    "\n",
    "k, b = 1.5, 3\n",
    "\n",
    "y = k * X + b\n",
    "\n",
    "np.random.seed(21)\n",
    "y += np.random.normal(loc=0.0, scale=3.0, size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1774689cdc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Real\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lower right\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot([0, 20], [b, k * 20 + b], \"r\", linewidth=2, label=\"Real\")\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([0, 35])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicLinearRegressionHomegrown(object):\n",
    "    \n",
    "    def __init__(self, l1=0.0, l2=0.0):\n",
    "        self.coef_ = None       # weight vector\n",
    "        self.intercept_ = None  # bias term\n",
    "        self._theta = None      # augmented weight vector, i.e., bias + weights\n",
    "                                # this allows to treat all decision variables homogeneously\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        \n",
    "    # calculate gradient of objective function\n",
    "    def _grad(self, X, y):\n",
    "        pred = np.dot(X, self._theta)\n",
    "        error = pred - y\n",
    "        gradient = 2 * np.dot(error, X) / X.shape[0]\n",
    "        return gradient\n",
    "    \n",
    "    # full gradient descent, i.e., not stochastic gd\n",
    "    def _gd(self, X, y, max_iter, alpha=0.005):\n",
    "        for i in range(max_iter):\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y)\n",
    "            # do gradient step\n",
    "            self._theta -= alpha * grad\n",
    "    \n",
    "    # public API for fitting a linear regression model\n",
    "    def fit(self, X, y, max_iter=1000):\n",
    "        # Augment the data with the bias term.\n",
    "        # So we can treat the the input variables and the bias term homogeneously \n",
    "        # from a vectorization perspective\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        # initialize if the first step\n",
    "        if self._theta is None:\n",
    "            np.random.seed(21)\n",
    "            self._theta = np.random.rand(X.shape[1])\n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, max_iter)\n",
    "        \n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        error = pred - y\n",
    "        obj = np.sum(error ** 2) / X.shape[0]\n",
    "        return obj\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # check whether X has appended bias feature or not\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            pred = np.dot(X, self._theta)\n",
    "        else:\n",
    "            pred = np.dot(X, self.coef_) + self.intercept_\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code for question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicLinearRegressionHomegrown(object):\n",
    "    \n",
    "    def __init__(self, l=0.0, mu=0.0):\n",
    "        self.coef_ = None       # weight vector\n",
    "        self.intercept_ = None  # bias term\n",
    "        self._theta = None      # augmented weight vector, i.e., bias + weights\n",
    "                                # this allows to treat all decision variables homogeneously\n",
    "        self.l = l\n",
    "        self.mu = mu\n",
    "    \n",
    "    # calculate gradient of objective function\n",
    "    def _grad(self, X, y):\n",
    "        pred = np.dot(X, self._theta)\n",
    "        error = pred - y\n",
    "        gradient = 2 * np.dot(error, X) / X.shape[0]\n",
    "        return gradient\n",
    "    \n",
    "    # full gradient descent, i.e., not stochastic gd\n",
    "    def _gd(self, X, y, max_iter, alpha=0.005):\n",
    "        for i in range(max_iter):\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y)\n",
    "            # do gradient step\n",
    "            self._theta -= alpha * grad\n",
    "    \n",
    "    # public API for fitting a linear regression model\n",
    "    def fit(self, X, y, max_iter=1000):\n",
    "        # Augment the data with the bias term.\n",
    "        # So we can treat the the input variables and the bias term homogeneously \n",
    "        # from a vectorization perspective\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        # initialize if the first step\n",
    "        if self._theta is None:\n",
    "            np.random.seed(21)\n",
    "            self._theta = np.random.rand(X.shape[1])\n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, max_iter)\n",
    "        \n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        error = pred - y\n",
    "        obj = np.sum(error ** 2) / X.shape[0]\n",
    "        return obj\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # check whether X has appended bias feature or not\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            pred = np.dot(X, self._theta)\n",
    "        else:\n",
    "            pred = np.dot(X, self.coef_) + self.intercept_\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# END of Exam"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
