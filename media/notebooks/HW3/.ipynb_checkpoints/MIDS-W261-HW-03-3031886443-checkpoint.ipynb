{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW3\" data-toc-modified-id=\"Assignment---HW3-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW3</a></div><div class=\"lev2 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSION\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSION-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSION</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.0-Questions\" data-toc-modified-id=\"HW3.0-Questions-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>HW3.0 Questions</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.1-consumer-complaints-dataset:-Use-Counters-to-do-EDA-(exploratory-data-analysis-and-to-monitor-progress)\" data-toc-modified-id=\"HW3.1-consumer-complaints-dataset:-Use-Counters-to-do-EDA-(exploratory-data-analysis-and-to-monitor-progress)-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)</a></div><div class=\"lev4 toc-item\"><a href=\"#Acquire-and-prep-consumer-complaints-dataset\" data-toc-modified-id=\"Acquire-and-prep-consumer-complaints-dataset-1401\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>Acquire and prep consumer complaints dataset</a></div><div class=\"lev4 toc-item\"><a href=\"#Mapper-and-reducer-with-counters\" data-toc-modified-id=\"Mapper-and-reducer-with-counters-1402\"><span class=\"toc-item-num\">1.4.0.2&nbsp;&nbsp;</span>Mapper and reducer with counters</a></div><div class=\"lev4 toc-item\"><a href=\"#Hadoop-streaming-command\" data-toc-modified-id=\"Hadoop-streaming-command-1403\"><span class=\"toc-item-num\">1.4.0.3&nbsp;&nbsp;</span>Hadoop-streaming command</a></div><div class=\"lev4 toc-item\"><a href=\"#Screenshot-of-the-3-counter-values-in-the-JobTracker-UI\" data-toc-modified-id=\"Screenshot-of-the-3-counter-values-in-the-JobTracker-UI-1404\"><span class=\"toc-item-num\">1.4.0.4&nbsp;&nbsp;</span>Screenshot of the 3 counter values in the JobTracker UI</a></div><div class=\"lev3 toc-item\"><a href=\"#HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters\" data-toc-modified-id=\"HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.A-SOLUTION\" data-toc-modified-id=\"3.2.A-SOLUTION-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>3.2.A SOLUTION</a></div><div class=\"lev4 toc-item\"><a href=\"#Prep-one-record-input-file\" data-toc-modified-id=\"Prep-one-record-input-file-1421\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>Prep one record input file</a></div><div class=\"lev4 toc-item\"><a href=\"#Mapper-and-reducer\" data-toc-modified-id=\"Mapper-and-reducer-1422\"><span class=\"toc-item-num\">1.4.2.2&nbsp;&nbsp;</span>Mapper and reducer</a></div><div class=\"lev4 toc-item\"><a href=\"#Hadoop-streaming-command\" data-toc-modified-id=\"Hadoop-streaming-command-1423\"><span class=\"toc-item-num\">1.4.2.3&nbsp;&nbsp;</span>Hadoop-streaming command</a></div><div class=\"lev4 toc-item\"><a href=\"#Screenshot-of-the-Mapper-and-Reducer-counter-values-in-the-JobTracker-UI\" data-toc-modified-id=\"Screenshot-of-the-Mapper-and-Reducer-counter-values-in-the-JobTracker-UI-1424\"><span class=\"toc-item-num\">1.4.2.4&nbsp;&nbsp;</span>Screenshot of the Mapper and Reducer counter values in the JobTracker UI</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.A-EXPLANATION\" data-toc-modified-id=\"3.2.A-EXPLANATION-1425\"><span class=\"toc-item-num\">1.4.2.5&nbsp;&nbsp;</span>3.2.A EXPLANATION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.B-SOLUTION\" data-toc-modified-id=\"3.2.B-SOLUTION-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>3.2.B SOLUTION</a></div><div class=\"lev4 toc-item\"><a href=\"#Mapper-and-reducer\" data-toc-modified-id=\"Mapper-and-reducer-1431\"><span class=\"toc-item-num\">1.4.3.1&nbsp;&nbsp;</span>Mapper and reducer</a></div><div class=\"lev4 toc-item\"><a href=\"#Hadoop-streamining-command\" data-toc-modified-id=\"Hadoop-streamining-command-1432\"><span class=\"toc-item-num\">1.4.3.2&nbsp;&nbsp;</span>Hadoop-streamining command</a></div><div class=\"lev4 toc-item\"><a href=\"#Screenshot-of-the-Mapper-and-Reducer-counter-values-in-the-JobTracker-UI\" data-toc-modified-id=\"Screenshot-of-the-Mapper-and-Reducer-counter-values-in-the-JobTracker-UI-1433\"><span class=\"toc-item-num\">1.4.3.3&nbsp;&nbsp;</span>Screenshot of the Mapper and Reducer counter values in the JobTracker UI</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.B-EXPLANATION\" data-toc-modified-id=\"3.2.B-EXPLANATION-1434\"><span class=\"toc-item-num\">1.4.3.4&nbsp;&nbsp;</span>3.2.B EXPLANATION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.C-SOLUTION\" data-toc-modified-id=\"3.2.C-SOLUTION-144\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>3.2.C SOLUTION</a></div><div class=\"lev4 toc-item\"><a href=\"#Mapper,-combiner,-and-reducer\" data-toc-modified-id=\"Mapper,-combiner,-and-reducer-1441\"><span class=\"toc-item-num\">1.4.4.1&nbsp;&nbsp;</span>Mapper, combiner, and reducer</a></div><div class=\"lev4 toc-item\"><a href=\"#Hadoop-streaming-command\" data-toc-modified-id=\"Hadoop-streaming-command-1442\"><span class=\"toc-item-num\">1.4.4.2&nbsp;&nbsp;</span>Hadoop-streaming command</a></div><div class=\"lev4 toc-item\"><a href=\"#Screenshot-of-the-Combiner,-Mapper,-and-Reducer-counter-values-in-the-JobTracker-UI\" data-toc-modified-id=\"Screenshot-of-the-Combiner,-Mapper,-and-Reducer-counter-values-in-the-JobTracker-UI-1443\"><span class=\"toc-item-num\">1.4.4.3&nbsp;&nbsp;</span>Screenshot of the Combiner, Mapper, and Reducer counter values in the JobTracker UI</a></div><div class=\"lev4 toc-item\"><a href=\"#Word-count-frequency-in-Issues-field-and-top/bottom-counts-using-one-reducer\" data-toc-modified-id=\"Word-count-frequency-in-Issues-field-and-top/bottom-counts-using-one-reducer-1444\"><span class=\"toc-item-num\">1.4.4.4&nbsp;&nbsp;</span>Word count frequency in Issues field and top/bottom counts using one reducer</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.C-EXPLANATION\" data-toc-modified-id=\"3.2.C-EXPLANATION-1445\"><span class=\"toc-item-num\">1.4.4.5&nbsp;&nbsp;</span>3.2.C EXPLANATION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.1--Two-Reducers\" data-toc-modified-id=\"3.2.1--Two-Reducers-145\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>3.2.1  Two Reducers</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)-1451\"><span class=\"toc-item-num\">1.4.5.1&nbsp;&nbsp;</span>START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW321\" data-toc-modified-id=\"END-STUDENT-CODE-HW321-1452\"><span class=\"toc-item-num\">1.4.5.2&nbsp;&nbsp;</span>END STUDENT CODE HW321</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.1-EXPLANATION\" data-toc-modified-id=\"3.2.1-EXPLANATION-1453\"><span class=\"toc-item-num\">1.4.5.3&nbsp;&nbsp;</span>3.2.1 EXPLANATION</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.3.-Shopping-Cart-Analysis-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>HW3.3. Shopping Cart Analysis</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)-1501\"><span class=\"toc-item-num\">1.5.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#Piped-linux-commands-for-exploring\" data-toc-modified-id=\"Piped-linux-commands-for-exploring-1502\"><span class=\"toc-item-num\">1.5.0.2&nbsp;&nbsp;</span>Piped linux commands for exploring</a></div><div class=\"lev4 toc-item\"><a href=\"#Top-50-most-frequently-purchased-items,-frequency,-and-relative-frequency\" data-toc-modified-id=\"Top-50-most-frequently-purchased-items,-frequency,-and-relative-frequency-1503\"><span class=\"toc-item-num\">1.5.0.3&nbsp;&nbsp;</span>Top 50 most frequently purchased items, frequency, and relative frequency</a></div><div class=\"lev4 toc-item\"><a href=\"#3.3-EXPLANATION\" data-toc-modified-id=\"3.3-EXPLANATION-1504\"><span class=\"toc-item-num\">1.5.0.4&nbsp;&nbsp;</span>3.3 EXPLANATION</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW33\" data-toc-modified-id=\"END-STUDENT-CODE-HW33-1505\"><span class=\"toc-item-num\">1.5.0.5&nbsp;&nbsp;</span>END STUDENT CODE HW33</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.1-OPTIONAL\" data-toc-modified-id=\"HW3.3.1-OPTIONAL-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>HW3.3.1 OPTIONAL</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)-1601\"><span class=\"toc-item-num\">1.6.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW331\" data-toc-modified-id=\"END-STUDENT-CODE-HW331-1602\"><span class=\"toc-item-num\">1.6.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW331</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs\" data-toc-modified-id=\"HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)-1701\"><span class=\"toc-item-num\">1.7.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#3.4-EXPLANATION\" data-toc-modified-id=\"3.4-EXPLANATION-1702\"><span class=\"toc-item-num\">1.7.0.2&nbsp;&nbsp;</span>3.4 EXPLANATION</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW34\" data-toc-modified-id=\"END-STUDENT-CODE-HW34-1703\"><span class=\"toc-item-num\">1.7.0.3&nbsp;&nbsp;</span>END STUDENT CODE HW34</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.5:-Stripes\" data-toc-modified-id=\"HW3.5:-Stripes-18\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>HW3.5: Stripes</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)-1801\"><span class=\"toc-item-num\">1.8.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#3.5-EXPLANATION\" data-toc-modified-id=\"3.5-EXPLANATION-1802\"><span class=\"toc-item-num\">1.8.0.2&nbsp;&nbsp;</span>3.5 EXPLANATION</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW35\" data-toc-modified-id=\"END-STUDENT-CODE-HW35-1803\"><span class=\"toc-item-num\">1.8.0.3&nbsp;&nbsp;</span>END STUDENT CODE HW35</a></div><div class=\"lev1 toc-item\"><a href=\"#OPTIONAL\" data-toc-modified-id=\"OPTIONAL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>OPTIONAL</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)\" data-toc-modified-id=\"HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.7-Apriori-Algorithm\" data-toc-modified-id=\"HW3.7-Apriori-Algorithm-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>HW3.7 Apriori Algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.8.-Shopping-Cart-Analysis-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>HW3.8. Shopping Cart Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.1\" data-toc-modified-id=\"HW3.8.1-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>HW3.8.1</a></div><div class=\"lev1 toc-item\"><a href=\"#END-OF-HOMEWORK\" data-toc-modified-id=\"END-OF-HOMEWORK-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>END OF HOMEWORK</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW3\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Jennifer Casper*   \n",
    "__Class:__ MIDS w261 (Section *Fall 2017 Group 2*)     \n",
    "__Email:__  *jenncasper*@berkeley.edu     \n",
    "__StudentId__  3031886443    __End of StudentId__     \n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "\n",
    "## INSTRUCTIONS for SUBMISSION\n",
    "\n",
    "This homework can be completed locally on your computer. __Please submit your notebook to your classroom github repository 24 hours prior to the next live session.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0 Questions\n",
    "\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "\n",
    "   With pointers to the head of each sorted list and a final list/array available to hold the result, the minimum (or maximum) value from sorted list1 or list2 is copied to the final list. The pointers for the final and min value list are incremented and the minimum (or maximum) assessment occurs again until all elements of the two sorted lists have been evaluated.<br><br>\n",
    "\n",
    "2. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "\n",
    "   Merge sorts occur during the shuffle and sort phase between the mappers an reducers. Hadoop ensures outputs from the mappers are merged, sorted, and partitioned so reducers receive data partitioned by key.<br><br>\n",
    "\n",
    "3. What is  a combiner function in the context of Hadoop? \n",
    "\n",
    "   A combiner function combines records with the same key in order to reduce network traffic. Hadoop may or may not execute the combiner function between the mappers and reducers.<br><br>\n",
    "\n",
    "4. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "   In the word count example, the combiner may be used to sum up the values for keys from the individual mappers. This would reduce the amount of data to be sorted and sent to the reducer for a final sum and sort.<br><br>\n",
    "\n",
    "5. What is the Hadoop shuffle?\n",
    "\n",
    "   The shuffle is the heart of Hadoop Map Reduce. The shuffle is responsible for the sorting and transfering mapper outputs to the reducers as input. The shuffle stage partitions, sorts, and combines data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "```\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "```\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire and prep consumer complaints dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-09-17 15:11:51--  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
      "Resolving www.dropbox.com... 162.125.6.1, 2620:100:601c:1:ffff:ffff:a27d:601\n",
      "Connecting to www.dropbox.com|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/SHnYsy8bXdoemrm6FdubBWw7MWglSPyOsTWs1i0mQHE15Dar5LviP3OHGye1xvPG/file [following]\n",
      "--2017-09-17 15:11:51--  https://dl.dropboxusercontent.com/content_link/SHnYsy8bXdoemrm6FdubBWw7MWglSPyOsTWs1i0mQHE15Dar5LviP3OHGye1xvPG/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.6.6, 2620:100:601c:6:ffff:ffff:a27d:606\n",
      "Connecting to dl.dropboxusercontent.com|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50906486 (49M) [text/csv]\n",
      "Saving to: `consumer_complaints.csv'\n",
      "\n",
      "100%[======================================>] 50,906,486  6.10M/s   in 7.8s    \n",
      "\n",
      "2017-09-17 15:12:00 (6.25 MB/s) - `consumer_complaints.csv' saved [50906486/50906486]\n",
      "\n",
      "\n",
      "312913 consumer_complaints.csv\n",
      "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
      "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
      "\n",
      "\n",
      "312912 consumer_complaints.csv\n",
      "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
      "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n"
     ]
    }
   ],
   "source": [
    "# Download text\n",
    "!wget 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0' -O consumer_complaints.csv\n",
    "\n",
    "# Check line number and output the first two lines\n",
    "!printf \"\\n\"\n",
    "!wc -l consumer_complaints.csv\n",
    "!head -2 consumer_complaints.csv\n",
    "# Remove the header line in place - only do this once\n",
    "!printf \"\\n\"\n",
    "!ex -sc '1d|x' consumer_complaints.csv\n",
    "# Check line number and output the first two lines post header removal\n",
    "!printf \"\\n\"\n",
    "!wc -l consumer_complaints.csv\n",
    "!head -2 consumer_complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper and reducer with counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw31_complaintCountsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw31_complaintCountsMapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31MAPPER\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw31_complaintCountsMapper running\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split(',')\n",
    "    print '%s\\t%s' % (values[1], 1)\n",
    "    if values[1] == \"Debt collection\":\n",
    "        sys.stderr.write(\"reporter:counter:Debt collection Counters,Calls,1\\n\")  \n",
    "    elif values[1] == \"Mortgage\":\n",
    "        sys.stderr.write(\"reporter:counter:Mortgage Counters,Calls,1\\n\") \n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Other Counters,Calls,1\\n\")\n",
    "\n",
    "# END STUDENT CODE HW31MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:complaintCountsMapper running\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Bank account or service\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Consumer loan\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Consumer loan\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Mortgage\t1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Mortgage\t1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Bank account or service\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Consumer loan\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Bank account or service\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Bank account or service\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Mortgage\t1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Bank account or service\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit card\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "Debt collection\t1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "Credit reporting\t1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n"
     ]
    }
   ],
   "source": [
    "# Unit test the mapper\n",
    "!chmod a+x hw31_complaintCountsMapper.py\n",
    "!head -50 consumer_complaints.csv | ./hw31_complaintCountsMapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw31_complaintCountsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw31_complaintCountsReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31REDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split('\\t')\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW31REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:complaintCountsMapper running\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Mortgage Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Debt collection Counters,Calls,1\r\n",
      "reporter:counter:Other Counters,Calls,1\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "Bank account or service\t5\r\n",
      "Consumer loan\t3\r\n",
      "Credit card\t1\r\n",
      "Credit reporting\t18\r\n",
      "Debt collection\t20\r\n",
      "Mortgage\t3\r\n"
     ]
    }
   ],
   "source": [
    "# System test the reducer\n",
    "!chmod a+x hw31_complaintCountsReducer.py\n",
    "!head -50 consumer_complaints.csv | ./hw31_complaintCountsMapper.py | sort -k1,1 | ./hw31_complaintCountsReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop-streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw31_complaintCountsMapper.py\n",
    "!chmod a+x hw31_complaintCountsReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted consumer_complaints.csv\n",
      "Deleted hw31_EDA-Counters\n",
      "17/09/17 16:24:04 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob1362678980419744964.jar tmpDir=null\n",
      "17/09/17 16:24:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 16:24:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 16:24:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/17 16:24:07 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/17 16:24:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0121\n",
      "17/09/17 16:24:07 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0121\n",
      "17/09/17 16:24:07 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0121/\n",
      "17/09/17 16:24:07 INFO mapreduce.Job: Running job: job_1504992575111_0121\n",
      "17/09/17 16:24:16 INFO mapreduce.Job: Job job_1504992575111_0121 running in uber mode : false\n",
      "17/09/17 16:24:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/17 16:24:26 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/17 16:24:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/17 16:24:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/17 16:24:35 INFO mapreduce.Job: Job job_1504992575111_0121 completed successfully\n",
      "17/09/17 16:24:35 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5504142\n",
      "\t\tFILE: Number of bytes written=11444783\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910642\n",
      "\t\tHDFS: Number of bytes written=184\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15515\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7381\n",
      "\t\tTotal time spent by all map tasks (ms)=15515\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7381\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15515\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7381\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15887360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7558144\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=4878312\n",
      "\t\tMap output materialized bytes=5504148\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=5504148\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=121\n",
      "\t\tCPU time spent (ms)=11200\n",
      "\t\tPhysical memory (bytes) snapshot=872235008\n",
      "\t\tVirtual memory (bytes) snapshot=4124389376\n",
      "\t\tTotal committed heap usage (bytes)=688390144\n",
      "\tDebt collection Counters\n",
      "\t\tCalls=44372\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tMortgage Counters\n",
      "\t\tCalls=125752\n",
      "\tOther Counters\n",
      "\t\tCalls=142788\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=184\n",
      "17/09/17 16:24:35 INFO streaming.StreamJob: Output directory: hw31_EDA-Counters\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW31HADOOP\n",
    "\n",
    "# Clean and set up HDFS environment\n",
    "!hdfs dfs -rm consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal consumer_complaints.csv \n",
    "!hdfs dfs -rm -r hw31_EDA-Counters\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.1 Consumer Complaints EDA\" \\\n",
    "  -files ./hw31_complaintCountsMapper.py,./hw31_complaintCountsReducer.py \\\n",
    "  -mapper hw31_complaintCountsMapper.py \\\n",
    "  -reducer hw31_complaintCountsReducer.py \\\n",
    "  -input consumer_complaints.csv \\\n",
    "  -output hw31_EDA-Counters  \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW31HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank account or service\t38073\r\n",
      "Consumer loan\t9387\r\n",
      "Credit card\t41563\r\n",
      "Credit reporting\t41214\r\n",
      "Debt collection\t44372\r\n",
      "Money transfers\t1540\r\n",
      "Mortgage\t125752\r\n",
      "Payday loan\t1579\r\n",
      "Student loan\t9432\r\n"
     ]
    }
   ],
   "source": [
    "# Product distribution output\n",
    "#!hdfs dfs -ls hw31_EDA-Counters\n",
    "!hdfs dfs -cat hw31_EDA-Counters/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot of the 3 counter values in the JobTracker UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hw31_screenshot.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"hw31_screenshot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only):    \n",
    "`foo foo quux labs foo bar quux`\n",
    "\n",
    "\n",
    "__3.2.A__     \n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "__3.2.B__   \n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "__3.2.C__     \n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "__NOTE:__ You can use: `WORD_RE = re.compile(r\"[\\w']+\")` to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.A SOLUTION\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. **What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job?** The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep one record input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 perf_input.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Prep the single line file input\n",
    "!printf \"foo foo quux labs foo bar quux\\n\" > perf_input.txt\n",
    "# Check the line count\n",
    "!wc -l perf_input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper and reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32A_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32A_mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AMAPPER\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32A_mapper running\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        print '%s\\t%s' % (word, 1) \n",
    "            \n",
    "# END STUDENT CODE HW32AMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32A_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "foo\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "foo\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "quux\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "labs\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "foo\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "bar\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Unit test the mapper\n",
    "!chmod a+x hw32A_mapper.py\n",
    "!cat perf_input.txt | ./hw32A_mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32A_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32A_reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW32AREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32A_mapper running\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "# System test the reducer\n",
    "!chmod a+x hw32A_reducer.py\n",
    "!cat perf_input.txt | ./hw32A_mapper.py| sort -k1,1 | ./hw32A_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop-streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw32A_mapper.py\n",
    "!chmod a+x hw32A_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted perf_input.txt\n",
      "Deleted hw32A_performance\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob4279838743133081808.jar tmpDir=null\n",
      "17/09/17 19:50:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 19:50:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 19:50:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/17 19:50:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "17/09/17 19:50:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0133\n",
      "17/09/17 19:50:32 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0133\n",
      "17/09/17 19:50:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0133/\n",
      "17/09/17 19:50:32 INFO mapreduce.Job: Running job: job_1504992575111_0133\n",
      "17/09/17 19:50:41 INFO mapreduce.Job: Job job_1504992575111_0133 running in uber mode : false\n",
      "17/09/17 19:50:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/17 19:50:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/17 19:50:54 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/09/17 19:50:55 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/17 19:50:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/17 19:50:58 INFO mapreduce.Job: Job job_1504992575111_0133 completed successfully\n",
      "17/09/17 19:50:58 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=726723\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=139\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3771\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23765\n",
      "\t\tTotal time spent by all map tasks (ms)=3771\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23765\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3771\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=23765\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3861504\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24335360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=108\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=148\n",
      "\t\tCPU time spent (ms)=5240\n",
      "\t\tPhysical memory (bytes) snapshot=1047969792\n",
      "\t\tVirtual memory (bytes) snapshot=6851694592\n",
      "\t\tTotal committed heap usage (bytes)=750256128\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=7\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "17/09/17 19:50:58 INFO streaming.StreamJob: Output directory: hw32A_performance\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32AHADOOP\n",
    "\n",
    "# Clean and set up HDFS environment\n",
    "!hdfs dfs -rm perf_input.txt \n",
    "!hdfs dfs -copyFromLocal perf_input.txt \n",
    "!hdfs dfs -rm -r hw32A_performance\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.A Performance\" \\\n",
    "  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "  -files ./hw32A_mapper.py,./hw32A_reducer.py \\\n",
    "  -mapper hw32A_mapper.py \\\n",
    "  -reducer hw32A_reducer.py \\\n",
    "  -input perf_input.txt \\\n",
    "  -output hw32A_performance  \\\n",
    "  -numReduceTasks 4\n",
    "\n",
    "# END STUDENT CODE HW32AHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "# Performance output\n",
    "#!hdfs dfs -ls hw32A_performance\n",
    "!hdfs dfs -cat hw32A_performance/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot of the Mapper and Reducer counter values in the JobTracker UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hw32A_screenshot.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"hw32A_screenshot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2.A EXPLANATION\n",
    "\n",
    "The default hadoop-streaming job, without forcing 1 mapper and 4 reducers, creates 2 mappers and 1 reducer. Hadoop configuration is likely set to 2 for a minimum number of mappers. This is inputs smaller than the minsplitsize still results in 2 mappers. The default number of reducers is set to one but may be changed with a parameter to the job declaration. Using the minsize and numReduceTasks options, 1 mapper and 4 reducers will be used. Perhaps this is worthwhile to create a reducer for each unique key if that is known ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.B SOLUTION\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers). Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere) using user defined Counters to count up how many time the mapper and reducer are called. **What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper and reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32B_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32B_mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BMAPPER\n",
    "\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32B_mapper running\\n\")\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split(',')\n",
    "    issueStr = regex.sub('', values[3].lower())\n",
    "    #print issueStr\n",
    "    for word in issueStr.split():\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        print '%s\\t%s' % (word, 1) \n",
    "        \n",
    "# END STUDENT CODE HW32BMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32B_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "disclosure\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "verification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "of\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "disclosure\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "verification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "of\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "deposits\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "and\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "withdrawals\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "managing\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "the\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "or\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "lease\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "false\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "statements\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "or\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "representation\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "managing\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "the\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "or\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "lease\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "disclosure\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "verification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "of\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporting\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "companys\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "investigation\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "communication\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "tactics\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "modification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "unable\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "to\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "get\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reportcredit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "score\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporting\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "companys\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "investigation\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "modification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "deposits\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "and\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "withdrawals\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporting\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "companys\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "investigation\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "taking\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "out\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "the\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "or\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "lease\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "deposits\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "and\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "withdrawals\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "deposits\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "and\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "withdrawals\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "loan\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "modification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "problems\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "caused\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "by\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "my\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "funds\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "being\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "low\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "communication\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "tactics\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "unable\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "to\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "get\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reportcredit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "score\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "health\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "club\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "advertising\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "and\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "marketing\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "contd\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "attempts\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "collect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "not\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "owed\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "incorrect\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "information\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "on\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "credit\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "report\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Unit test the mapper\n",
    "!chmod a+x hw32B_mapper.py\n",
    "!head -50 consumer_complaints.csv | ./hw32B_mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32B_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32B_reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW32BREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32B_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "advertising\t1\r\n",
      "and\t5\r\n",
      "attempts\t8\r\n",
      "being\t1\r\n",
      "by\t1\r\n",
      "caused\t1\r\n",
      "club\t6\r\n",
      "collect\t8\r\n",
      "communication\t2\r\n",
      "companys\t3\r\n",
      "contd\t8\r\n",
      "credit\t18\r\n",
      "debt\t11\r\n",
      "deposits\t4\r\n",
      "disclosure\t3\r\n",
      "false\t1\r\n",
      "funds\t1\r\n",
      "get\t2\r\n",
      "health\t6\r\n",
      "incorrect\t13\r\n",
      "information\t13\r\n",
      "investigation\t3\r\n",
      "lease\t3\r\n",
      "loan\t6\r\n",
      "low\t1\r\n",
      "managing\t2\r\n",
      "marketing\t1\r\n",
      "modification\t3\r\n",
      "my\t1\r\n",
      "not\t8\r\n",
      "of\t3\r\n",
      "on\t13\r\n",
      "or\t4\r\n",
      "out\t1\r\n",
      "owed\t8\r\n",
      "problems\t1\r\n",
      "report\t13\r\n",
      "reportcredit\t2\r\n",
      "reporting\t3\r\n",
      "representation\t1\r\n",
      "score\t2\r\n",
      "statements\t1\r\n",
      "tactics\t2\r\n",
      "taking\t1\r\n",
      "the\t3\r\n",
      "to\t2\r\n",
      "unable\t2\r\n",
      "verification\t3\r\n",
      "withdrawals\t4\r\n"
     ]
    }
   ],
   "source": [
    "# System test the reducer\n",
    "!chmod a+x hw32B_reducer.py\n",
    "!head -50 consumer_complaints.csv | ./hw32B_mapper.py  | sort -k1,1 | ./hw32B_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop-streamining command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw32B_mapper.py\n",
    "!chmod a+x hw32B_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted consumer_complaints.csv\n",
      "Deleted hw32B_performance\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob322172933802595801.jar tmpDir=null\n",
      "17/09/17 19:54:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 19:54:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 19:54:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/17 19:54:43 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/17 19:54:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0134\n",
      "17/09/17 19:54:44 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0134\n",
      "17/09/17 19:54:44 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0134/\n",
      "17/09/17 19:54:44 INFO mapreduce.Job: Running job: job_1504992575111_0134\n",
      "17/09/17 19:54:52 INFO mapreduce.Job: Job job_1504992575111_0134 running in uber mode : false\n",
      "17/09/17 19:54:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/17 19:55:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/17 19:55:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/17 19:55:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/17 19:55:14 INFO mapreduce.Job: Job job_1504992575111_0134 completed successfully\n",
      "17/09/17 19:55:14 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11142672\n",
      "\t\tFILE: Number of bytes written=22866678\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910642\n",
      "\t\tHDFS: Number of bytes written=2106\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18129\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14800\n",
      "\t\tTotal time spent by all map tasks (ms)=18129\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14800\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18129\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14800\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18564096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15155200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=966247\n",
      "\t\tMap output bytes=9210166\n",
      "\t\tMap output materialized bytes=11142684\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=167\n",
      "\t\tReduce shuffle bytes=11142684\n",
      "\t\tReduce input records=966247\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=1932494\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=175\n",
      "\t\tCPU time spent (ms)=17610\n",
      "\t\tPhysical memory (bytes) snapshot=1084915712\n",
      "\t\tVirtual memory (bytes) snapshot=5490315264\n",
      "\t\tTotal committed heap usage (bytes)=1028128768\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=966247\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2106\n",
      "17/09/17 19:55:14 INFO streaming.StreamJob: Output directory: hw32B_performance\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32BHADOOP\n",
    "\n",
    "# Clean and set up HDFS environment\n",
    "!hdfs dfs -rm consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal consumer_complaints.csv \n",
    "!hdfs dfs -rm -r hw32B_performance\n",
    "\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.B Performance\" \\\n",
    "  -files ./hw32B_mapper.py,./hw32B_reducer.py \\\n",
    "  -mapper hw32B_mapper.py \\\n",
    "  -reducer hw32B_reducer.py \\\n",
    "  -input consumer_complaints.csv \\\n",
    "  -output hw32B_performance  \\\n",
    "  -numReduceTasks 2\n",
    "\n",
    "# END STUDENT CODE HW32BHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t3503\r\n",
      "account\t20681\r\n",
      "acct\t163\r\n",
      "an\t2505\r\n",
      "and\t16448\r\n",
      "applied\t139\r\n",
      "apr\t3431\r\n",
      "arbitration\t168\r\n",
      "available\t274\r\n",
      "bankruptcy\t222\r\n",
      "being\t5663\r\n",
      "billing\t8158\r\n",
      "by\t5663\r\n",
      "cash\t240\r\n",
      "caused\t5663\r\n",
      "changes\t350\r\n",
      "charges\t131\r\n",
      "checks\t75\r\n",
      "closingcancelling\t2795\r\n",
      "convenience\t75\r\n",
      "credit\t50894\r\n",
      "debt\t19309\r\n",
      "delay\t243\r\n",
      "delinquent\t1061\r\n",
      "deposits\t10555\r\n",
      "determination\t1490\r\n",
      "didnt\t925\r\n",
      "disclosure\t5214\r\n",
      "disputes\t6938\r\n",
      "expect\t807\r\n",
      "false\t2508\r\n",
      "fees\t807\r\n",
      "for\t929\r\n",
      "i\t925\r\n",
      "incorrect\t29069\r\n",
      "incorrectmissing\t64\r\n",
      "issuance\t640\r\n",
      "issue\t1098\r\n",
      "loandid\t139\r\n",
      "makingreceiving\t3226\r\n",
      "not\t12353\r\n",
      "of\t10885\r\n",
      "on\t29069\r\n",
      "or\t22533\r\n",
      "overlimit\t127\r\n",
      "owed\t11848\r\n",
      "payments\t3226\r\n",
      "payoff\t1155\r\n",
      "process\t5505\r\n",
      "processing\t243\r\n",
      "promised\t274\r\n",
      "protection\t4139\r\n",
      "receive\t139\r\n",
      "received\t216\r\n",
      "relations\t1367\r\n",
      "repay\t1647\r\n",
      "repaying\t3844\r\n",
      "reportcredit\t4357\r\n",
      "representation\t2508\r\n",
      "sale\t139\r\n",
      "service\t1518\r\n",
      "servicer\t1944\r\n",
      "settlement\t4350\r\n",
      "statement\t1220\r\n",
      "tactics\t6920\r\n",
      "takingthreatening\t2505\r\n",
      "terms\t350\r\n",
      "the\t6248\r\n",
      "theft\t3276\r\n",
      "to\t8401\r\n",
      "transfer\t597\r\n",
      "unable\t8178\r\n",
      "unsolicited\t640\r\n",
      "use\t1477\r\n",
      "verification\t5214\r\n",
      "was\t274\r\n",
      "workout\t350\r\n",
      "wrong\t169\r\n",
      "you\t3821\r\n",
      "your\t3844\r\n",
      "action\t2505\r\n",
      "advance\t240\r\n",
      "advertising\t1193\r\n",
      "amount\t98\r\n",
      "amt\t71\r\n",
      "application\t8868\r\n",
      "apply\t118\r\n",
      "are\t3821\r\n",
      "atm\t2422\r\n",
      "attempts\t11848\r\n",
      "balance\t597\r\n",
      "bank\t202\r\n",
      "cant\t1999\r\n",
      "card\t4405\r\n",
      "charged\t976\r\n",
      "club\t12545\r\n",
      "collect\t11848\r\n",
      "collection\t1907\r\n",
      "communication\t6920\r\n",
      "companys\t4858\r\n",
      "contact\t3053\r\n",
      "contd\t11848\r\n",
      "costs\t4350\r\n",
      "credited\t92\r\n",
      "customer\t2734\r\n",
      "day\t71\r\n",
      "dealing\t1944\r\n",
      "debit\t2422\r\n",
      "decision\t2774\r\n",
      "disclosures\t64\r\n",
      "dispute\t904\r\n",
      "embezzlement\t3276\r\n",
      "fee\t3198\r\n",
      "forbearance\t350\r\n",
      "fraud\t3842\r\n",
      "funds\t5663\r\n",
      "get\t4357\r\n",
      "getting\t291\r\n",
      "health\t12545\r\n",
      "identity\t4729\r\n",
      "illegal\t2505\r\n",
      "improper\t4309\r\n",
      "increasedecrease\t1149\r\n",
      "info\t2896\r\n",
      "information\t29069\r\n",
      "interest\t4238\r\n",
      "investigation\t4858\r\n",
      "issues\t538\r\n",
      "late\t1797\r\n",
      "lease\t6337\r\n",
      "lender\t2165\r\n",
      "line\t1732\r\n",
      "loan\t119491\r\n",
      "low\t5663\r\n",
      "managing\t5006\r\n",
      "marketing\t1193\r\n",
      "modification\t70487\r\n",
      "money\t413\r\n",
      "monitoring\t1453\r\n",
      "my\t10731\r\n",
      "opening\t16205\r\n",
      "other\t7886\r\n",
      "out\t1242\r\n",
      "pay\t3821\r\n",
      "payment\t92\r\n",
      "plans\t350\r\n",
      "practices\t1003\r\n",
      "privacy\t240\r\n",
      "problems\t9484\r\n",
      "rate\t3431\r\n",
      "report\t30546\r\n",
      "reporting\t6559\r\n",
      "rewards\t1002\r\n",
      "scam\t566\r\n",
      "score\t4357\r\n",
      "servicing\t36767\r\n",
      "sharing\t2832\r\n",
      "shopping\t672\r\n",
      "statements\t2508\r\n",
      "stop\t131\r\n",
      "taking\t1242\r\n",
      "transaction\t1485\r\n",
      "underwriting\t2774\r\n",
      "using\t2422\r\n",
      "when\t4095\r\n",
      "with\t1944\r\n",
      "withdrawals\t10555\r\n"
     ]
    }
   ],
   "source": [
    "# 3.2.B OUTPUT/ANSWER\n",
    "#!hdfs dfs -ls hw32B_performance\n",
    "!hdfs dfs -cat hw32B_performance/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot of the Mapper and Reducer counter values in the JobTracker UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hw32B_screenshot.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"hw32B_screenshot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.B EXPLANATION\n",
    "\n",
    "Two mappers and two reducers were used to word count the Issue column of the consumer_complains.csv dataset. The output resulted in two files, one from each reducer, that contained sorted key (ie. word) value (ie. number of occurrances) pairs. However, the two output files were not combined thus will have duplicative keys that should be summed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.C SOLUTION\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. **What is the value of your user defined Mapper Counter, Combiner Counter, and Reducer Counter after completing your word count job?**\n",
    "\n",
    "Using a single reducer:\n",
    "  * What are the top 50 most frequent terms in your word count analysis?\n",
    "  * Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.\n",
    "  * Present bottom 10 tokens (least frequent items).\n",
    "\n",
    "NOTE: You can use: WORD_RE = re.compile(r\"[\\w']+\") to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper, combiner, and reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CMAPPER\n",
    "\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32C_mapper running\\n\")\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split(',')\n",
    "    issueStr = regex.sub('', values[3].lower())\n",
    "    #print issueStr\n",
    "    for word in WORD_RE.findall(issueStr):\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        print '%s\\t%s' % (word, 1) \n",
    "\n",
    "# END STUDENT CODE HW32CMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32C_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "disclosure\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "verification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "of\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "disclosure\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "verification\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "of\t1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Unit test the mapper\n",
    "!chmod a+x hw32C_mapper.py\n",
    "!head -2 consumer_complaints.csv | ./hw32C_mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_combiner.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CCOMBINER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW32CCOMBINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Combiner Counters,Calls,1\r\n",
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32C_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t2\r\n",
      "disclosure\t2\r\n",
      "of\t2\r\n",
      "verification\t2\r\n"
     ]
    }
   ],
   "source": [
    "# System test the combiner\n",
    "!chmod a+x hw32C_combiner.py\n",
    "!head -2 consumer_complaints.csv | ./hw32C_mapper.py  | sort -k1,1 | ./hw32C_combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW32CREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32C_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "debt\t2\r\n",
      "disclosure\t2\r\n",
      "of\t2\r\n",
      "verification\t2\r\n"
     ]
    }
   ],
   "source": [
    "# System test the reducer\n",
    "!chmod a+x hw32C_reducer.py\n",
    "!head -2 consumer_complaints.csv | ./hw32C_mapper.py  | sort -k1,1 | ./hw32C_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop-streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw32C_mapper.py\n",
    "!chmod a+x hw32C_combiner.py\n",
    "!chmod a+x hw32C_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted consumer_complaints.csv\n",
      "Deleted hw32C_performance\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob4735244368318290598.jar tmpDir=null\n",
      "17/09/17 20:01:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 20:01:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/17 20:01:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/17 20:01:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/17 20:01:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0135\n",
      "17/09/17 20:01:02 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0135\n",
      "17/09/17 20:01:02 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0135/\n",
      "17/09/17 20:01:02 INFO mapreduce.Job: Running job: job_1504992575111_0135\n",
      "17/09/17 20:01:11 INFO mapreduce.Job: Job job_1504992575111_0135 running in uber mode : false\n",
      "17/09/17 20:01:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/17 20:01:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/17 20:01:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/17 20:01:36 INFO mapreduce.Job: Job job_1504992575111_0135 completed successfully\n",
      "17/09/17 20:01:36 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4459\n",
      "\t\tFILE: Number of bytes written=447043\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910642\n",
      "\t\tHDFS: Number of bytes written=2106\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25976\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5819\n",
      "\t\tTotal time spent by all map tasks (ms)=25976\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5819\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=25976\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5819\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=26599424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5958656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=966247\n",
      "\t\tMap output bytes=9210166\n",
      "\t\tMap output materialized bytes=4465\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=966247\n",
      "\t\tCombine output records=309\n",
      "\t\tReduce input groups=167\n",
      "\t\tReduce shuffle bytes=4465\n",
      "\t\tReduce input records=309\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=618\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=165\n",
      "\t\tCPU time spent (ms)=14930\n",
      "\t\tPhysical memory (bytes) snapshot=901877760\n",
      "\t\tVirtual memory (bytes) snapshot=4133003264\n",
      "\t\tTotal committed heap usage (bytes)=808976384\n",
      "\tCombiner Counters\n",
      "\t\tCalls=2\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=966247\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2106\n",
      "17/09/17 20:01:36 INFO streaming.StreamJob: Output directory: hw32C_performance\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CHADOOP\n",
    "\n",
    "# Clean and set up HDFS environment\n",
    "!hdfs dfs -rm consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal consumer_complaints.csv \n",
    "!hdfs dfs -rm -r hw32C_performance\n",
    "\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.C Performance\" \\\n",
    "  -files ./hw32C_mapper.py,./hw32C_combiner.py,./hw32C_reducer.py \\\n",
    "  -mapper hw32C_mapper.py \\\n",
    "  -combiner hw32C_combiner.py \\\n",
    "  -reducer hw32C_reducer.py \\\n",
    "  -input consumer_complaints.csv \\\n",
    "  -output hw32C_performance  \n",
    "\n",
    "# END STUDENT CODE HW32CHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-09-17 20:01 hw32C_performance/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       2106 2017-09-17 20:01 hw32C_performance/part-00000\n",
      "a\t3503\n",
      "account\t20681\n",
      "acct\t163\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "an\t2505\n",
      "and\t16448\n",
      "application\t8868\n",
      "applied\t139\n",
      "apply\t118\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "available\t274\n",
      "balance\t597\n",
      "bank\t202\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "cant\t1999\n",
      "card\t4405\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charged\t976\n",
      "charges\t131\n",
      "checks\t75\n",
      "closingcancelling\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "companys\t4858\n",
      "contact\t3053\n",
      "contd\t11848\n",
      "convenience\t75\n",
      "costs\t4350\n",
      "credit\t50894\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "debt\t19309\n",
      "decision\t2774\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "didnt\t925\n",
      "disclosure\t5214\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "disputes\t6938\n",
      "embezzlement\t3276\n",
      "expect\t807\n",
      "false\t2508\n",
      "fee\t3198\n",
      "fees\t807\n",
      "for\t929\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "i\t925\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect\t29069\n",
      "incorrectmissing\t64\n",
      "increasedecrease\t1149\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t119491\n",
      "loandid\t139\n",
      "low\t5663\n",
      "makingreceiving\t3226\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "opening\t16205\n",
      "or\t22533\n",
      "other\t7886\n",
      "out\t1242\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "pay\t3821\n",
      "payment\t92\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "rate\t3431\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "report\t30546\n",
      "reportcredit\t4357\n",
      "reporting\t6559\n",
      "representation\t2508\n",
      "rewards\t1002\n",
      "sale\t139\n",
      "scam\t566\n",
      "score\t4357\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "servicing\t36767\n",
      "settlement\t4350\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statement\t1220\n",
      "statements\t2508\n",
      "stop\t131\n",
      "tactics\t6920\n",
      "taking\t1242\n",
      "takingthreatening\t2505\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "to\t8401\n",
      "transaction\t1485\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "underwriting\t2774\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "using\t2422\n",
      "verification\t5214\n",
      "was\t274\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n"
     ]
    }
   ],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw32C_performance\n",
    "!hdfs dfs -cat hw32C_performance/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot of the Combiner, Mapper, and Reducer counter values in the JobTracker UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hw32C_screenshot.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"hw32C_screenshot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count frequency in Issues field and top/bottom counts using one reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_identityMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_identityMapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CIDMAPPER\n",
    "\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32C_identityMapper running\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split()\n",
    "    print '%s\\t%s\\t%s' % (values[0], values[1], values[2])\n",
    "    \n",
    "# END STUDENT CODE HW32CIDMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_frequenciesMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_frequenciesMapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "\n",
    "import sys, re, string\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32C_frequenciesMapper running\\n\")\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "words = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split(',')\n",
    "    issueStr = regex.sub('', values[3].lower())\n",
    "    for word in WORD_RE.findall(issueStr):\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        words[word] += 1\n",
    "\n",
    "print '%s\\t%s\\t%s' % ('--ALL', sum(words.values()), 0)\n",
    "for k in words.keys():\n",
    "    print '%s\\t%s\\t%s' % (k, words[k], 0) \n",
    "\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32C_frequenciesMapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "--ALL\t42\t0\r\n",
      "and\t1\t0\r\n",
      "attempts\t1\t0\r\n",
      "club\t1\t0\r\n",
      "collect\t1\t0\r\n",
      "contd\t1\t0\r\n",
      "credit\t1\t0\r\n",
      "debt\t4\t0\r\n",
      "deposits\t1\t0\r\n",
      "disclosure\t3\t0\r\n",
      "false\t1\t0\r\n",
      "health\t1\t0\r\n",
      "incorrect\t1\t0\r\n",
      "information\t1\t0\r\n",
      "lease\t2\t0\r\n",
      "loan\t2\t0\r\n",
      "managing\t2\t0\r\n",
      "not\t1\t0\r\n",
      "of\t3\t0\r\n",
      "on\t1\t0\r\n",
      "or\t3\t0\r\n",
      "owed\t1\t0\r\n",
      "report\t1\t0\r\n",
      "representation\t1\t0\r\n",
      "statements\t1\t0\r\n",
      "the\t2\t0\r\n",
      "verification\t3\t0\r\n",
      "withdrawals\t1\t0\r\n"
     ]
    }
   ],
   "source": [
    "# Unit test the mapper\n",
    "!chmod a+x hw32C_frequenciesMapper.py\n",
    "!head -10 consumer_complaints.csv | ./hw32C_frequenciesMapper.py | sort -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_identityReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_identityReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CIDREDUCER\n",
    "\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32C_identityReducer running\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split()\n",
    "    print '%s\\t%s\\t%s' % (values[0], values[1], values[2])\n",
    "    \n",
    "# END STUDENT CODE HW32CIDREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw32C_frequenciesReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw32C_frequenciesReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "tot_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value, freq = line.split()\n",
    "    \n",
    "    if key == '--ALL':\n",
    "        tot_count += int(value)\n",
    "        continue\n",
    "        \n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s\\t%.3f' % (cur_key, cur_count, float(cur_count)/tot_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s\\t%.3f' % (cur_key, cur_count, float(cur_count)/tot_count)\n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:hw32C_frequenciesMapper running\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Token Counters,Calls,1\n",
      "reporter:counter:Reducer Counters,Calls,1\n",
      "and\t2\t0.024\n",
      "attempts\t2\t0.024\n",
      "club\t2\t0.024\n",
      "collect\t2\t0.024\n",
      "contd\t2\t0.024\n",
      "credit\t2\t0.024\n",
      "debt\t8\t0.095\n",
      "deposits\t2\t0.024\n",
      "disclosure\t6\t0.071\n",
      "false\t2\t0.024\n",
      "health\t2\t0.024\n",
      "incorrect\t2\t0.024\n",
      "information\t2\t0.024\n",
      "lease\t4\t0.048\n",
      "loan\t4\t0.048\n",
      "managing\t4\t0.048\n",
      "not\t2\t0.024\n",
      "of\t6\t0.071\n",
      "on\t2\t0.024\n",
      "or\t6\t0.071\n",
      "owed\t2\t0.024\n",
      "report\t2\t0.024\n",
      "representation\t2\t0.024\n",
      "statements\t2\t0.024\n",
      "the\t4\t0.048\n",
      "verification\t6\t0.071\n",
      "withdrawals\t2\t0.024\n"
     ]
    }
   ],
   "source": [
    "# System test the reducer\n",
    "!chmod a+x hw32C_frequenciesReducer.py\n",
    "!head -10 consumer_complaints.csv | ./hw32C_frequenciesMapper.py > sort_trials\n",
    "\n",
    "!cat sort_trials sort_trials | sort -k1,1 | ./hw32C_frequenciesReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\n",
      "and\t2\t0.024\n",
      "attempts\t2\t0.024\n",
      "club\t2\t0.024\n",
      "collect\t2\t0.024\n",
      "contd\t2\t0.024\n",
      "reporter:counter:Reducer Counters,Calls,1\n",
      "reporter:status:hw32C_identityReducer running\n",
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:hw32C_identityMapper running\n",
      "debt\t8\t0.095\n",
      "disclosure\t6\t0.071\n",
      "of\t6\t0.071\n",
      "or\t6\t0.071\n",
      "verification\t6\t0.071\n",
      "lease\t4\t0.048\n",
      "loan\t4\t0.048\n",
      "managing\t4\t0.048\n",
      "the\t4\t0.048\n",
      "and\t2\t0.024\n",
      "attempts\t2\t0.024\n",
      "club\t2\t0.024\n",
      "collect\t2\t0.024\n",
      "contd\t2\t0.024\n",
      "credit\t2\t0.024\n",
      "deposits\t2\t0.024\n",
      "false\t2\t0.024\n",
      "health\t2\t0.024\n",
      "incorrect\t2\t0.024\n",
      "information\t2\t0.024\n",
      "not\t2\t0.024\n",
      "on\t2\t0.024\n",
      "owed\t2\t0.024\n",
      "report\t2\t0.024\n",
      "representation\t2\t0.024\n",
      "statements\t2\t0.024\n",
      "withdrawals\t2\t0.024\n"
     ]
    }
   ],
   "source": [
    "# System test the identity Mapper and Reducer\n",
    "!chmod a+x hw32C_identityMapper.py\n",
    "!chmod a+x hw32C_identityReducer.py\n",
    "!cat sort_trials sort_trials | sort -k1,1 | ./hw32C_frequenciesReducer.py > sort_trials2\n",
    "!head -5 sort_trials2\n",
    "\n",
    "!cat sort_trials2 | ./hw32C_identityMapper.py | sort -k2,2nr -k1,1 | ./hw32C_identityReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw32C_frequenciesMapper.py\n",
    "!chmod a+x hw32C_frequenciesReducer.py\n",
    "!chmod a+x hw32C_identityMapper.py\n",
    "!chmod a+x hw32C_identityReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted consumer_complaints.csv\n",
      "Deleted hw32C_frequencies\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob234980707722207400.jar tmpDir=null\n",
      "17/09/19 11:59:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 11:59:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 11:59:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/19 11:59:49 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/19 11:59:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0174\n",
      "17/09/19 11:59:49 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0174\n",
      "17/09/19 11:59:49 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0174/\n",
      "17/09/19 11:59:49 INFO mapreduce.Job: Running job: job_1504992575111_0174\n",
      "17/09/19 11:59:57 INFO mapreduce.Job: Job job_1504992575111_0174 running in uber mode : false\n",
      "17/09/19 11:59:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/19 12:00:06 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/19 12:00:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/19 12:00:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/19 12:00:13 INFO mapreduce.Job: Job job_1504992575111_0174 completed successfully\n",
      "17/09/19 12:00:13 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5111\n",
      "\t\tFILE: Number of bytes written=446580\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910642\n",
      "\t\tHDFS: Number of bytes written=3108\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14165\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3911\n",
      "\t\tTotal time spent by all map tasks (ms)=14165\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3911\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14165\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3911\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14504960\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4004864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=311\n",
      "\t\tMap output bytes=4483\n",
      "\t\tMap output materialized bytes=5117\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=168\n",
      "\t\tReduce shuffle bytes=5117\n",
      "\t\tReduce input records=311\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=622\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=108\n",
      "\t\tCPU time spent (ms)=8600\n",
      "\t\tPhysical memory (bytes) snapshot=918999040\n",
      "\t\tVirtual memory (bytes) snapshot=4126093312\n",
      "\t\tTotal committed heap usage (bytes)=824180736\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=966247\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3108\n",
      "17/09/19 12:00:13 INFO streaming.StreamJob: Output directory: hw32C_frequencies\n",
      "Deleted hw32C_frequencies_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob2362489089855620867.jar tmpDir=null\n",
      "17/09/19 12:00:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 12:00:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 12:00:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/19 12:00:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/19 12:00:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0175\n",
      "17/09/19 12:00:21 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0175\n",
      "17/09/19 12:00:21 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0175/\n",
      "17/09/19 12:00:21 INFO mapreduce.Job: Running job: job_1504992575111_0175\n",
      "17/09/19 12:00:28 INFO mapreduce.Job: Job job_1504992575111_0175 running in uber mode : false\n",
      "17/09/19 12:00:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/19 12:00:34 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/19 12:00:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/19 12:00:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/19 12:00:41 INFO mapreduce.Job: Job job_1504992575111_0175 completed successfully\n",
      "17/09/19 12:00:41 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3615\n",
      "\t\tFILE: Number of bytes written=445694\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4906\n",
      "\t\tHDFS: Number of bytes written=3108\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8351\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3932\n",
      "\t\tTotal time spent by all map tasks (ms)=8351\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3932\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8351\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3932\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8551424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4026368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=167\n",
      "\t\tMap output records=167\n",
      "\t\tMap output bytes=3275\n",
      "\t\tMap output materialized bytes=3621\n",
      "\t\tInput split bytes=244\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=167\n",
      "\t\tReduce shuffle bytes=3621\n",
      "\t\tReduce input records=167\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=334\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=74\n",
      "\t\tCPU time spent (ms)=2300\n",
      "\t\tPhysical memory (bytes) snapshot=780611584\n",
      "\t\tVirtual memory (bytes) snapshot=4150267904\n",
      "\t\tTotal committed heap usage (bytes)=556793856\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4662\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3108\n",
      "17/09/19 12:00:41 INFO streaming.StreamJob: Output directory: hw32C_frequencies_sorted\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "\n",
    "# Clean and set up HDFS environment\n",
    "!hdfs dfs -rm consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal consumer_complaints.csv \n",
    "!hdfs dfs -rm -r hw32C_frequencies\n",
    "\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "#  -D mapreduce.job.maps=1 \\\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.C Frequencies Aggregates\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -files ./hw32C_frequenciesMapper.py,./hw32C_frequenciesReducer.py \\\n",
    "  -mapper hw32C_frequenciesMapper.py \\\n",
    "  -reducer hw32C_frequenciesReducer.py \\\n",
    "  -input consumer_complaints.csv \\\n",
    "  -output hw32C_frequencies \n",
    "\n",
    "!hdfs dfs -rm -r hw32C_frequencies_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.C Frequencies Sorter\" \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -files ./hw32C_identityMapper.py,./hw32C_identityReducer.py \\\n",
    "  -mapper hw32C_identityMapper.py \\\n",
    "  -reducer hw32C_identityReducer.py \\\n",
    "  -input hw32C_frequencies \\\n",
    "  -output hw32C_frequencies_sorted \n",
    "\n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-09-19 12:00 hw32C_frequencies_sorted/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       3108 2017-09-19 12:00 hw32C_frequencies_sorted/part-00000\n",
      "\n",
      "TOP 50 WORDS BY FREQUENCY\n",
      "loan\t119491\t0.124\n",
      "modification\t70487\t0.073\n",
      "credit\t50894\t0.053\n",
      "servicing\t36767\t0.038\n",
      "report\t30546\t0.032\n",
      "incorrect\t29069\t0.030\n",
      "information\t29069\t0.030\n",
      "on\t29069\t0.030\n",
      "or\t22533\t0.023\n",
      "account\t20681\t0.021\n",
      "debt\t19309\t0.020\n",
      "and\t16448\t0.017\n",
      "opening\t16205\t0.017\n",
      "club\t12545\t0.013\n",
      "health\t12545\t0.013\n",
      "not\t12353\t0.013\n",
      "attempts\t11848\t0.012\n",
      "collect\t11848\t0.012\n",
      "contd\t11848\t0.012\n",
      "owed\t11848\t0.012\n",
      "of\t10885\t0.011\n",
      "my\t10731\t0.011\n",
      "deposits\t10555\t0.011\n",
      "withdrawals\t10555\t0.011\n",
      "problems\t9484\t0.010\n",
      "application\t8868\t0.009\n",
      "to\t8401\t0.009\n",
      "unable\t8178\t0.008\n",
      "billing\t8158\t0.008\n",
      "other\t7886\t0.008\n",
      "disputes\t6938\t0.007\n",
      "communication\t6920\t0.007\n",
      "tactics\t6920\t0.007\n",
      "reporting\t6559\t0.007\n",
      "lease\t6337\t0.007\n",
      "the\t6248\t0.006\n",
      "being\t5663\t0.006\n",
      "by\t5663\t0.006\n",
      "caused\t5663\t0.006\n",
      "funds\t5663\t0.006\n",
      "low\t5663\t0.006\n",
      "process\t5505\t0.006\n",
      "disclosure\t5214\t0.005\n",
      "verification\t5214\t0.005\n",
      "managing\t5006\t0.005\n",
      "companys\t4858\t0.005\n",
      "investigation\t4858\t0.005\n",
      "identity\t4729\t0.005\n",
      "card\t4405\t0.005\n",
      "get\t4357\t0.005\n",
      "\n",
      "BOTTOM 10 WORDS BY FREQUENCY\n",
      "apply\t118\t0.000\n",
      "amount\t98\t0.000\n",
      "credited\t92\t0.000\n",
      "payment\t92\t0.000\n",
      "checks\t75\t0.000\n",
      "convenience\t75\t0.000\n",
      "amt\t71\t0.000\n",
      "day\t71\t0.000\n",
      "disclosures\t64\t0.000\n",
      "incorrectmissing\t64\t0.000\n"
     ]
    }
   ],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw32C_frequencies_sorted\n",
    "#!hdfs dfs -cat hw32C_frequencies_sorted/part-00000\n",
    "\n",
    "!printf \"\\nTOP 50 WORDS BY FREQUENCY\\n\"\n",
    "!hdfs dfs -cat hw32C_frequencies_sorted/part-0000* | head -50\n",
    "\n",
    "!printf \"\\nBOTTOM 10 WORDS BY FREQUENCY\\n\"\n",
    "!hdfs dfs -cat hw32C_frequencies_sorted/part-0000* | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.C EXPLANATION\n",
    "\n",
    "There must be a more efficient and correct way to do this while maximizing the strengths of Hadoop. I tried to focus Hadoop on the heavy lifting of sorting and kept the aggregating within the mappers and reducer. However, utilizing dicts may not be the best way for this - I've got to spend some time considering this. I will need to revisit this as I'm not getting it done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2.1\"></a>\n",
    "### 3.2.1  Two Reducers\n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw321_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw321_mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw32C_mapper running\\n\")\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split(',')\n",
    "    issueStr = regex.sub('', values[3].lower())\n",
    "    #print issueStr\n",
    "    for word in WORD_RE.findall(issueStr):\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        print '%s\\t%s' % (word, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw321_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw321_combiner.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw321_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw321_reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Combiner Counters,Calls,1\r\n",
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw32C_mapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "and\t1\r\n",
      "attempts\t1\r\n",
      "club\t1\r\n",
      "collect\t1\r\n",
      "contd\t1\r\n",
      "credit\t1\r\n",
      "debt\t4\r\n",
      "deposits\t1\r\n",
      "disclosure\t3\r\n",
      "false\t1\r\n",
      "health\t1\r\n",
      "incorrect\t1\r\n",
      "information\t1\r\n",
      "lease\t2\r\n",
      "loan\t2\r\n",
      "managing\t2\r\n",
      "not\t1\r\n",
      "of\t3\r\n",
      "on\t1\r\n",
      "or\t3\r\n",
      "owed\t1\r\n",
      "report\t1\r\n",
      "representation\t1\r\n",
      "statements\t1\r\n",
      "the\t2\r\n",
      "verification\t3\r\n",
      "withdrawals\t1\r\n"
     ]
    }
   ],
   "source": [
    "# System test the all\n",
    "!chmod a+x hw321_mapper.py\n",
    "!chmod a+x hw321_combiner.py\n",
    "!chmod a+x hw321_reducer.py\n",
    "!head -10 consumer_complaints.csv | ./hw321_mapper.py  | sort -k1,1 | ./hw321_combiner.py | sort -k1,1 | ./hw321_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hw321_freqMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw321_freqMapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw321_freqMapper running\\n\")\n",
    "\n",
    "tot_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split()\n",
    "    print '%s\\t%s' % (values[0], values[1])\n",
    "    tot_count += int(values[1])\n",
    "    \n",
    "print '%s\\t%s' % ('--ALL', tot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw321_freqReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw321_freqReducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw321_freqReducer running\\n\")\n",
    "\n",
    "tot_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    values = line.split()\n",
    "    if values[0] == '--ALL':\n",
    "        tot_count += int(values[1])\n",
    "        sys.stderr.write(\"reporter:status:hw321_freqReducer tot_count=%s\\n\" % tot_count)\n",
    "        continue\n",
    "    print '%s\\t%s\\t%s' % (values[0], values[1], float(values[1])/tot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw321_mapper.py\n",
    "!chmod a+x hw321_combiner.py\n",
    "!chmod a+x hw321_reducer.py\n",
    "!chmod a+x hw321_freqMapper.py\n",
    "!chmod a+x hw321_freqReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted consumer_complaints.csv\n",
      "Deleted hw32C_frequencies2\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob7611541733268350678.jar tmpDir=null\n",
      "17/09/19 21:08:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 21:08:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 21:08:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/19 21:08:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/19 21:08:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0189\n",
      "17/09/19 21:08:36 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0189\n",
      "17/09/19 21:08:36 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0189/\n",
      "17/09/19 21:08:36 INFO mapreduce.Job: Running job: job_1504992575111_0189\n",
      "17/09/19 21:08:43 INFO mapreduce.Job: Job job_1504992575111_0189 running in uber mode : false\n",
      "17/09/19 21:08:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/19 21:08:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/19 21:09:01 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/19 21:09:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/19 21:09:03 INFO mapreduce.Job: Job job_1504992575111_0189 completed successfully\n",
      "17/09/19 21:09:03 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4465\n",
      "\t\tFILE: Number of bytes written=593168\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910642\n",
      "\t\tHDFS: Number of bytes written=2106\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17554\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8675\n",
      "\t\tTotal time spent by all map tasks (ms)=17554\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8675\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=17554\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8675\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17975296\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8883200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=966247\n",
      "\t\tMap output bytes=9210166\n",
      "\t\tMap output materialized bytes=4477\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=966247\n",
      "\t\tCombine output records=309\n",
      "\t\tReduce input groups=167\n",
      "\t\tReduce shuffle bytes=4477\n",
      "\t\tReduce input records=309\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=618\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=128\n",
      "\t\tCPU time spent (ms)=14520\n",
      "\t\tPhysical memory (bytes) snapshot=1118138368\n",
      "\t\tVirtual memory (bytes) snapshot=5467418624\n",
      "\t\tTotal committed heap usage (bytes)=910688256\n",
      "\tCombiner Counters\n",
      "\t\tCalls=4\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=966247\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2106\n",
      "17/09/19 21:09:03 INFO streaming.StreamJob: Output directory: hw32C_frequencies2\n",
      "Deleted hw32C_frequencies2_sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob613589436030306798.jar tmpDir=null\n",
      "17/09/19 21:09:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 21:09:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 21:09:10 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "17/09/19 21:09:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/19 21:09:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0190\n",
      "17/09/19 21:09:11 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0190\n",
      "17/09/19 21:09:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0190/\n",
      "17/09/19 21:09:11 INFO mapreduce.Job: Running job: job_1504992575111_0190\n",
      "17/09/19 21:09:19 INFO mapreduce.Job: Job job_1504992575111_0190 running in uber mode : false\n",
      "17/09/19 21:09:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/19 21:09:25 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/19 21:09:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/19 21:09:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/19 21:09:33 INFO mapreduce.Job: Job job_1504992575111_0190 completed successfully\n",
      "17/09/19 21:09:33 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2476\n",
      "\t\tFILE: Number of bytes written=441112\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2352\n",
      "\t\tHDFS: Number of bytes written=4953\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8356\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3938\n",
      "\t\tTotal time spent by all map tasks (ms)=8356\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3938\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8356\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3938\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8556544\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4032512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=167\n",
      "\t\tMap output records=169\n",
      "\t\tMap output bytes=2132\n",
      "\t\tMap output materialized bytes=2482\n",
      "\t\tInput split bytes=246\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=168\n",
      "\t\tReduce shuffle bytes=2482\n",
      "\t\tReduce input records=169\n",
      "\t\tReduce output records=167\n",
      "\t\tSpilled Records=338\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=59\n",
      "\t\tCPU time spent (ms)=2410\n",
      "\t\tPhysical memory (bytes) snapshot=780935168\n",
      "\t\tVirtual memory (bytes) snapshot=4114526208\n",
      "\t\tTotal committed heap usage (bytes)=625475584\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2106\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4953\n",
      "17/09/19 21:09:33 INFO streaming.StreamJob: Output directory: hw32C_frequencies2_sorted\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "    \n",
    "!hdfs dfs -rm consumer_complaints.csv \n",
    "!hdfs dfs -copyFromLocal consumer_complaints.csv \n",
    "!hdfs dfs -rm -r hw32C_frequencies2\n",
    "\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "#  -D mapreduce.job.maps=1 \\\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.1 Frequencies2 Aggregates\" \\\n",
    "  -D mapreduce.job.reduces=2 \\\n",
    "  -files ./hw321_mapper.py,./hw321_reducer.py,./hw321_combiner.py \\\n",
    "  -mapper hw321_mapper.py \\\n",
    "  -combiner hw321_combiner.py \\\n",
    "  -reducer hw321_reducer.py \\\n",
    "  -input consumer_complaints.csv \\\n",
    "  -output hw32C_frequencies2 \n",
    "\n",
    "!hdfs dfs -rm -r hw32C_frequencies2_sorted\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.2.1 Frequencies2 Calculator\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -files ./hw321_freqMapper.py,./hw321_freqReducer.py \\\n",
    "  -mapper hw321_freqMapper.py \\\n",
    "  -reducer hw321_freqReducer.py \\\n",
    "  -input hw32C_frequencies2 \\\n",
    "  -output hw32C_frequencies2_sorted \n",
    "    \n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-09-19 21:09 hw32C_frequencies2_sorted/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       4953 2017-09-19 21:09 hw32C_frequencies2_sorted/part-00000\n",
      "loan\t119491\t0.123665067007\n",
      "modification\t70487\t0.0729492562461\n",
      "credit\t50894\t0.0526718323576\n",
      "servicing\t36767\t0.0380513471193\n",
      "report\t30546\t0.0316130347623\n",
      "incorrect\t29069\t0.0300844401069\n",
      "information\t29069\t0.0300844401069\n",
      "on\t29069\t0.0300844401069\n",
      "or\t22533\t0.0233201241505\n",
      "account\t20681\t0.0214034299718\n",
      "debt\t19309\t0.0199835031829\n",
      "and\t16448\t0.0170225625539\n",
      "opening\t16205\t0.0167710740628\n",
      "club\t12545\t0.0129832227163\n",
      "health\t12545\t0.0129832227163\n",
      "not\t12353\t0.0127845157605\n",
      "attempts\t11848\t0.0122618750692\n",
      "collect\t11848\t0.0122618750692\n",
      "contd\t11848\t0.0122618750692\n",
      "owed\t11848\t0.0122618750692\n",
      "of\t10885\t0.0112652354936\n",
      "my\t10731\t0.0111058559561\n",
      "deposits\t10555\t0.0109237079132\n",
      "withdrawals\t10555\t0.0109237079132\n",
      "problems\t9484\t0.00981529567492\n",
      "application\t8868\t0.00917777752479\n",
      "to\t8401\t0.00869446425189\n",
      "unable\t8178\t0.00846367440209\n",
      "billing\t8158\t0.00844297576086\n",
      "other\t7886\t0.00816147424002\n",
      "disputes\t6938\t0.00718035864536\n",
      "communication\t6920\t0.00716172986824\n",
      "tactics\t6920\t0.00716172986824\n",
      "reporting\t6559\t0.0067881193939\n",
      "lease\t6337\t0.00655836447616\n",
      "the\t6248\t0.00646625552266\n",
      "being\t5663\t0.00586082026645\n",
      "by\t5663\t0.00586082026645\n",
      "caused\t5663\t0.00586082026645\n",
      "funds\t5663\t0.00586082026645\n",
      "low\t5663\t0.00586082026645\n",
      "process\t5505\t0.00569730100068\n",
      "disclosure\t5214\t0.00539613577067\n",
      "verification\t5214\t0.00539613577067\n",
      "managing\t5006\t0.0051808699018\n",
      "companys\t4858\t0.00502769995664\n",
      "investigation\t4858\t0.00502769995664\n",
      "identity\t4729\t0.00489419372065\n",
      "card\t4405\t0.0045588757326\n",
      "get\t4357\t0.00450919899363\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw32C_frequencies2_sorted\n",
    "!hdfs dfs -cat hw32C_frequencies2_sorted/part-00000 | sort -k2,2nr -k1,1 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 EXPLANATION\n",
    "\n",
    "There is likely a better way to approach this, but it is starting to make more sense - kind of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "**How many unique items are available from this supplier?**\n",
    "\n",
    "**Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-09-19 03:03:16--  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.6.1, 2620:100:601c:1:ffff:ffff:a27d:601\n",
      "Connecting to www.dropbox.com|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/geZs9Exy1n5VvTpHSu7VBasEio5cfrxYAhQP3I0M5rmvSyiptEQUHl6IyVEtCjJq/file [following]\n",
      "--2017-09-19 03:03:17--  https://dl.dropboxusercontent.com/content_link/geZs9Exy1n5VvTpHSu7VBasEio5cfrxYAhQP3I0M5rmvSyiptEQUHl6IyVEtCjJq/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.6.6, 2620:100:601c:6:ffff:ffff:a27d:606\n",
      "Connecting to dl.dropboxusercontent.com|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3458517 (3.3M) [text/plain]\n",
      "Saving to: `browsing_behavior.txt'\n",
      "\n",
      "100%[======================================>] 3,458,517   2.92M/s   in 1.1s    \n",
      "\n",
      "2017-09-19 03:03:18 (2.92 MB/s) - `browsing_behavior.txt' saved [3458517/3458517]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download text\n",
    "!wget 'https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0' -O browsing_behavior.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n"
     ]
    }
   ],
   "source": [
    "!head -2 browsing_behavior.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Piped linux commands for exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF UNIQUE ITEMS\n",
      "380824\n"
     ]
    }
   ],
   "source": [
    "!printf \"NUMBER OF UNIQUE ITEMS\\n\"\n",
    "!cat browsing_behavior.txt | tr ' ' \"\\n\" | sort | uniq -c | tail -n +2 | awk '{sum+=$1} END{ print sum}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF CUSTOMERS\n",
      "31101 browsing_behavior.txt\n"
     ]
    }
   ],
   "source": [
    "!printf \"NUMBER OF CUSTOMERS\\n\"\n",
    "!wc -l browsing_behavior.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARGEST BASKET\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "!printf \"LARGEST BASKET\\n\"\n",
    "!awk '{print NF}' browsing_behavior.txt | sort -k1,1nr > output.txt\n",
    "!head -1 output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 50 PRODUCTS\n",
      "   6667 DAI62779\n",
      "   3881 FRO40251\n",
      "   3875 ELE17451\n",
      "   3602 GRO73461\n",
      "   3044 SNA80324\n",
      "   2851 ELE32164\n",
      "   2736 DAI75645\n",
      "   2455 SNA45677\n",
      "   2330 FRO31317\n",
      "   2293 DAI85309\n",
      "   2292 ELE26917\n",
      "   2233 FRO80039\n",
      "   2115 GRO21487\n",
      "   2083 SNA99873\n",
      "   2004 GRO59710\n",
      "   1920 GRO71621\n",
      "   1918 FRO85978\n",
      "   1840 GRO30386\n",
      "   1816 ELE74009\n",
      "   1784 GRO56726\n",
      "   1773 DAI63921\n",
      "   1756 GRO46854\n",
      "   1713 ELE66600\n",
      "   1712 DAI83733\n",
      "   1702 FRO32293\n",
      "   1697 ELE66810\n",
      "   1646 SNA55762\n",
      "   1627 DAI22177\n",
      "   1531 FRO78087\n",
      "   1516 ELE99737\n",
      "   1489 ELE34057\n",
      "   1489 GRO94758\n",
      "   1436 FRO35904\n",
      "   1420 FRO53271\n",
      "   1407 SNA93860\n",
      "   1390 SNA90094\n",
      "   1352 GRO38814\n",
      "   1345 ELE56788\n",
      "   1321 GRO61133\n",
      "   1316 DAI88807\n",
      "   1316 ELE74482\n",
      "   1311 ELE59935\n",
      "   1295 SNA96271\n",
      "   1290 DAI43223\n",
      "   1289 ELE91337\n",
      "   1275 GRO15017\n",
      "   1261 DAI31081\n",
      "   1220 GRO81087\n",
      "   1219 DAI22896\n",
      "   1214 GRO85051\n"
     ]
    }
   ],
   "source": [
    "!printf \"TOP 50 PRODUCTS\\n\"\n",
    "!cat browsing_behavior.txt | tr ' ' \"\\n\" | sort | uniq -c | tail -n +2 | sort -k1,1nr -k2,2 | sed -n 1,50p #| wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 50 most frequently purchased items, frequency, and relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw33_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw33_mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys, operator\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw33_freqMapper running\\n\")\n",
    "\n",
    "products = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    items = line.split()\n",
    "    for i in items:\n",
    "        sys.stderr.write(\"reporter:counter:Token Counters,Calls,1\\n\")\n",
    "        products[i] += 1\n",
    "\n",
    "tot_count = sum(products.values())\n",
    "for item, freq in sorted(products.items(), key=operator.itemgetter(1), reverse = True)[0:50]:\n",
    "    print '%s\\t%s\\t%s' % (item, freq, tot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw33_freqMapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "ELE17451\t17\t201\r\n",
      "GRO99222\t8\t201\r\n",
      "FRO86643\t7\t201\r\n",
      "FRO18919\t7\t201\r\n",
      "SNA90258\t7\t201\r\n",
      "FRO78087\t7\t201\r\n",
      "DAI22896\t7\t201\r\n",
      "GRO73461\t6\t201\r\n",
      "SNA80192\t6\t201\r\n",
      "GRO56989\t4\t201\r\n",
      "ELE11375\t4\t201\r\n",
      "SNA11465\t4\t201\r\n",
      "ELE37798\t4\t201\r\n",
      "GRO39357\t4\t201\r\n",
      "DAI22177\t4\t201\r\n",
      "ELE28573\t4\t201\r\n",
      "ELE26917\t4\t201\r\n",
      "GRO36567\t3\t201\r\n",
      "SNA69641\t3\t201\r\n",
      "FRO47475\t3\t201\r\n",
      "GRO75578\t3\t201\r\n",
      "SNA99873\t3\t201\r\n",
      "SNA91554\t2\t201\r\n",
      "ELE11111\t2\t201\r\n",
      "FRO81176\t2\t201\r\n",
      "DAI93692\t2\t201\r\n",
      "GRO39369\t2\t201\r\n",
      "GRO48282\t2\t201\r\n",
      "ELE59935\t2\t201\r\n",
      "ELE23393\t2\t201\r\n",
      "DAI91535\t2\t201\r\n",
      "FRO82427\t2\t201\r\n",
      "SNA55952\t2\t201\r\n",
      "GRO12935\t2\t201\r\n",
      "SNA94781\t2\t201\r\n",
      "GRO94758\t2\t201\r\n",
      "ELE82555\t2\t201\r\n",
      "DAI87514\t2\t201\r\n",
      "SNA17715\t2\t201\r\n",
      "SNA85662\t2\t201\r\n",
      "SNA47306\t2\t201\r\n",
      "DAI92253\t2\t201\r\n",
      "ELE96863\t1\t201\r\n",
      "DAI50921\t1\t201\r\n",
      "DAI49199\t1\t201\r\n",
      "DAI44355\t1\t201\r\n",
      "FRO70974\t1\t201\r\n",
      "ELE89019\t1\t201\r\n",
      "FRO32293\t1\t201\r\n",
      "DAI38969\t1\t201\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x hw33_mapper.py\n",
    "!head -20 browsing_behavior.txt | ./hw33_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw33_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw33_reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "tot_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value, total = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "        tot_count += int(total)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s\\t%.3f' % (cur_key, cur_count, float(cur_count)/tot_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "        tot_count = int(total)\n",
    "\n",
    "print '%s\\t%s\\t%.3f' % (cur_key, cur_count, float(cur_count)/tot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:hw33_freqMapper running\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n",
      "reporter:counter:Token Counters,Calls,1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\n",
      "50 hw33_systestout.txt\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x hw33_reducer.py\n",
    "!head -20 browsing_behavior.txt | ./hw33_mapper.py > hw33_systest.txt\n",
    "!cat hw33_systest.txt hw33_systest.txt | sort -k1,1 | ./hw33_reducer.py > hw33_systestout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hw33_mapper.py\n",
    "!chmod a+x hw33_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `browsing_behavior.txt': No such file or directory\n",
      "rm: `hw33_top50': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob8442602806113431423.jar tmpDir=null\n",
      "17/09/19 23:29:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 23:29:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/19 23:29:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/19 23:29:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/19 23:29:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0191\n",
      "17/09/19 23:29:56 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0191\n",
      "17/09/19 23:29:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0191/\n",
      "17/09/19 23:29:56 INFO mapreduce.Job: Running job: job_1504992575111_0191\n",
      "17/09/19 23:30:03 INFO mapreduce.Job: Job job_1504992575111_0191 running in uber mode : false\n",
      "17/09/19 23:30:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/19 23:30:11 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/19 23:30:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/19 23:30:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/19 23:30:17 INFO mapreduce.Job: Job job_1504992575111_0191 completed successfully\n",
      "17/09/19 23:30:18 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2242\n",
      "\t\tFILE: Number of bytes written=440428\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462843\n",
      "\t\tHDFS: Number of bytes written=1237\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10949\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3859\n",
      "\t\tTotal time spent by all map tasks (ms)=10949\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3859\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10949\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3859\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11211776\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3951616\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=2036\n",
      "\t\tMap output materialized bytes=2248\n",
      "\t\tInput split bytes=230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=63\n",
      "\t\tReduce shuffle bytes=2248\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=63\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=87\n",
      "\t\tCPU time spent (ms)=5500\n",
      "\t\tPhysical memory (bytes) snapshot=900751360\n",
      "\t\tVirtual memory (bytes) snapshot=4121968640\n",
      "\t\tTotal committed heap usage (bytes)=695730176\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tToken Counters\n",
      "\t\tCalls=380824\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1237\n",
      "17/09/19 23:30:18 INFO streaming.StreamJob: Output directory: hw33_top50\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm browsing_behavior.txt \n",
    "!hdfs dfs -copyFromLocal browsing_behavior.txt \n",
    "!hdfs dfs -rm -r hw33_top50\n",
    "\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "#  -D mapreduce.job.maps=1 \\\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.3 Top 50 Products\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -files ./hw33_mapper.py,./hw33_reducer.py \\\n",
    "  -mapper hw33_mapper.py \\\n",
    "  -reducer hw33_reducer.py \\\n",
    "  -input browsing_behavior.txt \\\n",
    "  -output hw33_top50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-09-19 23:30 hw33_top50/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       1237 2017-09-19 23:30 hw33_top50/part-00000\n",
      "DAI62779\t6667\t0.018\n",
      "FRO40251\t3881\t0.010\n",
      "ELE17451\t3875\t0.010\n",
      "GRO73461\t3602\t0.009\n",
      "SNA80324\t3044\t0.008\n",
      "ELE32164\t2851\t0.007\n",
      "DAI75645\t2736\t0.007\n",
      "SNA45677\t2455\t0.006\n",
      "FRO31317\t2330\t0.006\n",
      "DAI85309\t2293\t0.006\n",
      "ELE26917\t2292\t0.006\n",
      "FRO80039\t2233\t0.006\n",
      "GRO21487\t2115\t0.006\n",
      "SNA99873\t2083\t0.005\n",
      "GRO59710\t2004\t0.005\n",
      "GRO71621\t1920\t0.005\n",
      "FRO85978\t1918\t0.005\n",
      "GRO30386\t1840\t0.005\n",
      "ELE74009\t1816\t0.005\n",
      "GRO56726\t1784\t0.005\n",
      "DAI63921\t1773\t0.005\n",
      "ELE66600\t1713\t0.004\n",
      "DAI83733\t1712\t0.004\n",
      "FRO32293\t1702\t0.004\n",
      "ELE66810\t1697\t0.004\n",
      "SNA55762\t1646\t0.004\n",
      "DAI22177\t1627\t0.004\n",
      "FRO78087\t1531\t0.004\n",
      "ELE34057\t1489\t0.004\n",
      "FRO35904\t1436\t0.004\n",
      "FRO53271\t1420\t0.004\n",
      "SNA93860\t1407\t0.004\n",
      "SNA90094\t1390\t0.004\n",
      "ELE56788\t1345\t0.004\n",
      "GRO61133\t1321\t0.003\n",
      "GRO15017\t1275\t0.003\n",
      "GRO46854\t1234\t0.006\n",
      "DAI22896\t1219\t0.003\n",
      "ELE99737\t1153\t0.006\n",
      "ELE92920\t1079\t0.006\n",
      "DAI92600\t935\t0.005\n",
      "GRO94758\t924\t0.005\n",
      "GRO85051\t859\t0.005\n",
      "DAI88807\t845\t0.004\n",
      "ELE74482\t845\t0.004\n",
      "ELE91337\t834\t0.004\n",
      "GRO24246\t818\t0.004\n",
      "SNA62128\t800\t0.004\n",
      "GRO38814\t777\t0.004\n",
      "SNA96271\t764\t0.004\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hw33_top50\n",
    "!hdfs dfs -cat hw33_top50/part-00000 | sort -k2,2nr -k1,1 | sed -n 1,50p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 EXPLANATION\n",
    "\n",
    "So with this top 50 + freq + rel freq, I was trialing another method of each mapper just sends on the top 50 word counts. Each mapper also sends its total word count as a third field for each record. \n",
    "\n",
    "This does not work, as seen by comparing with the command line counts. This method requires every word to exist in every mapper split which won't be the case - thus the rel freqs will be off. And the freqs are off - likely due to the command line counts being global.\n",
    "\n",
    "I'll need to return to this but would like to take a hack at pairs and stripes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "## HW3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "**List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support for frequent (100>count) itemsets of size 2.** \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), and break ties in support (between pairs, if any exist) by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers). Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw34_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw34_mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:hw34_mapper running\\n\")\n",
    "\n",
    "products = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #print line\n",
    "    items = line.split()\n",
    "    #print sorted(items)\n",
    "    for key in list(itertools.combinations(sorted(items), 2)):\n",
    "        products[key] += 1\n",
    "        print '%s,%s\\t%s' % (key[0], key[1], 1)\n",
    "        \n",
    "# capture totals \n",
    "for i in np.unique(list(k[0] for k in products.keys())):\n",
    "    s = sum(v for k,v in products.iteritems() if i in k[0])\n",
    "    print '%s,%s\\t%s' % (i, \"*\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:hw34_mapper running\n",
      "ELE17451,*\t4\n",
      "ELE17451,ELE89019\t1\n",
      "ELE17451,FRO11987\t1\n",
      "ELE17451,GRO99222\t1\n",
      "ELE17451,SNA90258\t1\n",
      "ELE89019,*\t3\n",
      "ELE89019,FRO11987\t1\n",
      "ELE89019,GRO99222\t1\n",
      "ELE89019,SNA90258\t1\n",
      "FRO11987,*\t2\n",
      "FRO11987,GRO99222\t1\n",
      "FRO11987,SNA90258\t1\n",
      "GRO99222,*\t1\n",
      "GRO99222,SNA90258\t1\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x hw34_mapper.py\n",
    "!head -1 browsing_behavior.txt | ./hw34_mapper.py | sort -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw34_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw34_reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_t2 = None\n",
    "cur_count = 0\n",
    "cur_total = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    t1, t2 = key.split(',')\n",
    "    if key == cur_key:\n",
    "        # sum up the marginal or the co-occurance counts\n",
    "        if cur_t2 == '*':\n",
    "            cur_total += int(value)\n",
    "        else:\n",
    "            cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key and cur_count > 100  and cur_t2 != '*':\n",
    "            print '%s\\t%s\\t%s\\t%.3f' % (cur_key, cur_count, cur_total, float(cur_count)/cur_total)\n",
    "            \n",
    "        # init either a marginal or co-occurance count\n",
    "        if t2 == '*':\n",
    "            cur_total = int(value)\n",
    "        else:\n",
    "            cur_count = int(value)\n",
    "        # init the key\n",
    "        cur_key = key\n",
    "        cur_t2 = t2\n",
    "        \n",
    "if cur_key and cur_count > 100  and cur_t2 != '*':\n",
    "    print '%s\\t%s\\t%s\\t%.3f' % (cur_key, cur_count, cur_total, float(cur_count)/cur_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\n",
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:hw34_mapper running\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./hw34_reducer.py\", line 11, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"./hw34_mapper.py\", line 24, in <module>\n",
      "    for line in sys.stdin:\n",
      "KeyboardInterrupt\n",
      "    s = sum(v for k,v in products.iteritems() if i in k[0])\n",
      "  File \"./hw34_mapper.py\", line 24, in <genexpr>\n",
      "    s = sum(v for k,v in products.iteritems() if i in k[0])\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x hw34_reducer.py\n",
    "#!head -5 browsing_behavior.txt | ./hw34_mapper.py > hw34_testout.txt\n",
    "#!cat hw34_testout.txt hw34_testout.txt | sort -k1,1 | ./hw34_reducer.py #| sort -k2,2nr\n",
    "\n",
    "!cat browsing_behavior.txt | ./hw34_mapper.py | sort -k1,1 | ./hw34_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted browsing_behavior.txt\n",
      "rm: `hw34_pairstop50': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob548789030721135062.jar tmpDir=null\n",
      "17/09/21 01:51:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/21 01:51:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/21 01:51:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/21 01:51:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/21 01:51:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504992575111_0192\n",
      "17/09/21 01:51:41 INFO impl.YarnClientImpl: Submitted application application_1504992575111_0192\n",
      "17/09/21 01:51:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504992575111_0192/\n",
      "17/09/21 01:51:41 INFO mapreduce.Job: Running job: job_1504992575111_0192\n",
      "17/09/21 01:51:51 INFO mapreduce.Job: Job job_1504992575111_0192 running in uber mode : false\n",
      "17/09/21 01:51:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/21 01:51:57 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:51:59 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:52:04 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:52:05 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:52:11 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:52:12 INFO mapreduce.Job: Task Id : attempt_1504992575111_0192_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/09/21 01:52:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/21 01:52:19 INFO mapreduce.Job: Job job_1504992575111_0192 failed with state FAILED due to: Task failed task_1504992575111_0192_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "17/09/21 01:52:19 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36257\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=36257\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=36257\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37127168\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "17/09/21 01:52:19 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x hw34_mapper.py\n",
    "!chmod a+x hw34_reducer.py\n",
    "\n",
    "!hdfs dfs -rm browsing_behavior.txt \n",
    "!hdfs dfs -copyFromLocal browsing_behavior.txt \n",
    "!hdfs dfs -rm -r hw34_pairstop50\n",
    "\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "#  -D mapreduce.job.maps=1 \\\n",
    "#  -D mapreduce.partition.keycomparator.options=\"-k1,1nr\" \\\n",
    "#  -D mapreduce.input.fileinputformat.split.minsize=180000 \\\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.name=\"HW3.4 Pairs Products\" \\\n",
    "  -D mapreduce.job.reduces=1 \\\n",
    "  -files ./hw34_mapper.py,./hw34_reducer.py \\\n",
    "  -mapper hw34_mapper.py \\\n",
    "  -reducer hw34_reducer.py \\\n",
    "  -input browsing_behavior.txt \\\n",
    "  -output hw34_pairstop50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 EXPLANATION\n",
    "\n",
    "I have lost what grip I thought I had on the MR concept and how to divide after tonight's class. I will attempt to complete this homework as best as I can, rehab, and give a go at HW4. There is no way I'm getting close to a correct implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 EXPLANATION\n",
    "\n",
    "I have to break at this point and see if I can return after I final get the legal MR break-down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL\n",
    "QUESTIONS  BELOW THIS LINE ARE OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)\n",
    "\n",
    "Dataset description\n",
    "For this assignment you will explore a set of 100,000 Wikipedia documents:\n",
    "\n",
    "https://www.dropbox.com/s/n5lfbnztclo93ej/wikitext_100k.txt?dl=0\n",
    "s3://cs9223/wikitext_100k.txt, or\n",
    "https://s3.amazonaws.com/cs9223/wikitext_100k.txt\n",
    "Each line in this file consists of the plain text extracted from a Wikipedia document.\n",
    "\n",
    "Task\n",
    "Compute the relative frequencies of each word that occurs in the documents in wikitext_100k.txt and output the top 100 word pairs sorted by decreasing order of relative frequency.\n",
    "\n",
    "Recall that the relative frequency (RF) of word B given word A is defined as follows:\n",
    "\n",
    "   f(B|A) = Count(A, B) / Count (A)   =  Count(A, B) / sum_B'(Count (A, B')\n",
    "\n",
    "where count(A,B) is the number of times A and B co-occur within a window of two words (co-occurrence window size of two) in a document and count(A) the number of times A occurs with anything else. Intuitively, given a document collection, the relative frequency captures the proportion of time the word B appears in the same document as A. (See Section 3.3, in Data-Intensive Text Processing with MapReduce).\n",
    "\n",
    "In the async lecture you learned different approaches to do this, and in this assignment, you will implement them:\n",
    "\n",
    "a.\tWrite a mapreduce program which uses the Stripes approach and writes its output in a file named rfstripes.txt \n",
    "\n",
    "b.\tWrite a mapreduce program which uses the Pairs approach and writes its output in a file named rfpairs.txt\n",
    "\n",
    "c.\tCompare the performance of the two approaches and output the relative performance to a file named rfcomp.txt. Compute the relative performance as follows: (running time for Pairs/ running time for Stripes). Also include an analysis comparing the communication costs for the two approaches. Instrument your mapper and reduces for counters where necessary to aid with your analysis.\n",
    "\n",
    "NOTE: please limit your analysis to the top 100 word pairs sorted by decreasing order of relative frequency for each word (tokens with all alphabetical letters).\n",
    "\n",
    "Please include markdown cell named rf.txt that describes the following:\n",
    "\n",
    "the input/output format in each Hadoop task, i.e., the keys for the mappers and reducers\n",
    "the Hadoop cluster settings you used, i.e., number of mappers and reducers\n",
    "the running time for each approach: pairs and stripes\n",
    "\n",
    "You can write your program using Python or MrJob (with Hadoop streaming) and you should run it on AWS. It is a good idea to develop and test your program on a local machine  before deploying on AWS. Remember your notebook, needs to have all the commands you used to run each Mapreduce job (i.e., pairs and stripes) -- include the Hadoop streaming commands you used to run your jobs.\n",
    "\n",
    "In addition the All the following files should be compressed in one ZIP file and submitted. The ZIP file should contain:\n",
    "\n",
    "\n",
    "A.\tThe result files: rfstripes.txt, rfpairs.txt, rfcomp.txt\n",
    "\n",
    "Prior to working with Hadoop, the corpus should first be preprocessed as follows:\n",
    "perform tokenization (whitespace and all non-alphabetic characters) and stopword removal  using standard tools from the Lucene search engine. All tokens should  then be replaced\n",
    "with unique integers for a more efficient encoding. \n",
    "\n",
    "\n",
    "== Preliminary information for the remaing HW problems===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "## HW3.7 Apriori Algorithm\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "## HW3.8. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8.1\"></a>\n",
    "## HW3.8.1\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF HOMEWORK\n",
    "==============="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "654px",
    "left": "0px",
    "right": "auto",
    "top": "106px",
    "width": "387px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
